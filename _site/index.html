<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Mundana Free Jekyll Theme | Seri Lee Blog</title>

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Mundana Free Jekyll Theme | Seri Lee Blog</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Mundana Free Jekyll Theme">
<meta property="og:locale" content="en_US">
<meta name="description" content="A great Jekyll theme developed by Sal @wowthemesnet.">
<meta property="og:description" content="A great Jekyll theme developed by Sal @wowthemesnet.">
<link rel="canonical" href="http://localhost:4000/">
<meta property="og:url" content="http://localhost:4000/">
<meta property="og:site_name" content="Seri Lee Blog">
<link rel="next" href="http://localhost:4000/page2">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Mundana Free Jekyll Theme">
<script type="application/ld+json">
{"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"}},"url":"http://localhost:4000/","headline":"Mundana Free Jekyll Theme","name":"Seri Lee Blog","description":"A great Jekyll theme developed by Sal @wowthemesnet.","@type":"WebSite","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <link rel="stylesheet" href="/assets/css/custom.css">
    <link rel="stylesheet" href="/assets/css/colorful.css">
<!-- IMPORTANT -->
    
    <!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">-->
    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="/assets/js/lib.js" defer></script>
    <script> 
	    MathJax = {
		    loader: {load: ['input/tex', 'ui/menu']},
		    tex: {
			    inlineMath: {'[+]': [['$','$']]},
			    displayMath: {'[+]':[['$$', '$$']]}
			}
	    }
   </script>
    <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
 <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>
  <script>
    function convert() {
      //
      //  Get the MathML input string, and clear any previous output
      //
      var input = document.getElementById("input").value.trim();
      output = document.getElementById('output');
      output.innerHTML = '';
      //
      //  Convert the MathMl to an HTML node and append it to the output
      //
      output.appendChild(MathJax.mathml2chtml(input));
      //
      //  Then update the document to include the adjusted CSS for the
      //    content of the new equation.
      //
      MathJax.startup.document.clear();
      MathJax.startup.document.updateDocument();
    }
    </script>

    
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script> 
    <script>mermaid.initialize({startOnLoad:true});</script>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body class=" homefirstpage ">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Seri Lee Blog</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Author</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>




            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/_posts/2021-08-31-homogeneous.html",
    "title": "Homogeneous Coordinates and Projective Geometry",
    "body": "Introduction to Projective Geometry: Most of the time when working with 3D, we are thinking in terms of Euclidean geometry-that is, coordinates in three-dimensional space ($X$, $Y$ and $Z$). However, there are certain situations where it is useful to think in terms of projective geometry instead. Projective geometry has an extra dimension, called $W$, in addition to the $X$, $Y$, and $Z$ dimensions. This four-dimensional space is called projective space and coordinates in projective space are called homogenous coordinates. For the purposes of 3D software, the terms projective and homogeous are basically interchangeable with 4D. Not Quaternions: Quaternions look a lot like homogeneous coordinates. Both are 4D vectors, commonly depicted as $(X,Y,Z,W)$. However, quaternions and homogeneous coordinates are different concepts, with different uses. An Analogy in 2D: First, let‚Äôs look at how projective geometry works in 2D, before we move on to 3D.  Imagine a projector that is projecting a 2D image onto a screen. It‚Äôs easy to identify the $X$ and $Y$ dimensions of the projected image.  Now, if you step back from the 2D image and look at the projector and the screen, you can see the $W$ dimension, too. The $W$ dimension is the distance from the projector to the screen .  So what does the $W$ dimension do, exactly? Imagine what would happen to the 2D image if you increased or decreased $W$-that is, if you increased or decreased the distance between the projector and the screen. If you move the projector closer to the screen, the whole 2D image becomes smaller. If you move the projector away from the screen, the 2D image becomes larger. As you can see, the value of $W$ affects the size (a. k. a scale) of the image . Applying it to 3D: There is no such thing as a 3D projector (yet), so its‚Äô harder to imagine projective geometry in 3D, but the $W$ value works exactly the same as it does in 2D. When $W$ increases, the coordinates expands (scales up). When $W$ decreases, the coordinates shrinks (scales down). The $W$ is basically a scaling transformation for the 3D coordinates . When $W = 1$: The usual advice for 3D programming beginners is to always set $W=1$ whenever converting a 3D coordinate to 4D coordinate. The reason for that is that when you scale a coordinate by a 1 it doesn‚Äôt shrink or grow, it just stays the same size. So, when $W=1$, it has no effect on the $X$, $Y$, or $Z$ values . For this reason, when it comes to 3D computer graphics, coordinates are said to be correct only when $W=1$. If you tried to render with $W=0$ your program would crash when it attempted to divide by zero. With $W&lt;0$ everything would flip unside-down and back-to-front. Mathematically speaking, there is no such thing as an incorrect homogeneous coordinate.  Using coordinates with $W=1$ is just a useful convention for the 3D computer grahics . The Math: Now, let‚Äôs look at some actual numbers, to see how the math works.  Let‚Äôs say that the projector is $3$ meters away from the screen, and there is a dot on the 2D image at the coordinate $(15, 21)$. This gives us the projective coordinate vector $(X,Y,W) = (15,21,3)$.  Now imagine that the projector was pushed closer to the screen so that the distance was $1$ meter. The closer the project gets to the screen, the smaller the image becomes. The projector has moved three times closer, so the image becomes three times smaller. If we take the original coordinate vector and divide all the values by three, we get the new vector where $W=1$:  $$(\frac{15}{3}, \frac{21}{3}, \frac{3}{3}) = (5,7,1)$$. The dot is now at coordinate $(5,7)$. This is how an incorrect homogeneous coordinate is converted to a correct coordinate: divide all the values by $W$. The process is exactly the same for 2D and 3D coordinates. Dividing all the values in a vector is done by a scalar multiplication with the reciprocal of the divisor. Here is a 4D example:  $$\frac{1}{5}(10, 20, 30, 5) = (\frac{10}{5}, \frac{20}{5}, \frac{30}{5}, \frac{5}{5}) = (2,4,6,1)$$ Uses of Homogeneous Coordinates in Computer Graphics: As mentioned earlier, in regard to 3D computer graphics, homogeneous coordinates are useful in certain situations. We will look at some of those situations here. Translation Matrices for 3D Coordinates:  A four-column matrix can only be multiplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors. Rotation and scaling transformation matrices only require three columns. But, in order to do translation, the matrices need to have at least four columns . This is why transformations are often $4 \times 4$ matices. However, a matrix with four columns cannot be multiplied by a 3D vector, due to the rules of matrix multiplication. A four-column matrix can only be mulitplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors. The 4th dimension $W$ is usually unchanged, when using homogeneous coordinates in matrix transformation. $W$ is set to $1$ when converting 3D coordinates into 4D, and is usually still $1$ after the transformation matrices are applied, at which point it can be converted back into a 3D coordinate by ignoring $W$. This is true for all translation, rotation, and scaling transformations, which by far are the most common types of transformations. The notable exception is projection matrices, which do affect the $W$ dimension. Perspective Transformation: In 3D, perspective is the phenomenon where an object appears smaller the further away it is from the camera. A far-away mountain can appear to be smaller than a cat, if the cat is close enough to the camera. Perspective is implemented in 3D computer graphics by using a transformation matrix that changes the $W$ element of each vertex. After the camera matrix is applied to each vertex, but before the projection matrix is applied, the $Z$ element of each vertex represents the distance away from the camera. Therefore, the larger $Z$ is, the more the vertex should be scaled down. The $W$ dimension affects the scale, so the projection matrix just changes the $W$ based on the $Z$ value. Here is an example of a perspective projection matrix being applied to a homogeneous coordinate:  $$ \begin{bmatrix} 1&amp;0&amp;0&amp;0 \\ 0&amp;1&amp;0&amp;0&amp; \\ 0&amp;0&amp;1&amp;0 \\ 0&amp;0&amp;1&amp;0 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1\end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1 \end{bmatrix} $$ Notice how the $W$ value is changed to $4$, which comes from the $Z$ value. After the perspective projection matrix is applied, each vertex undergoes perspective division. Perspective division is just a specific term for converting the homogeneous coordinate back to $W=1$, as explained earlier in the article. Continuing with the example above, the perspective division step would look like this:  $$\frac{1}{4}(2,3,4,4) = (0. 5,0. 75, 1,1)$$ After perspective division, the $W$ value is discarded, and we are left with a 3D coordinate that has been correctly scaled according to a 3D perspective projection. Positioning Directional Lights: One property of homogeneous coordinates is that they allow you to have points at infinity (infinite length vectors), which is not possible with 3D coordinates. Points at infinity occur when $W=0$. If you try to convert a $W=0$ homogeneous coordinate into a normal $W=1$ coordinate, it results in a bunch of divide-by-zero operations:  $$ \frac{1}{0}(2,3,4,0) = (\frac{2}{0}, \frac{3}{0}, \frac{4}{0}, \frac{0}{0})$$. This means that homogeneous coordinates with $W=0$ can not be converted back into 3D coordinates. What use does this have? Well, directional lights can be thought of as point lights that are infinitely far away. When a point light is infinitely far away, the rays of light become parallel, and all of the light travels in a single direction. This is basically the definition of a directional light. So, traditionally, in 3D graphics, directional lights are differentiated from point lights by the value of $W$ in the position vector of the light. If $W=1$, then it is a point light. If $W=0$, then it is a directional light. This is more of a traditional convention, rather than a useful way to write lighting code. Directional lights and point lights are usually implemented with separate code, because they behave differently. Summary: Homogeneous coordinates have an extra dimension called $W$, which scales the $X$, $Y$, and $Z$ dimensions. Matrices for translation and perspective projection can only be applied to homogeneous coordinates, which is why they are so common in 3D computer graphics. The $X$, $Y$, and $Z$ values are said to be correct when $W=1$. Any homogeneous coordinates can be converted to have $W=1$ by dividing all four dimensions by the $W$ value, except if $W=0$. When $W=0$, the coordinate represents a point at infinity (a vector with infinite length), and this is often used to denote the direction of directional lights. References:  Tomdalling‚Äôs Blog Post Image Processing and Computer Vision Lecture Notes"
    }, {
    "id": 1,
    "url": "http://localhost:4000/_posts/2021-08-31-rigid.html",
    "title": "3D Rigid Body Motion (Part 1)",
    "body": "Introduction: In this article, I will introduce one of the fundamental problems of visual SLAM: How to describe a rigid body‚Äôs motion in 3-dimensional space ? Intuitively, we certainly know that this consists of one rotation plus one translation . The translation part does not really have any problems, but the rotation part is questionable. I will introduce the meaning of rotation matrices, quaternions, Euler angles and how they are computed and transformed. Rotation Matrix: Points, Vectors, and Coordinate Systems: The space of our daily life is 3-dimensional, so we are born to be used to 3D movements. The 3D space consists of three axes, so the position of one spatial point can be specified by three coordinates. However, we should now consider a rigid body , which has its position and orientation . The camera can also be viewed as a rigid body in three dimensions, so what we care about in Visual SLAM are the problem of the camera‚Äôs position and orientation. Combined, we can say, ‚Äúthe camera is at the $(0,0,0)$ point, facing the front‚Äù. Let‚Äôs describe this in a mathematical term. We start from the basic content: points and vectors. Points are the basic element in space, no length, no volume. Connecting the two points forms a vector. A vector can be thought of as an arrow pointing from one point to another. Here we need to warn you not to confuse the vector with its coordinates. A vector is one thing in space, such as $a$. Here, $a$ does not need to be associated with several real numbers. We can naturally talk about the plus or minus operation of two vectors, without relating to any real numbers. Only when we specify a coordinate system in this 3D space can we talk about the vector‚Äôs coordinates in this system, finding several real numbers corresponding to this vector. With the knowledge of linear algebra, the coordinates of a point in 3D space can be described as $\mathbb{R}^3$. How to do we describe this? Suppose that in this linear space, we fined a set of base $(e_1, e_2, e_3)$ , then, an arbitrary vector $a$ has a coordinate under this base: $$ a = \begin{bmatrix} e_1 &amp; e_2 &amp; e_3 \end{bmatrix} \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = a_1 e_1 + a_2 e_2 + a_3 e_3 $$. Here, $(a_1, a_2, a_3)^T$ is called $a$‚Äôs coordinates. The coordinates‚Äô specific values are related to the vector itself and the selection of the bases. In $\mathbb{R}^3$, the coordinate system usually consists of $3$ orthogonal coordinate axes (it can also be non-orthogonal, but it is rare in practice). For example, given $x$ and $y$ axis, the $z$ axis can be determined using the right-hand (or left-hand) rule. According to different definitions, the coordinate system is divided into left-handed and right-handed. The third axis of the left-hand rule is opposite to the right-hand rule. Most 3D libraries use right-handed coordinates. Based on basic linear algebra knowledge, we can talk about the operations between vectors/vectors, vectors/numbers, such as scalar multiplication, vector addition, subtraction, inner product, outer product and so on. For $a,b \in \mathbb{R}^3$, the inner product of $a,b$ can be written as: \$\$ a \cdot b = a^Tb = \sum_{i=1}^3 a_i b_i = |a\||b| \cos(&lt;a,b&gt;)\$\$where $ &lt;a. b&gt; $ refers to the angle between the vector $a, b$. The inner product can also describe the projection relationship between vectors. $$ a \times b = \begin{Vmatrix} e_1 &amp; e_2 &amp; e_3 \\ a_1 &amp; a_2 &amp; a_3 \\ b_1 &amp; b_2 &amp; b_3 \end{Vmatrix} = \begin{bmatrix} a_2b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1 \end{bmatrix} = \begin{bmatrix} 0 &amp; -a_3 &amp; a_2 \\ a_3 &amp; 0 &amp; -a_1 \\ -a_2 &amp; a_1 &amp; 0 \end{bmatrix} b = a \wedge b$$. The result of the outer product is a vector whose direction is perpendicular to the two vectors, and the length is $|a||b|\sin(&lt;a,b&gt;)$, which is also the area of the quadrilateral of the two vectors. From the outer product operation, we introduce the $\wedge$ operator here, which means writing $a$ as a skew-symmetric matrix . You can take $\wedge$ as a skew-symmetric symbol. It turns the outer product $a \times b$ into the multiplication of the matrix and the vector $a \wedge b$ is a linear operation. This symbol will be used frequently in the following sections. It is a one-to-one mapping, meaning that for any vector, it corresponds to a unique anti-symmetric matrix, and vice versa: [a \wedge = \begin{bmatrix} 0 &amp; -a_3 &amp; a_2 \ a_3 &amp; 0 &amp; -a_1 \ -a_2 &amp; a_1 &amp; 0 \end{bmatrix}] At the same time, note that the vector operations such as addition, subtraction, inner and outer products can be calculated even when we do not have their coordinates. For example, although the inner product can be expressed by the sum of the two vectors‚Äô product when we know the coordinates, the length and angle can also be calculated even if their coordinates are unknown. Therefore, the inner product result of the two vectors is independent of the selection of the coordinate system. Euclidean Transforms between Coordinate Systems: We often define a variety of coordinate systems in the real scene. In robotics, you define one coordinate system for each link and joint; in 3D mapping, we also define a coordinate system for each cuboid and cylinder. If we consider a moving robot, it is common practice to set a stationary inertial coordinate system (or world coordinate system), such as the $x_W, y_W, z_W$ defined in the picture above. Meanwhile, the camera or robot is a moving coordinate system, such as coordinate system defined by $x_C, y_C, z_C$. We might ask: a vector $p$ in the camera system may have coordinates $p_c$; and in the world coordinate system, its coordinates maybe $p_w$ .  Then what is the conversion between two coordinates? It is necessary to first obtain the coordinate values of the point in the camera system and then use the transform rule to do the coordinate transform. We need a mathematical way to describe this transformation. As we will see later, we can describe it with a transform matrix $T$. Intuitively, the motion between two coordinate systems consists of a rotation plus a translation , which is called rigid body motion . Obviously, the camera movement is rigid. During the rigid body motion, the length and angle of the vector will not change. Imagine that you throw your phone into the air and there may be differences in spatial position and orientation. But the length and the angle of each face will not change. At this point, we say that the phone‚Äôs motion is Euclidean. The Euclidean transform consists of rotation and translation . Let‚Äôs first consider the rotation. We have a unit-length orthogonal base $(e_1, e_2, e_3)$. After a rotation it becomes $(e_1‚Äô, e_2‚Äô, e_3‚Äô)$. Then, for the same vector $a$ (the vector does not move with the rotation of the coordinate system). its coordinates in these two coordinate systems are $[a_1, a_2, a_3]^T$ and $[a_1‚Äô, a_2‚Äô, a_3]^T$. Because the vector itself has not changed, according to the definition of coordinates, there are: [[e_1, e_2, e_3] \begin{bmatrix} a_1 \ a_2 \ a_3 \end{bmatrix} = [e_1‚Äô, e_2‚Äô, e_3‚Äô] \begin{bmatrix} a_1‚Äô \ a_2‚Äô \ a_3‚Äô \end{bmatrix}] To describe the relationship between the two coordinates, we multiply the left and right side of the above equation by $\begin{bmatrix} e_1^T \\ e_2^T \\ e_3^T \end{bmatrix}$, then the matrix on the left becomes an identity matrix, so: $$\begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} \triangleq Ra‚Äô$$. We take the intermediate matrix out and define it as a matrix $R$. This matrix consists of the inner product between the two sets of bases, describing the same vector‚Äôs coordinate transformation relationship before and after the rotation. It can be said that the matrix $R$ describes the rotation itself . So we call it the rotation matrix. Meanwhile, the components of the matrix are the inner product of the two coordinate system bases. Since the base vector‚Äôs length is $1$, it is actually the cosine of the angle between the base vectors. So this matrix is also called direction cosine matrix. The rotation matrix has some special properties.  In fact, it is an orthogonal matrix with a determinant of $1$. Conversely, an orthogonal matrix with a determinant of 1 is also a rotation matrix. So you can define a set of $n$ dimensional rotation matrices as follows:       $$SO(n) = \{ R \in \mathbb{R}^{n \times n}   RR^T = I, det(R) =1 \} $$   $SO(n)$ refers to the special orthogonal group. This set consists of a rotation matrix of $n$ dimensional space, in particular, $SO(3)$ refers to the rotation of the three-dimensional space. In this way, we can talk directly about the rotation transformation between the two coordinate systems without having to start from the bases. Since the rotation matrix is orthogonal, its inverse (i. e. , transpose) describes an opposite rotation. According to the above definition, there are: [a‚Äô = R^{-1}a = R^Ta] Obviously, the $R^T$ represents an opposite rotation. In the Euclidean transformation, there is a translation in addition to rotation. Consider the vector $a$ in the world coordinate system. After a rotation (depicted by $R$) and a translation of $t$, we get $a‚Äô$. Then we can put the rotation and translation together, and have:$$a‚Äô= Ra+t$$,where $t$ is called a translation vector. Compared to the rotation, the translation part simply adds the translation vector to the coordinates after the rotation , which si very simple. By the above formula, we completely describe the coordinate transformation relationship using a rotation matrix $R$ and a translation vector $t$. In practice, we may define the coordinate system 1 and 2, then the vector $a$ under the two coordinates is $a_1, a_2$. The relationship between the two systems should be: $$a_1 = R_{12} a_2 + t_{12}$$. Here, $R_{12}$ means the ‚Äúrotation of the vector from system 2 to system 1‚Äù. About $t_{12}$, readers may just take it as a translation vector without wondering about its physical meaning. In fact, it corresponds to a vector from the system 1‚Äôs origin pointing to system 2‚Äôs origin, and the coordinates are taken under tsystem 1. So I suggest you to understand it as ‚Äúa vector from 1 to 2‚Äù. But the reverse $t_{21}$, which is a vector from $2$‚Äôs origin to $1$‚Äôs origin, whose coordinates are taken in system $2$, is not equal to $-t_{12}$. It is also related to the rotation of the two systems. Therefore, when beginners ask the question ‚ÄúWhat are my coordinates?‚Äù, we need to clearly explain this sentence‚Äôs meaning. Here, ‚Äúmy coordinates‚Äù normally refers to the vector from the world system $W$ pointing to the origin of the camera system $C$, and then take the coordinates in the world‚Äôs base. Corresponding to the mathematical symbol, it should be the value of $t_{WC}$. For the same reason, it is not $-t_{CW}$ but actually $-R^T_{CW} t_{CW}$. Transform Matrix and Homogeneous Coordinates: The formula $a‚Äô = Ra+t$ fully expresses the rotation and the translation of Euclidean space, but there is still a small problem: the transformation relationship here is not a linear relationship. Suppose we made two transformations: $R_1,t_1$ and $R_2,t_2$:$$b = R_1 a + t_1, c = R_2 b + t_2$$. So the transformation from $a$ to $c$ is: $$c = R_2 (R_1 a + t_1) + t_2$$. This form is not elegant after multiple transformations. Therefore, we introduce homogeneous coordinates and transformation matrices, rewriting the formula:$$ \begin{bmatrix} a‚Äô \\ 1 \end{bmatrix} = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix} \begin{bmatrix} a \\ 1 \end{bmatrix} \triangleq T \begin{bmatrix} a \\ 1 \end{bmatrix}$$. This is a mathematical trick: we add $1$ at the end of the 3D vector and turn it into a 4D vector called homogeneous coordinates. For this four-dimensional vector, we can write the rotation and translation matrix, making the whole relationship a linear relationship. In this formula, the matrix $T$ is called transform matrix. We temporarily use $\tilde{a}$ to represent the homogeneous coordinates of $a$. Then, relying on homogeneous coordinates and transformation matrices, the superposition of the two transformations can have a good form: $$\tilde{b} = T_1 \tilde{a}, \tilde{c} = T_2 \tilde{b} \Rightarrow T_2 T_1 \tilde{a}$$. But the symbols that distinguish between homogeneous and non-homogeneous coordinates are annoying, because here we only need to add 1 at the end of the vector or remove 1 to turn it into a normal vector. So, without ambiguity, we will write it directly as $b = Ta$ and by default we just assume a homogeneous coordinate conversion is made if needed. The transformation matrix $T$ has a special structure: the upper left corner is the rotation matrix, the right side is the translation vector, the lower-left corner is $0$ vector, and the lower right corner is $1$. This set of transform matrix is also known as the special Euclidean group: $$SE(3) = \{ T = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix} \in \mathbb{R}^{4 \times 4}| R \in SO(3), t \in \mathbb{R}^3 \}$$. Like $SO(3)$, the inverse of the transformation matrix represents an inverse transformation: $$T^{-1} = \begin{bmatrix} R^T &amp; -R^T t \\ 0^T &amp; 1 \end{bmatrix}$$. Again, we use the notation of $T_{12}$ to represent a transformation from 2 to 1. Because the conversion between homogeneous and non-homogeneous coordinates is actually very easy, it is assumed that the conversion from homogeneous coordinates to normal coordinates is already done. Summary: First, we introduced the vector and its coordinate representation and introduced the operation between the vectors; then, the motion between the coordinate systems is described by the Euclidean transformation, which consists of translation and rotation. The rotation can be described by the rotation matrix $SO(3)$, while the translation is directly described by an $\mathbb{R}^3$ vector. Finally, if the translation and rotation are placed in a matrix, the transformation matrix $SE(3)$ is formed. References:  Introduction to Visual SLAM"
    }, {
    "id": 2,
    "url": "http://localhost:4000/_posts/2021-09-05-camera.html",
    "title": "Dissecting the Camera Matrix (Part 1)",
    "body": " Camera Calibration and Decomposition : Primarily, camera calibration is about finding the quantities internal to the camera that affect the imaging process. Here are some of the factors that will be taken care of:  image center : we need to find the position of the image center in the image. Wait, isn't the image center located at $(width/2, height/2)$? Well, no. Unless we calibrate the camera, the image will almost always appear to be off-center.  focal length : this is a very important parameter. Remember how people using DSLR cameras tend to focus on things before capturing the image? this parameter is directly related to the focus of the camera and it is very critical.  scaling factors : the scaling factors for row pixels and column pixels might be different. If we don't take care of this thing, the image will look stretched (either horizontally or vertically).  skew factor : this refers to shearing. the image will look like a parallelogram otherwise.  lens distortion : this refers to the pseudo zoom effect that we see near the center of any image.  shearing refers to a transformation in which all points along a given line $L$ remain fixed while other points are shifted parallel to $L$ by a distance proportional to their perpendicular distance from $L$.  Pinhole Camera Model : Before we jump into anything, let's see where this all began. When we capture an image, we are basically mapping the 3D scene to a 2D scene. It means that every point in the 3D world gets mapped to the 2D plane of our image. This is called the pinhole camera model . It basically describes the relationship between the coordinates of the 3D point and its projection on the 2D image. This, of course, is the ideal case where there is absolutely no distortion of any kind. Every camera is modeled based on this, and every camera aspires to simulate this as close as possible. But in the real world, we have to deal with things like geometric distortions, blurring, finite sized apertures, etc. The figure shown here depicts a pinhole camera model. The camera is placed at the origin $O$. The point $P$ represents a point in the real world. We are trying to capture that onto a 2D plane. The image plane represents the 2D plane that you get after capturing the image. The image plane actually contains the image that you see after capturing a picture. So basically, we are trying to map every 3D point to a point on the image plane. In this case, the point $P$ gets mapped to $P_c$. The distance between the origin $O$ and this image plane is called the focal length of the camera. This is the parameter you modify when you adjst the focus of the camera.  Intrinsic and Extrinsic Parameters : In the above figure, we want to estimate $(u,v)$ from $(X,Y,Z)$. Let's say the focal length is denoted by $f$. If you look at the triangle formed using the origin-$P_c$-and the $Z$-axis with the origin-$P$ and $Z$-axis, you will notice that they are similar triangles. This means that $u$ depends on the $f$, $X$ and $Z$. Similarly, $v$ depends on $f$, $Y$ and $Z$. $$\displaylines{u = fX/Z \\\v = fY/Z}$$Next, if the origin of the 2D image coordinate system does not coincide with where the $Z$-axis intersects the image plane, we need to translate $P_c$ into the desired origin. Let this translation be defined by $(t_u, t_v)$. So now, $u$ and $v$ are given by:$$\displaylines{u = fX/Z + t_u \\\v = fY/Z + t_v}$$So up until now, we have something that can translate $(X,Y,Z)$ to $(u,v)$. Let's denote this matrix $M$. So we can write:$$ P_c = MP$$Since this is a camera image, we need to express it in inches. For this, we will need to know the resolution of the camera in pixels/inch. If the pixels are square the resolution will be identical in both $u$ and $v$ directions of the camera image coordinates. However, for a more general case, we assume rectangular pixels with resolution $m_u$ and $m_v$ pixels/inch in $u$ and $v$ directions respectively. Therefore, to measure $P_c$ in pixels, its $u$ and $v$ coordinates should be multiplied by $m_u$ and $m_v$ respectively. So now, this new transformation matrix depends on $f, X, Y, Z, t_u, t_v, m_u, m_v$. Let's denote this by:$$P_c = KP$$Here, $K$ is called the intrinsic parameter matrix for the camera. Now, if the camera does not have its center of projection at $(0,0,0)$ and is oriented in an arbitrary fashion (not necessarily $z$-perpendicular to the image plane), then we need roation and translation to make the camera coordinate system coincide with the configuration in that pinhole camera figure. Let the rotation applied to coincide the principal axis with $Z$-axis given by a $3 \times 3$ rotation matrix $R$. Then the matrix is formed by first applying the translation followed by the rotation is given by the $3 \times 4$ matrix. $$E = \\( R|RT \\)$$This is called the extrinsic parameter matrix for the camera . So, the complete camera transformation can now be represented as: $$ K \\( R|RT \\) = KR \\( I|T \\)$$Hence, $P_c$ the projection of $P$ is given by:$$P\_c = KR \\( I|T \\) P = CP$$$C$ is a $3 \times 4$ matrix usually called the complete camera calibration matrix. So basically, camera calibration matrix is used to transform a 3D point in the real world to a 2D point on the image plane considering all the things like focal length of the camera, distortion, resolution, shifting of origin, etc.  References :  ksimek blog prateekvjoshi blog "
    }, {
    "id": 3,
    "url": "http://localhost:4000/_posts/2021-09-05-disparity.html",
    "title": "Depth from Disparity",
    "body": " üåü 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images ? What cues in the image provide 3D information? Before looking at binocular, let‚Äôs consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R)}{2(x_L - x_R)} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 4,
    "url": "http://localhost:4000/_posts/2021-09-08-group.html",
    "title": "Batch Normalization and Group Normalization",
    "body": " Batch Normalization: the Principles : Batch Normalization is an algorithmic method which makes the training of Deep Neural Networks faster and more stable . Batch normalization is computed differently during the training and the testing phase. At training , the BN layer determines the mean and standard deviation of the activation values across the batch. It then normalizes the activation vector with $\mu$ and $\sigma$. That way, each neuron‚Äôs output follows a standard normal distribution across the batch . It finally applies a linear transformation with $\gamma$ and $\beta$ which are the two trainable parameters. Such step allows the model to choose the optimum distribution for each hidden layer. $\gamma$ allows to adjust the standard deviation while $\beta$ allows to adjust the bias, shifting the curve on the right or on the left side. At each iteration, the network computes the mean $\mu$ and the standard deviation $\sigma$ corresponding to the current batch. Then it trains $\gamma$ and $\beta$ through gradient descent using an Exponential Moving Average (EMA) to give more importance to the latest iterations.  We mostly use Exponential Moving Average algorithm to reduce the noise or to smooth the data. The weight of each element decreases progressively over time, meaning the EMA gives greater weight to recent data points . EMA reacts faster to changes compared to Simple Moving Average. At the evaluation phase , we may not have a full batch to feed into the model. To tackle this issue, we compute $\mu_{pop}$ and $\sigma_{pop}$ as the estimated mean and standard deviation of the studied population . Those values are computed using all the $\mu_{batch}$ and $\sigma_{batch}$ during training, and directly fed during the evaluation phase.  Why Normalization? : What we can conclude from the original Batch Normalization paper is that:  Adding BN layers leads to better convergence and higher accuracy  Adding BN layers allows us to use higher learning rate without compromising convergence. To quote Ian Goodfellow about the use of batch normalization: Before BN, we thought it was almost impossible to efficiently train deep models using sigmoid in the hidden layers. Batch Normalization makes those unstable networks trainable. In practice, it is widely admitted that: For CNNs, Batch Normalization is better  For Recurrent Networks, Layer Normalization is better While BN uses the current batch to normalize every single value, LN uses all the current layer to do so.  The normalization is performed using other features from a single example instead of using the same feature across all current batch examples. The best way to understand why BN works is to understand the optimization landscape smoothness . BN reparameterizes the underlying optimization problem , making the training faster and easier. In additional recent studies, researchers observed that this effect is not unique to BN, but applies to other normalization methods (i. e. L1 normalization or L2 normalization).  The Drawbacks of BN : For BN to work, the batch size is required to be sufficiently large , usually at least 32 . However, there are situations when we have to settle for a small batch size. For example, when each data sample is highly memory consuming or when we train a very large neural network which leaves little GPU memory for processing data. For computer vision applications other than image classification, the restriction on batch sizes are more demanding and it is difficult to have higher batch sizes.  Comparisions of Normalization Methods : Group Normalization is one of the latest normalization methods that avoids exploiting the batch dimension, thus is independent of batch size. But there are other normalization methods as well.  Layer Normalization : Layer Normalization computes $\mu_i$ and $\sigma_i$ along the (C,H,W) axes. The computation for an input feature is entirely independent of other input features in a batch.  Instance Normalization : Instance Normalization computes $\mu_i$ and $\sigma_i$ along the (H,W) axes. Since the computation of IN is the same as that of BN with batch_size=1, IN actually makes the situation worse in most cases. However, for style transfer tasks , IN is better at discarding contrast information of an image, thus having superior performance than BN.  Group Normalization : Also notice that IN can be viewed as applying Layer Normalization to each channel individually as if the num_channels = 1. Group Normalization is the middle ground between IN and LN. It organizes the channels into different groups and computes $\mu_i$ and $\sigma_i$ along the (H,W) axes and along a group of channels. First, the batch with dimension (N,C,H,W) is reshaped to (N,G,C//G,H,W). The number of group $G$ is a pre-defined hyperparameter . Then we normalize along the (C//G,H,W) dimension and return the result after reshaping the batch back to (N,C,H,w). Group Normalization is better than Layer Normalization as GN allows different distribution to be learned for each group of channels. GN is also thought to be better than IN because GN can exploit the dependence across channels. If `C = G`, that is, if the number of groups are set to be equal to the number of channels, GN becomes IN . Likewise, if `G = 1` GN becomes LN .  References :  blog post medium article medium article2 "
    }, {
    "id": 5,
    "url": "http://localhost:4000/_posts/2021-09-09-no-bullshit.html",
    "title": "Guide to Linear Algebra (Part 1)",
    "body": " Computational Linear Algebra : This chapter covers the computational aspects of performing matrix calculations. Understanding matrix computations is important because all later chapters depend on them. Suppose we‚Äôre given a huge matrix $ A \in R^{n \times n} $ with $ n=1000 $. Hidden behind the innocent-looking mathematical notation of the matrix inverse $A^{-1}$, the matrix product $AA$, and the matrix determinant $ | A |$, lie monster coputations involving all the $1000 \times 1000 = 1$ million entries of the matrix $A$. Millions of arithmetic operations must be performed, so I hope you have at least a thousand pencil ready! Okay, calm down. I won‚Äôt actually make you calculate millions of arithmetic operations. In fact, to learn linear algebra, it is sufficient to know how to carry out calculations with $3 \times 3$ and $4 \times 4$ matrices. Even for such moderately sized matrices, computing products, inverses, and determinants by hand are serious computational tasks. If you‚Äôre ever required to take a linear algebra final exam, you need to make sure you can do these calculations quickly. Even if no exam looms in your imminent future, it‚Äôs important to practice matrix operations by hand to get a feel for them. This chapter will introduce you to the following computational tasks involving matrices: Gauss-Jordan elimination Suppose we're trying to solve two equations in two unknowns $x$ and $y$:$$ \displaylines{ax+by = c \\\dx+ ey= f}$$If we add $\alpha$\times the first equation to the second equation, we obtain an equivalent system of equations:$$\displaylines{ax + by = c \\\(d + \alpha a)x + (e + \alpha b)y = f + \alpha c}$$This is called a row operation : we added $\alpha$-times the first row to the second row. Row operations change the coefficient of the system of equations, but leave the solution unchanged. Gauss-Jordan elimination is a systematic procedure for solving systems of linear equations using row operations.  Matrix product The product $AB$ between matrices $A \in \mathbb{R}^{m \times l}$ and $B \in \mathbb{R}^{l \times n}$ is the matrix $C \in \mathbb{R}^{m \times n}$ whose coefficients $c_{ij}$ are defined by the formula $c_{ij} = \sum_{k=1}^{l}a_{ik}b_{kj}$ for all $i \in \lbrack 1, \dots, m \rbrack $ and $j \in \lbrack 1, \dots, n \rbrack $. We'll soon unpack this formula and learn about its intuitive interpretation: that computing $C = AB$ is computing all the dot products between the rows of $A$ and the columns of $B$.  Determinant The determinant of a matrix $A$, denoted $|A|$ is an operation that gives us useful information about the linear independence of the rows of the matrix. The determinant is connected to many notions of linear algebra: linear independence, geometry of vectors, solving systems of equations, and matrix invertibility. We'll soon discuss these aspects.  Matrix inverse We'll build upon our knowledge of Gauss-Jordan elimination, matrix products, and determinants to derive three different procedures for computing the matrix inverse $A^{-1}$. Reduced Row Echelon Form : In this section, we‚Äôll learn to solve systems of linear equations using the Gauss-Jordan elimination procedure. A system of equations can be represented as a matrix of coefficients. The Gauss-Jordan elimination procedure converts any matrix into its  reduced row echelon form (RREF) . We can easily find the solution (or solutions of the system of equations from the RREF. Listen up: the material covered in this section requires your full on, caffeinated attention, as the procedures you‚Äôll learn are somewhat tedious. Gauss-Jordan elimination involves many repetitive mathematical manipulations of arrays of numbers. It‚Äôs important you hang in there and follow through the step-by-step manipulations, as well as verify each step I present on your own with pen and paper.  Solving Equations : Suppose you‚Äôre asked to solve the following system of equations: [\displaylines{1x_1 + 2x_2 = 5 \3x_1 + 9x_2 = 21}] The standard approach is to use one of the equation-solving tricks we learned to combine the equations and find the values of the two unknowns $x_1$ and $x_2$. Observe that the names of the two unknowns are irrelevant to the solution of the system of equations. Indeed, the solution $(x_1, x_2)$ to the above system of equations is the same as the solution $(s,t)$ to the system of equations [\displaylines{1s+ 2t = 5 \3s+ 9t = 21}] The important parts of a system of linear equations are the coefficients in front of the variables and the constants on the right-hand side of each equation.  Augmented Matrix : The system of linear equations can be written as an augmented matrix : [\begin{pmatrix} 1 &amp; 2 &amp;\bigm | &amp; 5 \        3 &amp; 9 &amp;\bigm | &amp; 21\end{pmatrix}] The first column corresponds to the coefficients of the first variable, the second column is for the second variable, and the last column corresponds to the constants of the right-hand side. It is customary to draw a vertical line where the equal signs in the equations would normally appear. This line helps distinguish the coefficients of the equations from the column of constants on the right-hand side. Once we have the augmented matrix, we can simplify it by using row operations (which we‚Äôll discuss shortly) on its entries. After simplification by row operations, the augmented matrix will be transformed to [\begin{pmatrix}1 &amp; 0 &amp;\bigm | &amp; 5 \0 &amp; 1 &amp;\bigm | &amp; 2\end{pmatrix}] which corresponds to the system of equations [\displaylines{x_1 = 1 \x_2 = 2}] This is a trivial system of equations; there is nothing left to solve and we can see that the solutions are $x_1 = 1$ and $x_2 = 2$. This example illustrates the general idea of the Gauss-Jordan elimination procedure for solving the system of equations by manipulating an augmented matrix.  Row Operations : We can manipulate the rows of an augmented matrix without changing its solutions. We‚Äôre allowed to perform the following three types of row operations: Add a multiple of one row to another row Swap the position of the two rows Multiply a row by a constant Let‚Äôs trace the sequence of row operations needed to solve the system of equations [\displaylines{x_1 + 2x_2 = 5 \3x_1 + 9x_2 = 21}] starting from its augmented matrix: [\begin{pmatrix}1 &amp; 2 &amp;\bigm | &amp; 5 \3 &amp; 2 &amp;\bigm | &amp; 21\end{pmatrix}]  As a first step, we eliminate the first variable in the second row by subtracting three times the first row from the second row. [\begin{pmatrix}1 &amp; 2 &amp;\bigm | &amp; 5 \0 &amp; 3 &amp;\bigm | &amp; 6\end{pmatrix}] We denote this row operation as $R_2 \leftarrow R_2 - 3R_1$.  To simplify the second row, we divide it by 3 to obtain[\begin{pmatrix}1 &amp; 2 &amp;\bigm | &amp; 5 \0 &amp; 1 &amp;\bigm | &amp; 2 \end{pmatrix}] This row operation is denoted $R_2 \leftarrow \frac{1}{3}R_2$.  The final step is to eliminate the second variable from the first row. We do this by subtracting two times the second row from the first row $R_1 \leftarrow R_1 - 2 R_2$:[\begin{pmatrix}1 &amp; 0 &amp;\bigm | &amp; 1 \0 &amp; 1 &amp;\bigm | &amp; 2\end{pmatrix}] We can now read off the solution: $x_1 =1$ and $x_2 = 2$. Note how we simplified the augmented matrix through a specific procedure: we followed the Gauss-Jordan elimination algorithm to bring the matrix into its reduced row echelon form. The reduced row echelon form (RREF) is the simplest form for an augmented matrix. Each row contains a leading one (a numeral 1) also known as a pivot . Each column‚Äôs pivot is used to eliminate the numbers that lie below and above it in the same column. The end result of this procedure is the reduced row echelon form: [\begin{pmatrix}1 &amp; 0 &amp; \ast &amp; 0 &amp;\bigm | \ast \0 &amp; 1 &amp; \ast &amp; 0 &amp;\bigm | \ast \0 &amp; 0 &amp; 0 &amp; 1 &amp;\bigm | \ast\end{pmatrix}] Note the matrix contains only zero entries below and above the pivots. The asterisks $\ast$ denote arbitrary numbers that could not be eliminated because no leading one is present in these columns. The solution to a system of linear equations in the variables $x_1, x_2, \dots, x_n$ is the set of values $\{(x_1, x_2, \dots, x_n)\}$ that satisfy all the equations. Gaussian elimination is the process of bringing a matrix into row echelon form.  A matrix is said to be in row echelon form (REF) if all entries below the leading ones are zero. This form can be obtained by adding or subtracting the row with the leading one from the rows below it.  Gaussian-Jordan elimination is the process of bringing a matrix into reduced row echelon form.  A matrix is said to be in reduced row echelon form (RREF) if all the entries below and above the pivots are zero. Starting from the REF, we obtain the RREF by subtracting the row containing the pivots from the rows above them. the rank of the matrix $A$ is the number of pivots in the RREF of $A$.  Number of Solutions : A system of linear equations in three variables could have: one solution If the RREF of a matrix has a pivot in each row, we can read off the values of the solution by inspection. $$\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; c_1 \\\0 &amp; 1 &amp; 0 &amp; c_2 \\\0 &amp; 0 &amp; 1 &amp; c_3\end{bmatrix}$$The unique solution is $x_1 = c_1$, $x_2 = c_2$, and $x_3 = c_3$.  Infinitely many solutions 1 If one of the equations is redundant, a row of zeros will appear when the matrix is brought to the RREF. This happens when one of the original equations is a linear combination of the other two. In such cases, we're really solving two equations in three variables, so can't pin down one of the unknown variables. We say the solution contains a free variable . For example, consider the following RREF:$$\begin{bmatrix} 1 &amp; 0 &amp; a_1 &amp; c_1 \\\0 &amp; 1 &amp; a_2 &amp; c_2 \end{bmatrix}$$The column that doesn't contain a leading one corresponds to the free variable. To indicate that $x_3$ is a free variable, we give it a special label $x_3 \equiv t$. The variable $t$ could be any number $t \in \mathbb{R}$. In other words, when we say $t$ is free, it means $t$ can take on any value from $-\infty$ to $+\infty$. The information in the augmented matrix can now be used to express $x_1$ and $x_2$ in terms of the right-hand constants and the free variable $t$:$$\begin{Bmatrix} x_1 = c_1 - a_1 t \\ x_2 = c_2 - a_2 t \\ x_3 = t, \forall t \in \mathbb{R} \end{Bmatrix} = \begin{Bmatrix} \begin{bmatrix} c_1 \\ c_2 \\ 0 \end{bmatrix} + t \begin{bmatrix} -a_1 \\ -a_2 \\ 1 \end{bmatrix}, \forall t \in \mathbb{R}\end{Bmatrix}$$. The solution corresponds to the equation of a line passing through the point $(c_1, c_2, 0)$ with direction vector $(-a_1, -a_2, 1)$. We'll discuss the geometry of lines in the next section. For now, it's important that you understand that a system of equations can have more than one solution; any point on the line $l \equiv \{(c_1, c_2, 0) + t(-a_1, -a_2, 1), \forall t \in \mathbb{R}\}$ is a solution to the above system of equations.  Infinitely many solutions 2 It's also possible to obtain a two-dimensional solution space. This happens when two of the three equations are redundant. In this case, there will be a single leading one, and thus two free variables. For example, in the RREF$$\begin{bmatrix} 0 &amp; 1 &amp; a_1 &amp; c_1 \\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}$$the variables $x_1$ and $x_3$ are free. As in the previous infinitely-many-solutions case, we define new labels for the free variables $x_1 \equiv s$ and $x_3 \equiv t$, where $ s \in \mathbb{R}$ and $t \in \mathbb{R}$ are two arbitrary numbers. The solution to this system of equations is $$ \begin{Bmatrix} x_1 = s \\ x_2 = c_2 - a_2 t \\ x_3 = t , \\ \forall s,t \in \mathbb{R} \end{Bmatrix} = \begin{Bmatrix} \begin{bmatrix} 0 \\ c_2 \\ 0 \end{bmatrix} + s \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} + t \begin{bmatrix} 0 \\ -a_2 \\ 1 \end{bmatrix}, \forall s,t \in \mathbb{R} \end{Bmatrix} $$This solution set corresponds to the parametric equation of a plane that contains the point $(0,c_2, 0)$ and the vectors $(1,0,0)$ and $(0, -a_2, 1)$. The general equation for the solution plane is $0x+1y+a_2z = c_2$, as can be observed from the first row of the augmented matrix. In the next section, we'll learn more about the geometry of planes and how to convert between their general and parametric forms.  no solutions If there are no numbers $(x_1, x_2, x_3)$ that simultaneously satisfy all three equations, the system of equations has no solution. An example of a system of equations with no solution is the pair $ s+t = 4$ and $s+t = 44$. There are no numbers $(s,t)$ that satisfy both these equations. A system of equations has no solution if its reduced row echelon form contains a row of zero coefficients with a nonzero constant in the right-hand side:$$\begin{Bmatrix}\begin{array}{ccc|c}1 &amp; 0 &amp; 0 &amp; c_1 \\\ 0 &amp; 1 &amp; 0 &amp; c_2 \\\ 0 &amp; 0 &amp; 0 &amp; c_3 \end{array}\end{Bmatrix}$$If $c_3 \neq 0$ this system of equations is impossible to satisfy. There is no solution because there are no numbers $(x_1, x_2, x_3)$ such that $0x_1 + 0x_2 + 0x_3 = c_3$. Dear reader, we've reached the first moment in this book where you'll need to update your math vocabulary. The solution to an individual equation is a finite set of points. The solution to a system of equations can be an entire space containing infinitely many points, such as a line or a plane. The solution set of a system of three linear equations in three unknowns could be either the empty set $\{0\}$ (no solution), a set with one element $\{(x_1, x_2, x_3)\}$, or a set with infinitely many elements like a line $\{p_o + t \overrightarrow{v}, t \in \mathbb{R}\}$ or a plane $\{p_o + s \overrightarrow{v} + t \overrightarrow{w}, s,t \in \mathbb{R}\}$. Another possible solution set is all of $\mathbb{R}^3$; every vector $ \overrightarrow{x} \in \mathbb{R}^3 $ is a solution to the equation $$\begin{bmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$$Note the distinction between the three types of infinite solution sets. A line is one-dimensional, a plane is two-dimensional, and $\mathbb{R}^3$ is three-dimensional. Describing all points on a line requires one parameter, describing all points on a plane takes two parameters, and-of course-describing a point in $\mathbb{R}^3$ takes three parameters.  Geometric Interpretation : We can gain some intuition about solution sets by studying the geometry of the intersections of lines in $\mathbb{R}^2$ and planes in $\mathbb{R}^3$.  Lines in two dimensions : Equations of the form $ax+by = c$ corresponds to lines in $\mathbb{R}^2$. Solving systems of linear equations of the form$$\displaylines{a_1 x + b_1 y = c_1 \\\a_2 x + b_2 y = c_2}$$requires finding the point $(x,y) \in \mathbb{R}^2$ where these lines intersect. There are three possibilities for the solution set:- one solution if the two lines intersect at a point. - infinitely many solutions if the lines are superimposed. - no solution if the two lines are parallel and never intersect. Planes in three dimensions : Equations of the form $ax+by+cz = d$ correspond to planes in $\mathbb{R}^3$. When solving three such equations, $$ \displaylines{a_1 x + b_1 y + c_1 z = d_1 \\\a_2 x + b_2 y + c_2 z = d_2 \\\a_3 x + b_3 y + c_3 z = d_3}$$we want to find a set of points $(x,y,z)$ that satisfy all three equations simultaneously. There are four possibilities for the solution set:1.  one solution three non-parallel planes intersect at a point. 2.  infinitely many solutions 1 if only one of the plane equations is redundant, the solution corresponds to the intersection of two planes which is a line. 3.  infinitely many solutions 2 if two of the equations are redundant, then the solution space is a two-dimensional space. 4.  if two (or more) of the planes are parallel, they will never intersect.  Determinants : Overview : What is the volume of a rectangular box of length $1m$, width $2$ and height $3m$? It's easy to compute the volume of this box because its shape is right rectangular prism. The volume of this prism is $V = l \times w \times h = 6m^3$. What if the shape of the box was a parallelpiped instead? A parallelpiped is a box whose opposite faces are parallel but whose sides are slanted. How do we compute the volume of a parallelpiped? The determinant operation, specifically the $3 \times 3$ determinant, is the perfect tool for this purpose. The determinant of a matrix, denoted $det(A)$ or $|A|$, is a particular way to multiply the entries of the matrix to produce a single number. We use determinants for all kinds of tasks: to compute areas and volumes, to solve systems of linear equations, to check whether a matrix is invertible or not, etc. We can interpret the determinant of a matrix intuitively as a geometrical calculation. The determinant is the volume of the geometric shape whose edges are the rows of the matrix. For $2 \times 2$ matrices, the determinant corresponds to the area of a parallelogram. For $3 \times 3$ matices, the determinant corresponds to the volume of a parallelpiped. For dimensions $d &gt; 3$, we say the determinant measures a $d$-dimensional hyper-volume . Consider the linear transformation $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined through the matrix-vector product with a matrix $A_T: T(\overrightarrow{x}) \equiv A_T \overrightarrow{x}$. The determinant of the matrix $A_T$ is the scale factor associated with the linear transformation $T$. The scale factor of the linear transformation $T$ describes how the area of a unit square in the input space (a square with dimensions $1 \times 1$) is transformed by $T$ . After passing through $T$, the unit square is transformed to a parallelogram with with area $det(A_T)$. Linear transformations that shrink areas have $det(A_T) &lt; 1$, while linear transformations that enlarge areas have $det(A_T) &gt; 1$. A linear transformation that is area preserving has $det(A+T) = 1$. The determinant is also used to check linear independence for a given set of vectors. We construct a matrix using the vectors as the matrix rows , and compute its determinant. The determinant of a matrix tells us whether or not that matrix is invertible . If $det(A) \neq 0$, then $A$ is invertible; if $det(A) = 0$, $A$ is not invertible. The determinant shares a connection with the vector cross product , and is used in the definition of the eigenvalue equation . Formulas : The determinant of a $2 \times 2$ matrix is $$det(\begin{bmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{bmatrix}) = \begin{vmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{vmatrix} = a_{11}a_{22} - a_{12}a{21}$$The formulas for the determinants of larger matrices are defined recursively. For example, the determinant of $3 \times 3$ matirx is defined in terms of $2 \times 2$ determinants:$$ \begin{vmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\ a_{31} &amp; a_{32} &amp; a_{33} \end{vmatrix} = a_{11} = a_{11} \begin{vmatrix}a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33} \end{vmatrix} -a_{12} \begin{vmatrix} a_{21} &amp; a_{23} \\ a_{31} &amp; a_{33} \end{vmatrix}+ a_{13} \begin{vmatrix} a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32} \end{vmatrix}$$ There's a neat computational trick for computing $3 \times 3$ determinants by hand. The trick consists of extending the matrix $A$ into a $3 \times 5$ array that contains copies the columns of $A$: the $1^{st}$ column of $A$ is copied to the $4^{th}$ column of the extended array, and the $2^{nd}$ column of $A$ is copied to the $5^{th}$ column. The determinant is then computed by summing the products of the entries on the three positive diagonals and subtracting the products of the entries on the three negative diagonals. The general formula for the determinant of an $n \times n$ matrix is$$det(A) = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} M_{1j}$$where $M_{ij}$ is called the minor associated with the entry $a_{ij}$. The minor $M_{ij}$ is the determinant of the submatrix obtained by removing the $i^{th}$ row and the $j^{th}$ column of the matrix $A$. Note the alternating factor $(-1)^{i+j}$ that changes value between $+1$ and $-1$ for different terms in the formula. The determinant of a $4 \times 4$ matrix $B$ is $$ det(B) = b_{11}M_{11} - b_{12}M_{12} + b_{13}M_{13} - b_{14}M_{14}$$The general formula for determinants $det(A) = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} M_{1j}$ assumes we're expanding the determinant along the first row of the matrix. In fact, a determinant formula can be obtained by expanding the determinant along anyrow or column of the matrix. The expand-along-any-row-or-column nature of determinants can be very handy: if you need to calculate the determinant of a matrix with one row (or column) containing many zero entries, it makes sense to expand along that row since many of the terms in the formula will be zero. If a matrix contains a row (or column) consisting entirely of zeros, we can immediately tell its determinant is zero.  Geometric interpretation : Area of a parallelogram Suppose we're given vectors $overrightarrow{v} = (v_1, v_2)$ and $\overrightarrow{w} = (w_1, w_2)$ in $\mathbb{R}^2$ and we construct a parallelogram with corner points $(0,0), \overrightarrow{v}, \overrightarrow{w}$ and $\overrightarrow{v} + \overrightarrow{w}$. The area of this parallelogram is equal to the determinant of the matrix that contains $(v_1, v_2)$ and $(w_1, w_2)$ as rows:$$area = \begin{vmatrix} v_1 &amp; v_2 \\ w_1 &amp; w_2 \end{vmatrix} = v_1 w_2 - v_2 w_1 $$ Volume of a parallelpiped  Sign and absolute value of the determinant Calculating determinants can produce positive or negative numbers. "
    }, {
    "id": 6,
    "url": "http://localhost:4000/_posts/2021-09-12-pytorch.html",
    "title": "Advanced PyTorch: Things You Didn't Know",
    "body": " Flatten Operation for a Batch of Image Inputs to a CNN : Flattening specific tensor axis is often required with CNNs because we work with batches of inputs opposed to single inputs. A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flattened out so that the fully connected layer can accept them as the input. A flatten operation is a specific type of reshaping operation where by all of the axes are smooshed or squashed together. To flatten a tensor, we need to have at least two axes. This makes it so that we are starting with something that is not already flat. For example, in the MNIST dataset, we will look at an handwritten image of eight. This image has 2 distinct dimensions, height and width. The height and width are $18 \times 18$ respectively. These dimensions tell use that this is a cropped image becaue the MNIST dataset contains $28 \times 28$ images. Let‚Äôs see how these two axes of height and width are flattened out into a single axis of length 324 (c. f. 324 what we get when multiplying 18 with 18).  Flattening Specific Axes of a Tensor : Tensor inputs to a convolutional neural network typically have 4 axes, one for batch size, one for color channels, and one each for height and width. [[B,C,H,W]] Suppose we have the following three tensors: t1 = torch. tensor([  [1,1,1,1],  [1,1,1,1],  [1,1,1,1],  [1,1,1,1]])t2 = torch. tensor([  [2,2,2,2],  [2,2,2,2],  [2,2,2,2],  [2,2,2,2]])t3 = torch. tensor([  [3,3,3,3],  [3,3,3,3],  [3,3,3,3],  [3,3,3,3]])Each of these has a shape of $4 \times 4$, so we have three rank-2 tensors. For our purpose, we‚Äôll consider these to be three $4 \times 4$ images that we will use to create a batch that can be passed to a CNN. Batches are represented using a single tensor, so we‚Äôll need to combine these three tensors into a single larger tensor that has 3 axes instead of 2. t = torch. stack((t1, t2, t3))t. shape &gt; torch. Size([3,4,4])Here, we used the stack() method to concatenate our sequence of tensors along a new axis. Since we have three tensors along a new axis, we know that the length of this axis should be 3. At this point, we have a rank-3 tensor that contains a batch of three $4 \times 4$ images. All we need to do now to get this tensor into a form that a CNN expects is add an axis for the color channels. We basically have an implicit single color channel for each of these image tensors, so in practice, these would be grayscale images. torch. reshape(3,1,4,4)Notice how the additional axis of length 1 doesn‚Äôt change the number of elements in the tensor. This is because the product of the components values doesn‚Äôt change when we multiply by one. The first axis has 3 elements. Each element of the first axis represents an image. For each image, we have a single color channel on the channel axis. Each of these channels contain 4 arrays that contain 4 numbers or scalar components.  Flattening the Tensor Batch : Let‚Äôs see how to flatten images in this batch. Remember the whole batch is a single tensor that will be passed to the CNN, we don‚Äôt want to flatten the whole thing We only want to flatten the image tensors within the batch tensor. For example, if we do the following operations on t: t. flatten() &gt;&gt; tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,  2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])# this is the same operation as t. flatten()t. reshape(-1)What I want you to notice about this output is that we have flattened the entire batch, and this smashes all the batches together into a single axis. The flattened batch won‚Äôt work well inside our CNN because we need individual predictions for each image within our batch tensor, and now we have a flattened mess. The solution here, is to flatten each image while still maintaining the batch axis . This means we want to flatten only part of the tensor . We want to flatten the color channel axis with the height and width axes.  The Axes that Need to be Flattened: $[C,H,W]$ This can be done with PyTorch‚Äôs built in flatten() function. t. flatten(start_dim=1). shape&gt;&gt; torch. Size([3,16])t. flatten(start_dim=1)&gt;&gt; [  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])Notice how we specified the start_dim parameter. This tells the flatten() method which axis it should start the flatten operation. Now we have a rank-2 tensor with three single color channel images that have been flattened out into 16 pixels.  Flattening an RGB Image : If we flatten an RGB image, what happens to the color? Each color channel will be flattened first, then the flattened channels will be lined up side by side on a single axis of the tensor. For example, we build an RGB image tensor like the following code: r = torch. ones(1,2,2)g = torch. ones(1,2,2)+1b = torch. ones(1,2,2)+2img = torch. cat((r,g,b),dim=0)img. shape&gt;&gt; torch. Size([3,2,2])By flattening the image tensor, this is how it is going to look like. img. flatten(start_dim=0)&gt;&gt; tensor([1. , 1. , 1. , 1. , 2. , 2. , 2. , 2. , 3. , 3. , 3. , 3. ]) Broadcasting and Element-Wise Operations with PyTorch : Remember, all these rules apply to PyTorch Tensors! Python built-in types such as list will not behave this way. Element-Wise Operations : An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two elements are said to be corresponding if the two elements occupy the same position within the tensor. The position is determined by the indexes used to locate each element. Therefore, we can deduce that tensors must have the same shape in order to perform an element-wise operation.  Broadcasting Tensors : Broadcasting describes how tensors with different shapes are treated during element-wise operations. For example, suppose we have the following two tensors: t1 = torch. tensor([[1,1],[1,1]],dtype=torch. float32)t1. shape&gt;&gt; torch. Size([2,2])t2 = torch. tensor([2,4], dtype=torch. float32)t2. shape&gt;&gt; torch. Size([2])What will be the result of this two tensors‚Äô element-wise addition operation? Even though these two tensors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor t2 will be transformed via broadcasting to match the shape of the higher rank tensor t1, and the element-wise operation will be performed as usual. The concept of broadcasting is the key to understanding how this operation will be carried out. We can check the broadcast transformation using the broadcast_to() numpy function. np. broadcast_to(t2. numpy(), t1. shape)&gt;&gt; array([[2. , 4. ],    [2. , 4. ]], dtype=float32)t1 + t2&gt;&gt; tensor([[3. , 5. ],    [3. , 5. ]])When do we actually use broadcasting? We often need to use broadcasting when we are preprocessing and especially during normalization routines.  Element-Wise Operation Applies to Comparision and Functions : Comparison operations are also element-wise operations. For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element containing torch. bool value of True or False. It is also fine to assume that the function is applied to each element of the tensor.  there are other ways to refer to element-wise operations, such as component-wise or point-wise Argmax and Reduction Operations for Tensors : Now, we will focus in on the frequently used argmax() function, and we‚Äôll see how to access the data inside our tensors.  Tensor Reduction Operation : A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. Reduction operations allow us to perform operations on element within a single tensor. Let‚Äôs look at an example. Suppose we have the following rank-2 tensor: t = torch. tensor([[0,1,0],[2,0,2],[0,3,0]],dtype=torch. float32)t. sum()&gt;&gt; tensor(8. )t. numel()&gt;&gt; 9t. sum(). numel()&gt;&gt; 1The sum of our tensor‚Äôs scalar components is calculated using the sum() tensor method. The result of this call is a scalar-valued tensor . Since the number of elements have been reduced by the operation, we can conclude that the sum() method is a reduction operation. Other common reduction functions include t. sum(), t. prod(), t. mean() or t. std(). All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the tensor‚Äôs elements. Reduction operations in general allow us to compute aggregate values across data structures. But do reduction operations always reduce to a tensor with a single element? The answer is no. In fact, we often reduce specific axes at a time. This process is important.  References : deeplizard "
    }, {
    "id": 7,
    "url": "http://localhost:4000/_posts/2021-09-14-extrinsic.html",
    "title": "Dissecting the Camera Matrix (Part 2)",
    "body": " Overview of the Camera Calibration Parameters The Extrinsic Camera Matrix : The extrinsic matrix takes the form of a rigid transformation matrix: a $3 \times 3$ rotation matrix in the left-block, and $3 \times 1$ translation column-vector in the right. [\begin{bmatrix}\begin{array}{ccc|c} r_{11} &amp; r_{12} &amp; r_{13} &amp; t_1  r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2  r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3\end{array}\end{bmatrix}] It is common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation: [\begin{bmatrix}\begin{array}{ccc|c}1 &amp; 0 &amp; 0 &amp; t_1 0 &amp; 1 &amp; 0 &amp; t_2 0 &amp; 0 &amp; 0 &amp; t_3 \hline0 &amp; 0 &amp; 0 &amp; 1\end{array}\end{bmatrix} \times \begin{bmatrix}\begin{array}{ccc|c}r_{11} &amp; r_{12} &amp; r_{13} &amp; 0 r_{21} &amp; r_{22} &amp; r_{23} &amp; 0 r_{31} &amp; r_{32} &amp; r_{33} &amp; 0 \hline0 &amp; 0 &amp; 0 &amp; 1\end{array}\end{bmatrix}] The matrix describes how to transform points in world coordinates to camera coordinates. The important thing to remember about the extrinsic matrix is that it describes how the world is transformed relative to the camera . This if often counter-intuitive, because we usually want to specify how the camera is transformed relative to the world .  Building the Extrinsic Matrix from Camera Pose : Like I said before, it is often more natural to specify the camera‚Äôs pose directly rather than specifying how world points should transform to camera coordinates . Luckily, building an extrinsic camera matrix this way is easy: just build a rigid transformation matrix that describes the camera‚Äôs pose and then take its inverse . [\begin{bmatrix}\begin{array}{c|c}R &amp; t 0 &amp; 1 \end{array}\end{bmatrix} = \begin{bmatrix}\begin{array}{c|c}R_c &amp; C 0 &amp; 1 \end{array}\end{bmatrix}^{-1}] Let $C$ be a column vector describing the location of the camera-center in world coordinates, and let $R_c$ be the rotation matrix describing the camera‚Äôs orientation with respect to the world coordinate axes. Then extrinsic matrix is obtained by inverting the camera‚Äôs pose matrix.  Algebraically a rotation matrix in $n$-dimensions is a $n \times n$ special orthogonal matrix, i. e. an orthogonal matrix whose determinant is 1.  We can define matrix $R$ that rotates in the $xy$-Cartesian plane counterclock-wise through an angle $\theta$ about the origin of the Cartesian system as follows:$$R = \begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta &amp; \cos\theta\end{bmatrix}$$ The set of all rotation matrices form a group, known as the special orthogonal group. The inverse of a rotation matrix is its transpose, which is also a rotation matrix. $$\displaylines{R^T = R^{-1} \\det(R) = 1}$$ the extrinsic matrix is obtained by inverting the camera's pose matrix We here use the fact that the inverse of a rotation matrix is its transpose, and inverting a translation matrix simply negates the translation vector. Relationship between the extrinsic matrix parameters and the camera‚Äôs pose is straightforward: [\displaylines{R = R^T_c t = -RC}] References :  ksimek blog prateekvjoshi blog "
    }, {
    "id": 8,
    "url": "http://localhost:4000/_posts/2021-09-14-gan.html",
    "title": "All About Training GAN",
    "body": " Generative Adversarial Networks : Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. The discriminator should not be able to distinguish between real and fake images. Another way to look at the GAN setup is that the discriminator is trying to guide the generator by telling what real images look like. The two networks try to achieve what is called the Nash Equilibrium with respect to each other.  Training GANs : GAN networks are a dynamic system where the optimization process is seeking not a minimum, but a equilibrium between two forces. There are no good objective metrics for evaluating whether a GAN is performing well during training, e. g. reviewing the loss is not sufficient. Instead the best approach is to visually inspect the generated examples and use subjective evaluation. Other quantitative measures, such as Inception Score (IS) or Frechet Inception Distance (FID) rely on pretrained models with a specific set of object classes. They lack an upper bound (which means hypothetically the highest possible score is infinity).  Look at the Loss : In a discriminative model,the loss measures the accuracy of the prediction and we use it to monitor the progress of training. However, the loss in GAN measures how well we are doing compared with our opponent. Often, the generator cost increases but the image quality is actually improving. If you see the discriminator loss rapidly approaching, there is probably no chance of recovery and it is time to change something.  Look at the Gradients : Monitor the gradients along with the losses in the networks. These can give you a good idea about the progress of training and can even help in debugging if things are not really working well. Ideally, the generator should receive large gradients early in the training because it needs to learn how to generate real-looking data. The discriminator on the other hand does not always get large gradients early on, because it can easily distinguish real and fake images. If the gradients at the layer of generator are too small, learning might be slow or not happening at all. The generator should get large gradients early on and the discriminator getting consistently high gradients at the top layer once the generator has been trained enough.  Detecting GAN Failure Modes : The reason why GANs are difficult to train is that both generator and the discriminator are trained simultaneously in a zero-sum game. This means that improvements to one model come at the expense of the other model. The goal of training two models involves finding a point of equilibrium between the two competing concerns. It also means that everytime the parameters of one model are updated the nature of the optimization problem that is being solved is updated as well. The technical challenge of training two competing neural networks at the same time is that they can fail to converge.  Convergence Failure The fact that GANs are composed by two networks, and each of them has its loss function leads to GANs unstability. In GAN architecture, the discriminator tries to minimize a cross-entropy while the generator tries to maximize it. When discriminator confidence is high and the discriminator starts to reject the samples that are produced by the generator, generator's gradient vanishes. This scenario happens when the generator score reaches near zero and the discriminator score reaches near one. The discriminator is overpowering the generator. If the score does not recover from these values for many iterations, it is better to stop training.  Mode Collapse Mode collapse is when the GAN produces a small variety of images with many duplicates. This happens when the generator is unable to learn a rich feature representation because it learns to associate similar outputs to multiple different inputs. The most promising way to check for mode collapse is to inspect the generated images. If there is little diversity in the output and some of them are almost identical, there is likely mode collapse. If you observe this happening, you should try to increase the ability of the generator to create more diverse outputs or impair the discriminator by randomly giving false labels to real images. Another type of behavior you should look out for is when the generator oscillates between generating specific examples in the domain. They progress from generating one kind of sample to generating another kind of sample without eventually reaching equilibrium.  Diminisheed Gradient This situation happens when the discriminator gets too successful that the generator gradient vanishes and learns nothing. Lessons I Learned : Use a batch size smaller than or equal to 64. In my experience, using bigger batch sizes often hurt the performance. I suspect it fuels the problem of discriminator getting too good at discriminating the real and fake images, since large batch size means providing a lot of examples to train on.  Add noise to both real and synthetic data.  It is well known that making the training of discriminator more difficult is beneficial for the overall stability. Adding noise increases the complexity of the discriminator training and stabilizes the data distribution of the two competing networks.  Use Label Smoothing If the label for real images is set to 1, change it to a lower value like 0. 9. This solution discourages the discriminator from being overconfident.  Different learning rates for the generator and discriminator a. k. a. Two Time-Scale Update Rule In my experience, choosing a higher learning rate for the discriminator(i. e. 0. 0004) and a lower one(i. e. 0. 0001) for the generator works well in practice. I guess the reason is that the generator has to make small steps to fool the discriminator so it does not choose fast but not precise solutions to win the adversarial game.  Use some kind of normalization method For me, applying Spectral Normalization, a particular kind of normalization applied on the convolutional kernels, greatly helped the stability of training. I learned that hyperparameter tuning takes a lot of time and patience especially for training GANs.  References : &lt;a=href=  &gt; TheAILearner &lt;/a&gt;"
    }, {
    "id": 9,
    "url": "http://localhost:4000/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 10,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": "Made by Seri @porfolio. "
    }, {
    "id": 11,
    "url": "http://localhost:4000/author-seri.html",
    "title": "Sal",
    "body": "                        Sal /span&gt;&lt;/h2&gt;        https://sites. google. com/snu. ac. kr/sally20921porfolio         Hi, I am Seri, the author of this blog.       &lt;/div&gt;                        &lt;/div&gt;    Posts by Sal:                   		All About Training GAN	: 		  Generative Adversarial Networks Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. . . 	 			In 				computer vision, 								Sep 14, 2021						            		Dissecting the Camera Matrix (Part 2)	: 		  This is the second part of our journey to master the camera matrix. In this blog post, we will study the extrinsic camera parameters. Extrinsic matrix describes the camera's location . . . 	 			In 				computer vision, 								Sep 14, 2021						            		Image Processing: the Basics	: 		  References &lt;a=href=  &gt; TheAILearner &lt;/a&gt;	 			In 				computer vision, 								Sep 14, 2021						            		Advanced PyTorch: Things You Didn't Know	: 		  This blog post is for those who know the basics of PyTorch but want to go a step further. We will be diving into principles and applications of deep learning via PyTorch. 	 			In 				PyTorch, 								Sep 12, 2021						            		Guide to Linear Algebra (Part 1)	: 		  This article goes through the fundamentals of linear algebra. Linear algebra is the branch of mathematics concerning linear equations and their representations in vector spaces. This . . . 	 			In 				linear algebra, 								Sep 09, 2021						            		Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision	: 		  Introduction In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is . . . 	 			In 				computer vision, 								Sep 09, 2021						            		Depth Estimation - An Introduction	: 		  Paradigms for 3D Images Representation over a Plane As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the d. . . 	 			In 				computer vision, 								Sep 09, 2021						            		3D Packing for Self-Supervised Depth Estimation	: 		  Self-Supervised Scale-Aware SfM In self-supervised monocular SfM training, we aim to learn: &lt;ul&gt;&lt;li&gt; a monocular depth model $f_D = I \rightarrow D$ that predicts the sca. . . 	 			In 				paper, 								Sep 09, 2021						            		Batch Normalization and Group Normalization	: 		  Batch normalization is used in most state-of-the-art computer vision techniques to stabilize training, but it also suffers from drawbacks. Group normalization can be an awesome altern. . . 	 			In 				deep learning, 								Sep 08, 2021						            		An Intuitive Overview of Linear Algebra Fundamentals	: 		  Introduction Why Learn Linear Algebra? This write up is an overview of some of the linear algebra fundamentals. It focuses on providing an intuitive/geometric review of some of the . . . 	 			In 				linear algebra, 								Sep 08, 2021						            		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, SfM, optical flow, etc. 	 			In 				computer vision, 								Sep 05, 2021						            		Dissecting the Camera Matrix (Part 1)	: 		  This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices. We will untangle the issues that arise due to different coordinate conventions. 	 			In 				computer vision, 								Sep 05, 2021						            		Dissecting the Camera Matrix (Part 2)	: 		  This is the second part of our journey to master the camera matrix. In this blog post, we will study the intrinsic camera parameters. Intrinsic camera matrix can be examined with two . . . 	 			In 				computer vision, 								Sep 05, 2021						            		Inverse Projection Transformation	: 		  When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a. . . 	 			In 				computer vision, 								Sep 03, 2021						            		Depth Estimation: Basics and Intuition	: 		  IntroductionIn computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting. . . 	 			In 				computer vision, 								Sep 02, 2021						            		3D Rigid Body Motion (Part 2)	: 		  Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f. . . 	 			In 				computer vision, 								Sep 01, 2021						            		3D Rigid Body Motion (Part 1)	: 		  The goal of this article is to introduce the rigid body geometry in 3-dimensional spaace: rotation matrix, transformation matrix, quaternion and Euler angle. 	 			In 				computer vision, 								Aug 31, 2021						            		Homogeneous Coordinates and Projective Geometry	: 		  In this article, I'm going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. We need projective geometry for that. 	 			In 				computer vision, 								Aug 31, 2021						        &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;: "
    }, {
    "id": 12,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "Authors:                                             Seri :       (View Posts)      Hi, I am Seri, the author of this blog.                           &nbsp;       &nbsp;                                      "
    }, {
    "id": 13,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories               computer vision:                                  		All About Training GAN	: 		  Generative Adversarial Networks Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. . . 	 			In 				computer vision, 								Sep 14, 2021						                                 		Dissecting the Camera Matrix (Part 2)	: 		  This is the second part of our journey to master the camera matrix. In this blog post, we will study the extrinsic camera parameters. Extrinsic matrix describes the camera's location . . . 	 			In 				computer vision, 								Sep 14, 2021						                                 		Image Processing: the Basics	: 		  References &lt;a=href=  &gt; TheAILearner &lt;/a&gt;	 			In 				computer vision, 								Sep 14, 2021						                                 		Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision	: 		  Introduction In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is . . . 	 			In 				computer vision, 								Sep 09, 2021						                                 		Depth Estimation - An Introduction	: 		  Paradigms for 3D Images Representation over a Plane As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the d. . . 	 			In 				computer vision, 								Sep 09, 2021						                                 		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, SfM, optical flow, etc. 	 			In 				computer vision, 								Sep 05, 2021						                                 		Dissecting the Camera Matrix (Part 1)	: 		  This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices. We will untangle the issues that arise due to different coordinate conventions. 	 			In 				computer vision, 								Sep 05, 2021						                                 		Dissecting the Camera Matrix (Part 2)	: 		  This is the second part of our journey to master the camera matrix. In this blog post, we will study the intrinsic camera parameters. Intrinsic camera matrix can be examined with two . . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Inverse Projection Transformation	: 		  When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a. . . 	 			In 				computer vision, 								Sep 03, 2021						                                 		Depth Estimation: Basics and Intuition	: 		  IntroductionIn computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting. . . 	 			In 				computer vision, 								Sep 02, 2021						                                 		3D Rigid Body Motion (Part 2)	: 		  Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f. . . 	 			In 				computer vision, 								Sep 01, 2021						                                 		3D Rigid Body Motion (Part 1)	: 		  The goal of this article is to introduce the rigid body geometry in 3-dimensional spaace: rotation matrix, transformation matrix, quaternion and Euler angle. 	 			In 				computer vision, 								Aug 31, 2021						                                 		Homogeneous Coordinates and Projective Geometry	: 		  In this article, I'm going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. We need projective geometry for that. 	 			In 				computer vision, 								Aug 31, 2021						                              linear algebra:                                  		Guide to Linear Algebra (Part 1)	: 		  This article goes through the fundamentals of linear algebra. Linear algebra is the branch of mathematics concerning linear equations and their representations in vector spaces. This . . . 	 			In 				linear algebra, 								Sep 09, 2021						                                 		An Intuitive Overview of Linear Algebra Fundamentals	: 		  Introduction Why Learn Linear Algebra? This write up is an overview of some of the linear algebra fundamentals. It focuses on providing an intuitive/geometric review of some of the . . . 	 			In 				linear algebra, 								Sep 08, 2021						                              deep learning:                                  		Batch Normalization and Group Normalization	: 		  Batch normalization is used in most state-of-the-art computer vision techniques to stabilize training, but it also suffers from drawbacks. Group normalization can be an awesome altern. . . 	 			In 				deep learning, 								Sep 08, 2021						                              paper:                                  		3D Packing for Self-Supervised Depth Estimation	: 		  Self-Supervised Scale-Aware SfM In self-supervised monocular SfM training, we aim to learn: &lt;ul&gt;&lt;li&gt; a monocular depth model $f_D = I \rightarrow D$ that predicts the sca. . . 	 			In 				paper, 								Sep 09, 2021						                              PyTorch:                                  		Advanced PyTorch: Things You Didn't Know	: 		  This blog post is for those who know the basics of PyTorch but want to go a step further. We will be diving into principles and applications of deep learning via PyTorch. 	 			In 				PyTorch, 								Sep 12, 2021						                                             Featured:    				                                          All About Training GAN                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 2)                          In                     computer vision,                                                                                           Image Processing: the Basics                          In                     computer vision,                                                                                           Advanced PyTorch: Things You Didn't Know                          In                     PyTorch,                                                                                           Guide to Linear Algebra (Part 1)                          In                     linear algebra,                                                                                           Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision                          In                     computer vision,                                                                                           Depth Estimation - An Introduction                          In                     computer vision,                                                                                           3D Packing for Self-Supervised Depth Estimation                          In                     paper,                                                                                           Batch Normalization and Group Normalization                          In                     deep learning,                                                                                           An Intuitive Overview of Linear Algebra Fundamentals                          In                     linear algebra,                                                                                           Depth from Disparity                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 1)                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 2)                          In                     computer vision,                                                                                           Inverse Projection Transformation                          In                     computer vision,                                                                                           Depth Estimation: Basics and Intuition                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 2)                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 1)                          In                     computer vision,                                                                                           Homogeneous Coordinates and Projective Geometry                          In                     computer vision,                                                                   "
    }, {
    "id": 14,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to Seri Lee Blog. We will reply as soon as possible!   "
    }, {
    "id": 15,
    "url": "http://localhost:4000/",
    "title": "Mundana Free Jekyll Theme",
    "body": "                                  All About Training GAN  :       Generative Adversarial Networks Ultimately, if everything goes well, the generator learns the true distribution of the training data. . .               In                 computer vision,                                        Sep 14, 2021                                                                                                                             Dissecting the Camera Matrix (Part 2)          :                       In                         computer vision,                                                                  Sep 14, 2021                                                                                                                                     Image Processing: the Basics          :                       In                         computer vision,                                                                  Sep 14, 2021                                                                                                                                    Advanced PyTorch: Things You Didn't Know          :                       In                         PyTorch,                                                                  Sep 12, 2021                                                               Homogeneous Coordinates and Projective Geometry                  In this article, I'm going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. We need projective geometry . . .                 Read More            	                        All Stories:                   		All About Training GAN	: 		  Generative Adversarial Networks Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. . . 	 			In 				computer vision, 								Sep 14, 2021						                  		Dissecting the Camera Matrix (Part 2)	: 		  This is the second part of our journey to master the camera matrix. In this blog post, we will study the extrinsic camera parameters. Extrinsic matrix describes the camera's location . . . 	 			In 				computer vision, 								Sep 14, 2021						                  		Image Processing: the Basics	: 		  References &lt;a=href=  &gt; TheAILearner &lt;/a&gt;	 			In 				computer vision, 								Sep 14, 2021						                  		Advanced PyTorch: Things You Didn't Know	: 		  This blog post is for those who know the basics of PyTorch but want to go a step further. We will be diving into principles and applications of deep learning via PyTorch. 	 			In 				PyTorch, 								Sep 12, 2021						                  		Guide to Linear Algebra (Part 1)	: 		  This article goes through the fundamentals of linear algebra. Linear algebra is the branch of mathematics concerning linear equations and their representations in vector spaces. This . . . 	 			In 				linear algebra, 								Sep 09, 2021						                  		Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision	: 		  Introduction In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is . . . 	 			In 				computer vision, 								Sep 09, 2021						                  		Depth Estimation - An Introduction	: 		  Paradigms for 3D Images Representation over a Plane As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the d. . . 	 			In 				computer vision, 								Sep 09, 2021						                  		3D Packing for Self-Supervised Depth Estimation	: 		  Self-Supervised Scale-Aware SfM In self-supervised monocular SfM training, we aim to learn: &lt;ul&gt;&lt;li&gt; a monocular depth model $f_D = I \rightarrow D$ that predicts the sca. . . 	 			In 				paper, 								Sep 09, 2021						                  		Batch Normalization and Group Normalization	: 		  Batch normalization is used in most state-of-the-art computer vision techniques to stabilize training, but it also suffers from drawbacks. Group normalization can be an awesome altern. . . 	 			In 				deep learning, 								Sep 08, 2021						                  		An Intuitive Overview of Linear Algebra Fundamentals	: 		  Introduction Why Learn Linear Algebra? This write up is an overview of some of the linear algebra fundamentals. It focuses on providing an intuitive/geometric review of some of the . . . 	 			In 				linear algebra, 								Sep 08, 2021						                                                &laquo;                              1                               2                              Next &raquo;                                          Featured:    				                                          All About Training GAN                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 2)                          In                     computer vision,                                                                                           Image Processing: the Basics                          In                     computer vision,                                                                                           Advanced PyTorch: Things You Didn't Know                          In                     PyTorch,                                                                                           Guide to Linear Algebra (Part 1)                          In                     linear algebra,                                                                                           Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision                          In                     computer vision,                                                                                           Depth Estimation - An Introduction                          In                     computer vision,                                                                                           3D Packing for Self-Supervised Depth Estimation                          In                     paper,                                                                                           Batch Normalization and Group Normalization                          In                     deep learning,                                                                                           An Intuitive Overview of Linear Algebra Fundamentals                          In                     linear algebra,                                                                                           Depth from Disparity                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 1)                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 2)                          In                     computer vision,                                                                                           Inverse Projection Transformation                          In                     computer vision,                                                                                           Depth Estimation: Basics and Intuition                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 2)                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 1)                          In                     computer vision,                                                                                           Homogeneous Coordinates and Projective Geometry                          In                     computer vision,                                                               &lt;/div&gt; "
    }, {
    "id": 16,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "‚Äù{{site. name}}‚Äù takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor‚Äôs preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google‚Äôs cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 17,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 18,
    "url": "http://localhost:4000/page2/",
    "title": "Mundana Free Jekyll Theme",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 19,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ ‚Äúsitemap. xml‚Äù   absolute_url }}   "
    }, {
    "id": 20,
    "url": "http://localhost:4000/gan.html",
    "title": "All About Training GAN",
    "body": "2021/09/14 -  Generative Adversarial Networks : Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. The discriminator should not be able to distinguish between real and fake images. Another way to look at the GAN setup is that the discriminator is trying to guide the generator by telling what real images look like. The two networks try to achieve what is called the Nash Equilibrium with respect to each other.  Training GANs : GAN networks are a dynamic system where the optimization process is seeking not a minimum, but a equilibrium between two forces. There are no good objective metrics for evaluating whether a GAN is performing well during training, e. g. reviewing the loss is not sufficient. Instead the best approach is to visually inspect the generated examples and use subjective evaluation. Other quantitative measures, such as Inception Score (IS) or Frechet Inception Distance (FID) rely on pretrained models with a specific set of object classes. They lack an upper bound (which means hypothetically the highest possible score is infinity).  Look at the Loss : In a discriminative model,the loss measures the accuracy of the prediction and we use it to monitor the progress of training. However, the loss in GAN measures how well we are doing compared with our opponent. Often, the generator cost increases but the image quality is actually improving. If you see the discriminator loss rapidly approaching, there is probably no chance of recovery and it is time to change something.  Look at the Gradients : Monitor the gradients along with the losses in the networks. These can give you a good idea about the progress of training and can even help in debugging if things are not really working well. Ideally, the generator should receive large gradients early in the training because it needs to learn how to generate real-looking data. The discriminator on the other hand does not always get large gradients early on, because it can easily distinguish real and fake images. If the gradients at the layer of generator are too small, learning might be slow or not happening at all. The generator should get large gradients early on and the discriminator getting consistently high gradients at the top layer once the generator has been trained enough.  Detecting GAN Failure Modes : The reason why GANs are difficult to train is that both generator and the discriminator are trained simultaneously in a zero-sum game. This means that improvements to one model come at the expense of the other model. The goal of training two models involves finding a point of equilibrium between the two competing concerns. It also means that everytime the parameters of one model are updated the nature of the optimization problem that is being solved is updated as well. The technical challenge of training two competing neural networks at the same time is that they can fail to converge.  Convergence Failure The fact that GANs are composed by two networks, and each of them has its loss function leads to GANs unstability. In GAN architecture, the discriminator tries to minimize a cross-entropy while the generator tries to maximize it. When discriminator confidence is high and the discriminator starts to reject the samples that are produced by the generator, generator's gradient vanishes. This scenario happens when the generator score reaches near zero and the discriminator score reaches near one. The discriminator is overpowering the generator. If the score does not recover from these values for many iterations, it is better to stop training.  Mode Collapse Mode collapse is when the GAN produces a small variety of images with many duplicates. This happens when the generator is unable to learn a rich feature representation because it learns to associate similar outputs to multiple different inputs. The most promising way to check for mode collapse is to inspect the generated images. If there is little diversity in the output and some of them are almost identical, there is likely mode collapse. If you observe this happening, you should try to increase the ability of the generator to create more diverse outputs or impair the discriminator by randomly giving false labels to real images. Another type of behavior you should look out for is when the generator oscillates between generating specific examples in the domain. They progress from generating one kind of sample to generating another kind of sample without eventually reaching equilibrium.  Diminisheed Gradient This situation happens when the discriminator gets too successful that the generator gradient vanishes and learns nothing. Lessons I Learned : Use a batch size smaller than or equal to 64. In my experience, using bigger batch sizes often hurt the performance. I suspect it fuels the problem of discriminator getting too good at discriminating the real and fake images, since large batch size means providing a lot of examples to train on.  Add noise to both real and synthetic data.  It is well known that making the training of discriminator more difficult is beneficial for the overall stability. Adding noise increases the complexity of the discriminator training and stabilizes the data distribution of the two competing networks.  Use Label Smoothing If the label for real images is set to 1, change it to a lower value like 0. 9. This solution discourages the discriminator from being overconfident.  Different learning rates for the generator and discriminator a. k. a. Two Time-Scale Update Rule In my experience, choosing a higher learning rate for the discriminator(i. e. 0. 0004) and a lower one(i. e. 0. 0001) for the generator works well in practice. I guess the reason is that the generator has to make small steps to fool the discriminator so it does not choose fast but not precise solutions to win the adversarial game.  Use some kind of normalization method For me, applying Spectral Normalization, a particular kind of normalization applied on the convolutional kernels, greatly helped the stability of training. I learned that hyperparameter tuning takes a lot of time and patience especially for training GANs.  References : &lt;a=href=  &gt; TheAILearner &lt;/a&gt;"
    }, {
    "id": 21,
    "url": "http://localhost:4000/extrinsic.html",
    "title": "Dissecting the Camera Matrix (Part 2)",
    "body": "2021/09/14 -  Overview of the Camera Calibration Parameters The Extrinsic Camera Matrix : The extrinsic matrix takes the form of a rigid transformation matrix: a $3 \times 3$ rotation matrix in the left-block, and $3 \times 1$ translation column-vector in the right. [\begin{bmatrix}\begin{array}{ccc|c} r_{11} &amp; r_{12} &amp; r_{13} &amp; t_1  r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2  r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3\end{array}\end{bmatrix}] It is common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation: [\begin{bmatrix}\begin{array}{ccc|c}1 &amp; 0 &amp; 0 &amp; t_1 0 &amp; 1 &amp; 0 &amp; t_2 0 &amp; 0 &amp; 0 &amp; t_3 \hline0 &amp; 0 &amp; 0 &amp; 1\end{array}\end{bmatrix} \times \begin{bmatrix}\begin{array}{ccc|c}r_{11} &amp; r_{12} &amp; r_{13} &amp; 0 r_{21} &amp; r_{22} &amp; r_{23} &amp; 0 r_{31} &amp; r_{32} &amp; r_{33} &amp; 0 \hline0 &amp; 0 &amp; 0 &amp; 1\end{array}\end{bmatrix}] The matrix describes how to transform points in world coordinates to camera coordinates. The important thing to remember about the extrinsic matrix is that it describes how the world is transformed relative to the camera . This if often counter-intuitive, because we usually want to specify how the camera is transformed relative to the world .  Building the Extrinsic Matrix from Camera Pose : Like I said before, it is often more natural to specify the camera‚Äôs pose directly rather than specifying how world points should transform to camera coordinates . Luckily, building an extrinsic camera matrix this way is easy: just build a rigid transformation matrix that describes the camera‚Äôs pose and then take its inverse . [\begin{bmatrix}\begin{array}{c|c}R &amp; t 0 &amp; 1 \end{array}\end{bmatrix} = \begin{bmatrix}\begin{array}{c|c}R_c &amp; C 0 &amp; 1 \end{array}\end{bmatrix}^{-1}] Let $C$ be a column vector describing the location of the camera-center in world coordinates, and let $R_c$ be the rotation matrix describing the camera‚Äôs orientation with respect to the world coordinate axes. Then extrinsic matrix is obtained by inverting the camera‚Äôs pose matrix.  Algebraically a rotation matrix in $n$-dimensions is a $n \times n$ special orthogonal matrix, i. e. an orthogonal matrix whose determinant is 1.  We can define matrix $R$ that rotates in the $xy$-Cartesian plane counterclock-wise through an angle $\theta$ about the origin of the Cartesian system as follows:$$R = \begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta &amp; \cos\theta\end{bmatrix}$$ The set of all rotation matrices form a group, known as the special orthogonal group. The inverse of a rotation matrix is its transpose, which is also a rotation matrix. $$\displaylines{R^T = R^{-1} \\det(R) = 1}$$ the extrinsic matrix is obtained by inverting the camera's pose matrix We here use the fact that the inverse of a rotation matrix is its transpose, and inverting a translation matrix simply negates the translation vector. Relationship between the extrinsic matrix parameters and the camera‚Äôs pose is straightforward: [\displaylines{R = R^T_c t = -RC}] References :  ksimek blog prateekvjoshi blog "
    }, {
    "id": 22,
    "url": "http://localhost:4000/template.html",
    "title": "Image Processing: the Basics",
    "body": "2021/09/14 -  References : &lt;a=href=  &gt; TheAILearner &lt;/a&gt;"
    }, {
    "id": 23,
    "url": "http://localhost:4000/pytorch.html",
    "title": "Advanced PyTorch: Things You Didn't Know",
    "body": "2021/09/12 -  Flatten Operation for a Batch of Image Inputs to a CNN : Flattening specific tensor axis is often required with CNNs because we work with batches of inputs opposed to single inputs. A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flattened out so that the fully connected layer can accept them as the input. A flatten operation is a specific type of reshaping operation where by all of the axes are smooshed or squashed together. To flatten a tensor, we need to have at least two axes. This makes it so that we are starting with something that is not already flat. For example, in the MNIST dataset, we will look at an handwritten image of eight. This image has 2 distinct dimensions, height and width. The height and width are $18 \times 18$ respectively. These dimensions tell use that this is a cropped image becaue the MNIST dataset contains $28 \times 28$ images. Let‚Äôs see how these two axes of height and width are flattened out into a single axis of length 324 (c. f. 324 what we get when multiplying 18 with 18).  Flattening Specific Axes of a Tensor : Tensor inputs to a convolutional neural network typically have 4 axes, one for batch size, one for color channels, and one each for height and width. [[B,C,H,W]] Suppose we have the following three tensors: t1 = torch. tensor([  [1,1,1,1],  [1,1,1,1],  [1,1,1,1],  [1,1,1,1]])t2 = torch. tensor([  [2,2,2,2],  [2,2,2,2],  [2,2,2,2],  [2,2,2,2]])t3 = torch. tensor([  [3,3,3,3],  [3,3,3,3],  [3,3,3,3],  [3,3,3,3]])Each of these has a shape of $4 \times 4$, so we have three rank-2 tensors. For our purpose, we‚Äôll consider these to be three $4 \times 4$ images that we will use to create a batch that can be passed to a CNN. Batches are represented using a single tensor, so we‚Äôll need to combine these three tensors into a single larger tensor that has 3 axes instead of 2. t = torch. stack((t1, t2, t3))t. shape &gt; torch. Size([3,4,4])Here, we used the stack() method to concatenate our sequence of tensors along a new axis. Since we have three tensors along a new axis, we know that the length of this axis should be 3. At this point, we have a rank-3 tensor that contains a batch of three $4 \times 4$ images. All we need to do now to get this tensor into a form that a CNN expects is add an axis for the color channels. We basically have an implicit single color channel for each of these image tensors, so in practice, these would be grayscale images. torch. reshape(3,1,4,4)Notice how the additional axis of length 1 doesn‚Äôt change the number of elements in the tensor. This is because the product of the components values doesn‚Äôt change when we multiply by one. The first axis has 3 elements. Each element of the first axis represents an image. For each image, we have a single color channel on the channel axis. Each of these channels contain 4 arrays that contain 4 numbers or scalar components.  Flattening the Tensor Batch : Let‚Äôs see how to flatten images in this batch. Remember the whole batch is a single tensor that will be passed to the CNN, we don‚Äôt want to flatten the whole thing We only want to flatten the image tensors within the batch tensor. For example, if we do the following operations on t: t. flatten() &gt;&gt; tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,  2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])# this is the same operation as t. flatten()t. reshape(-1)What I want you to notice about this output is that we have flattened the entire batch, and this smashes all the batches together into a single axis. The flattened batch won‚Äôt work well inside our CNN because we need individual predictions for each image within our batch tensor, and now we have a flattened mess. The solution here, is to flatten each image while still maintaining the batch axis . This means we want to flatten only part of the tensor . We want to flatten the color channel axis with the height and width axes.  The Axes that Need to be Flattened: $[C,H,W]$ This can be done with PyTorch‚Äôs built in flatten() function. t. flatten(start_dim=1). shape&gt;&gt; torch. Size([3,16])t. flatten(start_dim=1)&gt;&gt; [  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])Notice how we specified the start_dim parameter. This tells the flatten() method which axis it should start the flatten operation. Now we have a rank-2 tensor with three single color channel images that have been flattened out into 16 pixels.  Flattening an RGB Image : If we flatten an RGB image, what happens to the color? Each color channel will be flattened first, then the flattened channels will be lined up side by side on a single axis of the tensor. For example, we build an RGB image tensor like the following code: r = torch. ones(1,2,2)g = torch. ones(1,2,2)+1b = torch. ones(1,2,2)+2img = torch. cat((r,g,b),dim=0)img. shape&gt;&gt; torch. Size([3,2,2])By flattening the image tensor, this is how it is going to look like. img. flatten(start_dim=0)&gt;&gt; tensor([1. , 1. , 1. , 1. , 2. , 2. , 2. , 2. , 3. , 3. , 3. , 3. ]) Broadcasting and Element-Wise Operations with PyTorch : Remember, all these rules apply to PyTorch Tensors! Python built-in types such as list will not behave this way. Element-Wise Operations : An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two elements are said to be corresponding if the two elements occupy the same position within the tensor. The position is determined by the indexes used to locate each element. Therefore, we can deduce that tensors must have the same shape in order to perform an element-wise operation.  Broadcasting Tensors : Broadcasting describes how tensors with different shapes are treated during element-wise operations. For example, suppose we have the following two tensors: t1 = torch. tensor([[1,1],[1,1]],dtype=torch. float32)t1. shape&gt;&gt; torch. Size([2,2])t2 = torch. tensor([2,4], dtype=torch. float32)t2. shape&gt;&gt; torch. Size([2])What will be the result of this two tensors‚Äô element-wise addition operation? Even though these two tensors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor t2 will be transformed via broadcasting to match the shape of the higher rank tensor t1, and the element-wise operation will be performed as usual. The concept of broadcasting is the key to understanding how this operation will be carried out. We can check the broadcast transformation using the broadcast_to() numpy function. np. broadcast_to(t2. numpy(), t1. shape)&gt;&gt; array([[2. , 4. ],    [2. , 4. ]], dtype=float32)t1 + t2&gt;&gt; tensor([[3. , 5. ],    [3. , 5. ]])When do we actually use broadcasting? We often need to use broadcasting when we are preprocessing and especially during normalization routines.  Element-Wise Operation Applies to Comparision and Functions : Comparison operations are also element-wise operations. For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element containing torch. bool value of True or False. It is also fine to assume that the function is applied to each element of the tensor.  there are other ways to refer to element-wise operations, such as component-wise or point-wise Argmax and Reduction Operations for Tensors : Now, we will focus in on the frequently used argmax() function, and we‚Äôll see how to access the data inside our tensors.  Tensor Reduction Operation : A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. Reduction operations allow us to perform operations on element within a single tensor. Let‚Äôs look at an example. Suppose we have the following rank-2 tensor: t = torch. tensor([[0,1,0],[2,0,2],[0,3,0]],dtype=torch. float32)t. sum()&gt;&gt; tensor(8. )t. numel()&gt;&gt; 9t. sum(). numel()&gt;&gt; 1The sum of our tensor‚Äôs scalar components is calculated using the sum() tensor method. The result of this call is a scalar-valued tensor . Since the number of elements have been reduced by the operation, we can conclude that the sum() method is a reduction operation. Other common reduction functions include t. sum(), t. prod(), t. mean() or t. std(). All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the tensor‚Äôs elements. Reduction operations in general allow us to compute aggregate values across data structures. But do reduction operations always reduce to a tensor with a single element? The answer is no. In fact, we often reduce specific axes at a time. This process is important.  References : deeplizard "
    }, {
    "id": 24,
    "url": "http://localhost:4000/no-bullshit.html",
    "title": "Guide to Linear Algebra (Part 1)",
    "body": "2021/09/09 -  Computational Linear Algebra : This chapter covers the computational aspects of performing matrix calculations. Understanding matrix computations is important because all later chapters depend on them. Suppose we‚Äôre given a huge matrix $ A \in R^{n \times n} $ with $ n=1000 $. Hidden behind the innocent-looking mathematical notation of the matrix inverse $A^{-1}$, the matrix product $AA$, and the matrix determinant $ | A |$, lie monster coputations involving all the $1000 \times 1000 = 1$ million entries of the matrix $A$. Millions of arithmetic operations must be performed, so I hope you have at least a thousand pencil ready! Okay, calm down. I won‚Äôt actually make you calculate millions of arithmetic operations. In fact, to learn linear algebra, it is sufficient to know how to carry out calculations with $3 \times 3$ and $4 \times 4$ matrices. Even for such moderately sized matrices, computing products, inverses, and determinants by hand are serious computational tasks. If you‚Äôre ever required to take a linear algebra final exam, you need to make sure you can do these calculations quickly. Even if no exam looms in your imminent future, it‚Äôs important to practice matrix operations by hand to get a feel for them. This chapter will introduce you to the following computational tasks involving matrices: Gauss-Jordan elimination Suppose we're trying to solve two equations in two unknowns $x$ and $y$:$$ \displaylines{ax+by = c \\\dx+ ey= f}$$If we add $\alpha$\times the first equation to the second equation, we obtain an equivalent system of equations:$$\displaylines{ax + by = c \\\(d + \alpha a)x + (e + \alpha b)y = f + \alpha c}$$This is called a row operation : we added $\alpha$-times the first row to the second row. Row operations change the coefficient of the system of equations, but leave the solution unchanged. Gauss-Jordan elimination is a systematic procedure for solving systems of linear equations using row operations.  Matrix product The product $AB$ between matrices $A \in \mathbb{R}^{m \times l}$ and $B \in \mathbb{R}^{l \times n}$ is the matrix $C \in \mathbb{R}^{m \times n}$ whose coefficients $c_{ij}$ are defined by the formula $c_{ij} = \sum_{k=1}^{l}a_{ik}b_{kj}$ for all $i \in \lbrack 1, \dots, m \rbrack $ and $j \in \lbrack 1, \dots, n \rbrack $. We'll soon unpack this formula and learn about its intuitive interpretation: that computing $C = AB$ is computing all the dot products between the rows of $A$ and the columns of $B$.  Determinant The determinant of a matrix $A$, denoted $|A|$ is an operation that gives us useful information about the linear independence of the rows of the matrix. The determinant is connected to many notions of linear algebra: linear independence, geometry of vectors, solving systems of equations, and matrix invertibility. We'll soon discuss these aspects.  Matrix inverse We'll build upon our knowledge of Gauss-Jordan elimination, matrix products, and determinants to derive three different procedures for computing the matrix inverse $A^{-1}$. Reduced Row Echelon Form : In this section, we‚Äôll learn to solve systems of linear equations using the Gauss-Jordan elimination procedure. A system of equations can be represented as a matrix of coefficients. The Gauss-Jordan elimination procedure converts any matrix into its  reduced row echelon form (RREF) . We can easily find the solution (or solutions of the system of equations from the RREF. Listen up: the material covered in this section requires your full on, caffeinated attention, as the procedures you‚Äôll learn are somewhat tedious. Gauss-Jordan elimination involves many repetitive mathematical manipulations of arrays of numbers. It‚Äôs important you hang in there and follow through the step-by-step manipulations, as well as verify each step I present on your own with pen and paper.  Solving Equations : Suppose you‚Äôre asked to solve the following system of equations: [\displaylines{1x_1 + 2x_2 = 5 \3x_1 + 9x_2 = 21}] The standard approach is to use one of the equation-solving tricks we learned to combine the equations and find the values of the two unknowns $x_1$ and $x_2$. Observe that the names of the two unknowns are irrelevant to the solution of the system of equations. Indeed, the solution $(x_1, x_2)$ to the above system of equations is the same as the solution $(s,t)$ to the system of equations [\displaylines{1s+ 2t = 5 \3s+ 9t = 21}] The important parts of a system of linear equations are the coefficients in front of the variables and the constants on the right-hand side of each equation.  Augmented Matrix : The system of linear equations can be written as an augmented matrix : [\begin{pmatrix} 1 &amp; 2 &amp;\bigm | &amp; 5 \        3 &amp; 9 &amp;\bigm | &amp; 21\end{pmatrix}] The first column corresponds to the coefficients of the first variable, the second column is for the second variable, and the last column corresponds to the constants of the right-hand side. It is customary to draw a vertical line where the equal signs in the equations would normally appear. This line helps distinguish the coefficients of the equations from the column of constants on the right-hand side. Once we have the augmented matrix, we can simplify it by using row operations (which we‚Äôll discuss shortly) on its entries. After simplification by row operations, the augmented matrix will be transformed to [\begin{pmatrix}1 &amp; 0 &amp;\bigm | &amp; 5 \0 &amp; 1 &amp;\bigm | &amp; 2\end{pmatrix}] which corresponds to the system of equations [\displaylines{x_1 = 1 \x_2 = 2}] This is a trivial system of equations; there is nothing left to solve and we can see that the solutions are $x_1 = 1$ and $x_2 = 2$. This example illustrates the general idea of the Gauss-Jordan elimination procedure for solving the system of equations by manipulating an augmented matrix.  Row Operations : We can manipulate the rows of an augmented matrix without changing its solutions. We‚Äôre allowed to perform the following three types of row operations: Add a multiple of one row to another row Swap the position of the two rows Multiply a row by a constant Let‚Äôs trace the sequence of row operations needed to solve the system of equations [\displaylines{x_1 + 2x_2 = 5 \3x_1 + 9x_2 = 21}] starting from its augmented matrix: [\begin{pmatrix}1 &amp; 2 &amp;\bigm | &amp; 5 \3 &amp; 2 &amp;\bigm | &amp; 21\end{pmatrix}]  As a first step, we eliminate the first variable in the second row by subtracting three times the first row from the second row. [\begin{pmatrix}1 &amp; 2 &amp;\bigm | &amp; 5 \0 &amp; 3 &amp;\bigm | &amp; 6\end{pmatrix}] We denote this row operation as $R_2 \leftarrow R_2 - 3R_1$.  To simplify the second row, we divide it by 3 to obtain[\begin{pmatrix}1 &amp; 2 &amp;\bigm | &amp; 5 \0 &amp; 1 &amp;\bigm | &amp; 2 \end{pmatrix}] This row operation is denoted $R_2 \leftarrow \frac{1}{3}R_2$.  The final step is to eliminate the second variable from the first row. We do this by subtracting two times the second row from the first row $R_1 \leftarrow R_1 - 2 R_2$:[\begin{pmatrix}1 &amp; 0 &amp;\bigm | &amp; 1 \0 &amp; 1 &amp;\bigm | &amp; 2\end{pmatrix}] We can now read off the solution: $x_1 =1$ and $x_2 = 2$. Note how we simplified the augmented matrix through a specific procedure: we followed the Gauss-Jordan elimination algorithm to bring the matrix into its reduced row echelon form. The reduced row echelon form (RREF) is the simplest form for an augmented matrix. Each row contains a leading one (a numeral 1) also known as a pivot . Each column‚Äôs pivot is used to eliminate the numbers that lie below and above it in the same column. The end result of this procedure is the reduced row echelon form: [\begin{pmatrix}1 &amp; 0 &amp; \ast &amp; 0 &amp;\bigm | \ast \0 &amp; 1 &amp; \ast &amp; 0 &amp;\bigm | \ast \0 &amp; 0 &amp; 0 &amp; 1 &amp;\bigm | \ast\end{pmatrix}] Note the matrix contains only zero entries below and above the pivots. The asterisks $\ast$ denote arbitrary numbers that could not be eliminated because no leading one is present in these columns. The solution to a system of linear equations in the variables $x_1, x_2, \dots, x_n$ is the set of values $\{(x_1, x_2, \dots, x_n)\}$ that satisfy all the equations. Gaussian elimination is the process of bringing a matrix into row echelon form.  A matrix is said to be in row echelon form (REF) if all entries below the leading ones are zero. This form can be obtained by adding or subtracting the row with the leading one from the rows below it.  Gaussian-Jordan elimination is the process of bringing a matrix into reduced row echelon form.  A matrix is said to be in reduced row echelon form (RREF) if all the entries below and above the pivots are zero. Starting from the REF, we obtain the RREF by subtracting the row containing the pivots from the rows above them. the rank of the matrix $A$ is the number of pivots in the RREF of $A$.  Number of Solutions : A system of linear equations in three variables could have: one solution If the RREF of a matrix has a pivot in each row, we can read off the values of the solution by inspection. $$\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; c_1 \\\0 &amp; 1 &amp; 0 &amp; c_2 \\\0 &amp; 0 &amp; 1 &amp; c_3\end{bmatrix}$$The unique solution is $x_1 = c_1$, $x_2 = c_2$, and $x_3 = c_3$.  Infinitely many solutions 1 If one of the equations is redundant, a row of zeros will appear when the matrix is brought to the RREF. This happens when one of the original equations is a linear combination of the other two. In such cases, we're really solving two equations in three variables, so can't pin down one of the unknown variables. We say the solution contains a free variable . For example, consider the following RREF:$$\begin{bmatrix} 1 &amp; 0 &amp; a_1 &amp; c_1 \\\0 &amp; 1 &amp; a_2 &amp; c_2 \end{bmatrix}$$The column that doesn't contain a leading one corresponds to the free variable. To indicate that $x_3$ is a free variable, we give it a special label $x_3 \equiv t$. The variable $t$ could be any number $t \in \mathbb{R}$. In other words, when we say $t$ is free, it means $t$ can take on any value from $-\infty$ to $+\infty$. The information in the augmented matrix can now be used to express $x_1$ and $x_2$ in terms of the right-hand constants and the free variable $t$:$$\begin{Bmatrix} x_1 = c_1 - a_1 t \\ x_2 = c_2 - a_2 t \\ x_3 = t, \forall t \in \mathbb{R} \end{Bmatrix} = \begin{Bmatrix} \begin{bmatrix} c_1 \\ c_2 \\ 0 \end{bmatrix} + t \begin{bmatrix} -a_1 \\ -a_2 \\ 1 \end{bmatrix}, \forall t \in \mathbb{R}\end{Bmatrix}$$. The solution corresponds to the equation of a line passing through the point $(c_1, c_2, 0)$ with direction vector $(-a_1, -a_2, 1)$. We'll discuss the geometry of lines in the next section. For now, it's important that you understand that a system of equations can have more than one solution; any point on the line $l \equiv \{(c_1, c_2, 0) + t(-a_1, -a_2, 1), \forall t \in \mathbb{R}\}$ is a solution to the above system of equations.  Infinitely many solutions 2 It's also possible to obtain a two-dimensional solution space. This happens when two of the three equations are redundant. In this case, there will be a single leading one, and thus two free variables. For example, in the RREF$$\begin{bmatrix} 0 &amp; 1 &amp; a_1 &amp; c_1 \\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}$$the variables $x_1$ and $x_3$ are free. As in the previous infinitely-many-solutions case, we define new labels for the free variables $x_1 \equiv s$ and $x_3 \equiv t$, where $ s \in \mathbb{R}$ and $t \in \mathbb{R}$ are two arbitrary numbers. The solution to this system of equations is $$ \begin{Bmatrix} x_1 = s \\ x_2 = c_2 - a_2 t \\ x_3 = t , \\ \forall s,t \in \mathbb{R} \end{Bmatrix} = \begin{Bmatrix} \begin{bmatrix} 0 \\ c_2 \\ 0 \end{bmatrix} + s \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} + t \begin{bmatrix} 0 \\ -a_2 \\ 1 \end{bmatrix}, \forall s,t \in \mathbb{R} \end{Bmatrix} $$This solution set corresponds to the parametric equation of a plane that contains the point $(0,c_2, 0)$ and the vectors $(1,0,0)$ and $(0, -a_2, 1)$. The general equation for the solution plane is $0x+1y+a_2z = c_2$, as can be observed from the first row of the augmented matrix. In the next section, we'll learn more about the geometry of planes and how to convert between their general and parametric forms.  no solutions If there are no numbers $(x_1, x_2, x_3)$ that simultaneously satisfy all three equations, the system of equations has no solution. An example of a system of equations with no solution is the pair $ s+t = 4$ and $s+t = 44$. There are no numbers $(s,t)$ that satisfy both these equations. A system of equations has no solution if its reduced row echelon form contains a row of zero coefficients with a nonzero constant in the right-hand side:$$\begin{Bmatrix}\begin{array}{ccc|c}1 &amp; 0 &amp; 0 &amp; c_1 \\\ 0 &amp; 1 &amp; 0 &amp; c_2 \\\ 0 &amp; 0 &amp; 0 &amp; c_3 \end{array}\end{Bmatrix}$$If $c_3 \neq 0$ this system of equations is impossible to satisfy. There is no solution because there are no numbers $(x_1, x_2, x_3)$ such that $0x_1 + 0x_2 + 0x_3 = c_3$. Dear reader, we've reached the first moment in this book where you'll need to update your math vocabulary. The solution to an individual equation is a finite set of points. The solution to a system of equations can be an entire space containing infinitely many points, such as a line or a plane. The solution set of a system of three linear equations in three unknowns could be either the empty set $\{0\}$ (no solution), a set with one element $\{(x_1, x_2, x_3)\}$, or a set with infinitely many elements like a line $\{p_o + t \overrightarrow{v}, t \in \mathbb{R}\}$ or a plane $\{p_o + s \overrightarrow{v} + t \overrightarrow{w}, s,t \in \mathbb{R}\}$. Another possible solution set is all of $\mathbb{R}^3$; every vector $ \overrightarrow{x} \in \mathbb{R}^3 $ is a solution to the equation $$\begin{bmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$$Note the distinction between the three types of infinite solution sets. A line is one-dimensional, a plane is two-dimensional, and $\mathbb{R}^3$ is three-dimensional. Describing all points on a line requires one parameter, describing all points on a plane takes two parameters, and-of course-describing a point in $\mathbb{R}^3$ takes three parameters.  Geometric Interpretation : We can gain some intuition about solution sets by studying the geometry of the intersections of lines in $\mathbb{R}^2$ and planes in $\mathbb{R}^3$.  Lines in two dimensions : Equations of the form $ax+by = c$ corresponds to lines in $\mathbb{R}^2$. Solving systems of linear equations of the form$$\displaylines{a_1 x + b_1 y = c_1 \\\a_2 x + b_2 y = c_2}$$requires finding the point $(x,y) \in \mathbb{R}^2$ where these lines intersect. There are three possibilities for the solution set:- one solution if the two lines intersect at a point. - infinitely many solutions if the lines are superimposed. - no solution if the two lines are parallel and never intersect. Planes in three dimensions : Equations of the form $ax+by+cz = d$ correspond to planes in $\mathbb{R}^3$. When solving three such equations, $$ \displaylines{a_1 x + b_1 y + c_1 z = d_1 \\\a_2 x + b_2 y + c_2 z = d_2 \\\a_3 x + b_3 y + c_3 z = d_3}$$we want to find a set of points $(x,y,z)$ that satisfy all three equations simultaneously. There are four possibilities for the solution set:1.  one solution three non-parallel planes intersect at a point. 2.  infinitely many solutions 1 if only one of the plane equations is redundant, the solution corresponds to the intersection of two planes which is a line. 3.  infinitely many solutions 2 if two of the equations are redundant, then the solution space is a two-dimensional space. 4.  if two (or more) of the planes are parallel, they will never intersect.  Determinants : Overview : What is the volume of a rectangular box of length $1m$, width $2$ and height $3m$? It's easy to compute the volume of this box because its shape is right rectangular prism. The volume of this prism is $V = l \times w \times h = 6m^3$. What if the shape of the box was a parallelpiped instead? A parallelpiped is a box whose opposite faces are parallel but whose sides are slanted. How do we compute the volume of a parallelpiped? The determinant operation, specifically the $3 \times 3$ determinant, is the perfect tool for this purpose. The determinant of a matrix, denoted $det(A)$ or $|A|$, is a particular way to multiply the entries of the matrix to produce a single number. We use determinants for all kinds of tasks: to compute areas and volumes, to solve systems of linear equations, to check whether a matrix is invertible or not, etc. We can interpret the determinant of a matrix intuitively as a geometrical calculation. The determinant is the volume of the geometric shape whose edges are the rows of the matrix. For $2 \times 2$ matrices, the determinant corresponds to the area of a parallelogram. For $3 \times 3$ matices, the determinant corresponds to the volume of a parallelpiped. For dimensions $d &gt; 3$, we say the determinant measures a $d$-dimensional hyper-volume . Consider the linear transformation $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined through the matrix-vector product with a matrix $A_T: T(\overrightarrow{x}) \equiv A_T \overrightarrow{x}$. The determinant of the matrix $A_T$ is the scale factor associated with the linear transformation $T$. The scale factor of the linear transformation $T$ describes how the area of a unit square in the input space (a square with dimensions $1 \times 1$) is transformed by $T$ . After passing through $T$, the unit square is transformed to a parallelogram with with area $det(A_T)$. Linear transformations that shrink areas have $det(A_T) &lt; 1$, while linear transformations that enlarge areas have $det(A_T) &gt; 1$. A linear transformation that is area preserving has $det(A+T) = 1$. The determinant is also used to check linear independence for a given set of vectors. We construct a matrix using the vectors as the matrix rows , and compute its determinant. The determinant of a matrix tells us whether or not that matrix is invertible . If $det(A) \neq 0$, then $A$ is invertible; if $det(A) = 0$, $A$ is not invertible. The determinant shares a connection with the vector cross product , and is used in the definition of the eigenvalue equation . Formulas : The determinant of a $2 \times 2$ matrix is $$det(\begin{bmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{bmatrix}) = \begin{vmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{vmatrix} = a_{11}a_{22} - a_{12}a{21}$$The formulas for the determinants of larger matrices are defined recursively. For example, the determinant of $3 \times 3$ matirx is defined in terms of $2 \times 2$ determinants:$$ \begin{vmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\ a_{31} &amp; a_{32} &amp; a_{33} \end{vmatrix} = a_{11} = a_{11} \begin{vmatrix}a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33} \end{vmatrix} -a_{12} \begin{vmatrix} a_{21} &amp; a_{23} \\ a_{31} &amp; a_{33} \end{vmatrix}+ a_{13} \begin{vmatrix} a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32} \end{vmatrix}$$ There's a neat computational trick for computing $3 \times 3$ determinants by hand. The trick consists of extending the matrix $A$ into a $3 \times 5$ array that contains copies the columns of $A$: the $1^{st}$ column of $A$ is copied to the $4^{th}$ column of the extended array, and the $2^{nd}$ column of $A$ is copied to the $5^{th}$ column. The determinant is then computed by summing the products of the entries on the three positive diagonals and subtracting the products of the entries on the three negative diagonals. The general formula for the determinant of an $n \times n$ matrix is$$det(A) = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} M_{1j}$$where $M_{ij}$ is called the minor associated with the entry $a_{ij}$. The minor $M_{ij}$ is the determinant of the submatrix obtained by removing the $i^{th}$ row and the $j^{th}$ column of the matrix $A$. Note the alternating factor $(-1)^{i+j}$ that changes value between $+1$ and $-1$ for different terms in the formula. The determinant of a $4 \times 4$ matrix $B$ is $$ det(B) = b_{11}M_{11} - b_{12}M_{12} + b_{13}M_{13} - b_{14}M_{14}$$The general formula for determinants $det(A) = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} M_{1j}$ assumes we're expanding the determinant along the first row of the matrix. In fact, a determinant formula can be obtained by expanding the determinant along anyrow or column of the matrix. The expand-along-any-row-or-column nature of determinants can be very handy: if you need to calculate the determinant of a matrix with one row (or column) containing many zero entries, it makes sense to expand along that row since many of the terms in the formula will be zero. If a matrix contains a row (or column) consisting entirely of zeros, we can immediately tell its determinant is zero.  Geometric interpretation : Area of a parallelogram Suppose we're given vectors $overrightarrow{v} = (v_1, v_2)$ and $\overrightarrow{w} = (w_1, w_2)$ in $\mathbb{R}^2$ and we construct a parallelogram with corner points $(0,0), \overrightarrow{v}, \overrightarrow{w}$ and $\overrightarrow{v} + \overrightarrow{w}$. The area of this parallelogram is equal to the determinant of the matrix that contains $(v_1, v_2)$ and $(w_1, w_2)$ as rows:$$area = \begin{vmatrix} v_1 &amp; v_2 \\ w_1 &amp; w_2 \end{vmatrix} = v_1 w_2 - v_2 w_1 $$ Volume of a parallelpiped  Sign and absolute value of the determinant Calculating determinants can produce positive or negative numbers. "
    }, {
    "id": 25,
    "url": "http://localhost:4000/point-cloud.html",
    "title": "Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision",
    "body": "2021/09/09 -  Introduction : In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is affected by a variety of components. From the electro-optic sensors to the image resolution, contrast, exposure and blurriness variables, all add to the complexity of analyzing a scene and processing the imagery.  Monocular Vision : Traditional stereoscopy systems, such as human visual system, utilize multiple viewing angles of the same object in order to do triangulation and get a depth perception. Monocular vision, better described as vision through a single camera source, presents new challenges when compared to stereo vision or a multi-camera system. In a stereo system, similar to human vision, distances between the cameras (the baseline) and their orientation is known and in most circumstances remains constant. In order to generate various viewing angles with a monocular system, the camera must continuously be moving. With a moving camera, the system obtains two different viewing angles form two points in time. The challenge becomes to accurately compute the distance the camera has traveled or the exact location of the camera at each frame of video. In addition, the orientation of the camera at each point in time must be computed from the scene.  Visual Structure from Motion : In his work, Wu et al. describes the methodology of Visual Structure from Motion as following. Using a set of image feature locations and correspondences, the goal of bundle adjustment is to find 3D point positions and camera parameters that minimize the re-projection error. This optimization problem is constructed as a non-linear least squares problem, where the error is the squared $L_2$ norm of the difference between the observed feature location and the projection of the corresponding 3D point on the image plane of the camera. Wu explains by letting $x$ be a vector of parameters and $f(x) = [f_1(x), /dots, f_k(x)]$ be the vector of residual errors for 3D reconstruction. Then the optimization problem he wishes to solve is the non-linear least squares problem shown in the below equation. \(x* = \argmin_{x} \sum_{i=1}^{k} \lVert f_i(x) \rVert^2\) The Levenberg-Marquardt (LM) algorithm is an extremely popular algorithm for solving non-linear least squares problems and is the algorithm choice for bundle adjustment. LM operates by computing a series of regularized linear approximations to the original nonlinear problem. Let $J(x)$ be the Jacobian of $f(x)$, then in each iteration LM solves a linear least squares problem of the form\(\delta* = \argmin_{\delta} \lVert J(x)\delta + f(x) \rVert^2 + \lambda \lVert D(x) \delta \rVert^2\)and updates $x \lefta. rrow x + \delta$ if $\lVert f(x+\delta)\rVert &lt; \lVert f(x) \rVert$. Here $D(x)$ is a non-negative diagonal matrix, typically the square root of the diagonal of the matrix $J(x)^TJ(x)$ and $\lambda$ is a nonnegative parameter that controls the strength of regularization. Wu explains that the regularization is needed to ensure a convergent algorithm. LM updates the value of $\lambda$ at each step based on how well the Jacobian $J(x)$ approximates $f(x)$.  SIFT Features : The SIFT algorithm can be broken down into four main stages:  scale-space peak selection point localization orientation assignment point descriptorThe first stage is to search for interest points over location and scale. The image is constructed in a Gaussian Pyramid, where the image is downsampled and blurred at each level. These blurred images at each level are used to compute the Difference of Gaussians (DoG), which locate edges and corners within an image. Interesting points are then extracted in stage 2 by locating the maxima/minimal pixels within different scales of DoG at sub-pixel accuracy. Once interest points have been located, an orientation is assigned based on the gradient orientation of the pixels around the interest point. Once orientation and scale have been addressed, the final stage is the generation of point descriptors. "
    }, {
    "id": 26,
    "url": "http://localhost:4000/depth-intro.html",
    "title": "Depth Estimation - An Introduction",
    "body": "2021/09/09 -  Paradigms for 3D Images Representation over a Plane : As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the depth information should be able to be represented in a plane, for printing purposes, for example. There are three widely used modes for depth representation: Gray scale 2. 5D representation. This paradigm uses the gray scale intensity to represent the depth of each pixel in the image. Thus, the color, texture and luminosity of the original image are lost in this representation. The name  2. 5D  refers to the fact that this kind of images has the depth information directly in each pixel, while it is represented over a 2D space. In this paradigm, the gray level represents the inverse of the distance. Thus, more a pixel is bright, closer is the point represented. Vice versa, the darker is a pixel, further is the represented point. This is most commonly used way for depth representation.  Color 2. 5D representation. This representation is similar to the previous one. The difference is the use of colors to represent depth. In the following image, red-black colors represent closer points, and blue-dark colors the further points. However, other color representations are available in the literature.  Pseudo-3D representation. This representation provides different points of view of the reconstructed space. The main advantage of the first two methods is the possibility of implementing objective comparison among algorithms, as it is done in the Middlebury database and test system.  Important terms and issues in depth estimation : The depth estimation world is quite complex research field, where many techniques and setups have been proposed. The set of algorithms which solve the depth map estimation problem deals with many different mathematical concepts which should be briefly explained for a minimum overall comprehension of the matter. In this section, we will review some important points about image processing applied to depth estimation. Standard Test Beds : The availability of common tests and comparable results is a mandatory constraint in active and widely explored fields. Likewise, the possibility of objective comparisons make easier to classify different proposals. In depth estimation, and more specifically in stereo vision, one of the most important test bed is the Middlebury database and test bed. The test beds provides both eyes images of a 3D scene, as well as the ground truth map. The same test allow, as said, algorithms classification. Color or Grayscale Images? : The first point when we want to process an image, whichever is the goal, is to decide what to process. In this case, color or grayscale images. As it can be seen in the following figure, color images have much more information than gray scale images. Color images should, hence, be more appropriate for data extraction, among them, depth information. However, the color images have an important disadvantage: for a 256 level definition, they are represented by 3 bytes (24-bit representation), while grayscale images with the same level only require one single byte. The consequence is obvious: color image processing requires much more time and operations. The Epipolar Geometry : When dealing with stereo vision setups, we have to face the epipolar geometry problem. Let $C_l$ and $C_r$ be the focal centers of the left and right sensors (or eyes), and $L$ and $R$ the left and right image planes. Finally, $P$ will be a physical point of the scene and $p_l$ and $p_r$ the projections of $P$ over $L$ and $R$, respectively. In this figure, we can also see both  epipoles , i. e. , the points where the line connecting both focal centers intersect the image planes. They are noted as $e_l$ and $e_r$. The geometrical properties of this setup force that every point of the line $Pp_l$ lies on the line $p_re_r$ which is called  epipole line . The correspondence ofa poitn seen in one image must be searched in the corresponding epipolar line in the other one. A simplified version of the geometry arise when the image planes are parallel. This is the base of the so-called fronto-parallel hypothesis. The Fronto-Parallel Hypothesis : The epipolar geometry of two sensors can be simplified, as said, positioning both planes parallel, arriving to the following setup:The epipoles are placed in the infinite, and the epipolar (and search) lines become horizontal. The point (except the occluded ones) are only decaled horizontally. This geometrical setup can be implemented by properly orienting the sensors, or by means of mathematical transformation fo the original images. If the last option is the case, the result is called  rectified image . The most important consequences of this geometry, regarding the Cartesian plane can be written as follows: $y_l = y_r$. The height of a physical point is the same in both images.  $x_l = x_r + \nabla d$. The abcissa of a physical point is decaled by the so-called parallax or disparity , which is inversely related to the depth.  A point in the infinite has identical abscissa coordinates in both image planes.  Matching : When different viewpoints from the same scene are compared, a further problem arises that is associated with the mutual identification of images. The solution to this problem is commonly referrred to as matching. The matching process consists of identifying each physical points within different images. However, matching techniques are not only used in stereo or multivision procedures but also widely used for image retrieval or fingerprint identification where it is important to allow rotational and scalar distortions. There are also various constraints that are generally satisfied by true matches thus simplifying the depth estimation algorithm, such as similarity, smoothness, ordering and uniqueness. As we will see, the matching process is a conceptual approach to identify similar characteristics in different images. It is, then, subjected to errors. The matching is, hence, implemented by means of comparators allowing different identification strategies such as minimum square errors (MSE), sum of absolute differences (SAD) or sum of squared differences (SSD). The characteristic compared through the matching process can be anything quantifiable. Thus, we will see algorithms matching points, edges, regions or other image cues.  References :  ksimek blog prateekvjoshi blog "
    }, {
    "id": 27,
    "url": "http://localhost:4000/3d-packing.html",
    "title": "3D Packing for Self-Supervised Depth Estimation",
    "body": "2021/09/09 -  Self-Supervised Scale-Aware SfM : In self-supervised monocular SfM training, we aim to learn: &lt;ul&gt;&lt;li&gt; a monocular depth model $f_D = I \rightarrow D$ that predicts the scale-ambiguous depth $\hat{D} = f_D(I(p))$ for every pixel $p$ in the target image $I$;&lt;/li&gt; &lt;li&gt; a monocular ego-motion estimator $f_x: (I_t, I_s) \rightarrow x_{t \rightarrow s}$ that predicts the set of 6-DoF rigid transformations for all $s \in S$ given by $x_{t \rightarrow s} = \begin{pmatrix} R &amp; t \\ 0 &amp; 1 \end{pmatrix} \in SE(3)$, between the target image $I_t$ and a set of source images $I_s \in I_S$ considered as part of the temporal context. &lt;/li&gt;&lt;/ul&gt; In practice, we use the frames $I_{t-1}$ and $I_{t+1}$ as source images, although using a larger context is possible. Note that in the case of monocular SfM, both depth and pose are estimated up to an unknown scale factor, due to the inherent ambiguity of the photometric loss.  Self-Supervised Objective : Following the work of Zhou et al. , we train the depth and pose network simultaneously in a self-supervised manner. In this work, however, we learn to recover the inverse depth $f_d: I \rightarrow f_D^{-1}(I)$ instead, along with the ego-motion estimator $f_x$. The overall self-supervised objective consists of an appearance matching loss term $L_p$ that is imposed between the synthesized image $\hat{I_t}$ and the target image $I_t$, and a depth regularization term $L_s$ that ensures edge-aware smoothing in the depth estimates $\hat{D_t}$. The objective takes the following form: [L(I_t, \hat{I_t}) = L_p(I_t, I_s) \bigodot M_p \bigodot M_t + \lambdda_1 L_s(\hat{D_t})] where $M_t$ is a binary mask that avoids computing the photometric loss on the pixels that do not have a valid mapping, and $\bigodot$ denotes element-wise multiplication. Additionally, $\lambda_1$ enforces a weighted depth regularization on the objective. The overall loss is averaged per-pixel, pyramid-scale and image batch during training.  The proposed scale-aware self-supervised monocular structure-from-motion architecture. This paper introduces PackNet as a novel depth network, and optionally include weak velocity supervision at training time to produce scale-aware depth and pose models.  Appearance Matching Loss : The pixel-level similarity between the target image $I_t$ and the synthesized target image $\hat{I_t}$ is estimated using the Structural Similarity (SSIM) term combined with a L1 pixel-wise loss term, inducing an overall photometric loss given by the equation below. [L_p(I_t, \hat{I_t}) = \alpha \frac{1-SSIM(I_t, \hat{I_t})}{2} + (1 - \alpha) \lVert I_t - \hat{I_t} \rVert] While multi-view projective geometry provides strong cues for self-supervision, errors due to parallax in the scene have an undesirable effect incurred on the photometric loss. We mitigate these undesirable effects by calculating the minimum photometric loss per pixel for each source image in the context $I_S$, so that: [L_p(I_t, I_S) = \min_{I_S} (I_t, \hat{I_t})] The intuition is that the same pixel level will not be occluded or out-of-bounds in all context images, and that the association with minimal photometric loss should be the correct one. Furthermore, we also mask out static pixels by removing those which have a warped $L_p(I_t, \hat{I_t})$ higher than their corresponding unwarped photometric loss $L_p(I_t, I_s)$, calculated using the original source image without view synthesis. This auto-mask removes pixels whose appearance does not change between frames, which include static scenes and dynamic objects with no relative motion, since these will have smaller photometric loss when assuming no ego-motion. [M_p = \min_{I_S} L_p(I_t, I_s) \geq L_p(I_t, \hat{I_t})] Depth Smoothness Loss : In order to regularize the depth in texture-less low-image gradient regions, we incorporate an edge-aware term. The loss is weighted for each of the pyramid-levels, and is decayed by a factor of 2 on down-sampling, starting with a weight of 1 for the 0th pyramid level.       [L_s(\hat{D_t}) =   \delta_x \hat{D_t}   e^{-   \delta_x I_t   } +   \delta_y \hat{D_t}   e^{   \delta_y I_t   }]    Scale-Aware SfM : As previously mentioned, both the monocular depth and ego-motion estimators $f_d$ and $f_x$ predict scale-ambiguous values, due to the limitations of the monocular SfM training objective. In other words, the scene depth and the camera ego-motion can only be estimated up to an unknown and ambiguous scale factor. This is also reflected in the overall learning objective, where the photometric loss is agnostic to the metric depth of the scene. Furthermore, we note that all previous approaches which operate in the self-supervised monocular regime suffer from this limitation, and resort to artificially incorporating this scale factor at test-time, using LiDAR measurements.  Velocity Supervision Loss : PackNet: 3D Packing for Depth Estimation : Standard convolutional architectures use aggresive striding and pooling to increase their receptive field size. However, this potentially decreases the model performance for tasks requiring fine-grained representations. Similarly, traditional upsampling strategies fail to propagate and preserve sufficient details at the decoder layers to recover accurate depth predictions. In contrast, we propose a novel encoder-decoder architecture, called PackNet, that introduces new 3D packing and unpacking blocks to leran to jointly preserve and recover important spatial information for depth estimation. This is in alignments with recent observations that information loss is not a necessary condition to learn representations capable of generalizing to different scenarios. In fact, progressive expansion and contraction in a fully invertible manner, without discarding uninformative input variability, has been shown to increase performance in a variety of tasks. We first describe the different blocks of our proposed architecture, and then proceed to show how they are integrated together in a single model for monocular depth estimation.  Packing Block : The packing block starts by folding the spatial dimensions of convolutional feature maps into extra feature channels via a Space2Depth operation. The resulting tensor is at a reduced resolution, but in contrast to striding or pooling, this transformation is invertible and comes at no loss. Next, we learn to compress this concatenated feature space in order to reduce its dimensionality to a desired number of output channels. As shown in experiments, 2D convolutions are not designed to directly leverage the tiled structure of this feature space. Instead, we propose to first learn to expand this structured representation via a 3D covolutional layer. The resulting higher dimensional feature space is then flattened (by simple reshaping) before a final 2D convolutional contraction layer. This structured feature expansion-contraction, inspired by invertible networks, although we do not ensure invertibility, allows our architecture to dedicate more parameters to learn how to compress key spatial details that need to be preserved for high resolution depth decoding.  Unpacking Block : Symmetrically, the unpacking block learns to decompress and unfold packed convolutional feature channels back into higher resolution spatial dimensions during the decoding process. The unpacking block replaces convolutional feature upsampling, typically performed via nearest-neighbor or with learnable transposed convolutional weights. "
    }, {
    "id": 28,
    "url": "http://localhost:4000/group.html",
    "title": "Batch Normalization and Group Normalization",
    "body": "2021/09/08 -  Batch Normalization: the Principles : Batch Normalization is an algorithmic method which makes the training of Deep Neural Networks faster and more stable . Batch normalization is computed differently during the training and the testing phase. At training , the BN layer determines the mean and standard deviation of the activation values across the batch. It then normalizes the activation vector with $\mu$ and $\sigma$. That way, each neuron‚Äôs output follows a standard normal distribution across the batch . It finally applies a linear transformation with $\gamma$ and $\beta$ which are the two trainable parameters. Such step allows the model to choose the optimum distribution for each hidden layer. $\gamma$ allows to adjust the standard deviation while $\beta$ allows to adjust the bias, shifting the curve on the right or on the left side. At each iteration, the network computes the mean $\mu$ and the standard deviation $\sigma$ corresponding to the current batch. Then it trains $\gamma$ and $\beta$ through gradient descent using an Exponential Moving Average (EMA) to give more importance to the latest iterations.  We mostly use Exponential Moving Average algorithm to reduce the noise or to smooth the data. The weight of each element decreases progressively over time, meaning the EMA gives greater weight to recent data points . EMA reacts faster to changes compared to Simple Moving Average. At the evaluation phase , we may not have a full batch to feed into the model. To tackle this issue, we compute $\mu_{pop}$ and $\sigma_{pop}$ as the estimated mean and standard deviation of the studied population . Those values are computed using all the $\mu_{batch}$ and $\sigma_{batch}$ during training, and directly fed during the evaluation phase.  Why Normalization? : What we can conclude from the original Batch Normalization paper is that:  Adding BN layers leads to better convergence and higher accuracy  Adding BN layers allows us to use higher learning rate without compromising convergence. To quote Ian Goodfellow about the use of batch normalization: Before BN, we thought it was almost impossible to efficiently train deep models using sigmoid in the hidden layers. Batch Normalization makes those unstable networks trainable. In practice, it is widely admitted that: For CNNs, Batch Normalization is better  For Recurrent Networks, Layer Normalization is better While BN uses the current batch to normalize every single value, LN uses all the current layer to do so.  The normalization is performed using other features from a single example instead of using the same feature across all current batch examples. The best way to understand why BN works is to understand the optimization landscape smoothness . BN reparameterizes the underlying optimization problem , making the training faster and easier. In additional recent studies, researchers observed that this effect is not unique to BN, but applies to other normalization methods (i. e. L1 normalization or L2 normalization).  The Drawbacks of BN : For BN to work, the batch size is required to be sufficiently large , usually at least 32 . However, there are situations when we have to settle for a small batch size. For example, when each data sample is highly memory consuming or when we train a very large neural network which leaves little GPU memory for processing data. For computer vision applications other than image classification, the restriction on batch sizes are more demanding and it is difficult to have higher batch sizes.  Comparisions of Normalization Methods : Group Normalization is one of the latest normalization methods that avoids exploiting the batch dimension, thus is independent of batch size. But there are other normalization methods as well.  Layer Normalization : Layer Normalization computes $\mu_i$ and $\sigma_i$ along the (C,H,W) axes. The computation for an input feature is entirely independent of other input features in a batch.  Instance Normalization : Instance Normalization computes $\mu_i$ and $\sigma_i$ along the (H,W) axes. Since the computation of IN is the same as that of BN with batch_size=1, IN actually makes the situation worse in most cases. However, for style transfer tasks , IN is better at discarding contrast information of an image, thus having superior performance than BN.  Group Normalization : Also notice that IN can be viewed as applying Layer Normalization to each channel individually as if the num_channels = 1. Group Normalization is the middle ground between IN and LN. It organizes the channels into different groups and computes $\mu_i$ and $\sigma_i$ along the (H,W) axes and along a group of channels. First, the batch with dimension (N,C,H,W) is reshaped to (N,G,C//G,H,W). The number of group $G$ is a pre-defined hyperparameter . Then we normalize along the (C//G,H,W) dimension and return the result after reshaping the batch back to (N,C,H,w). Group Normalization is better than Layer Normalization as GN allows different distribution to be learned for each group of channels. GN is also thought to be better than IN because GN can exploit the dependence across channels. If `C = G`, that is, if the number of groups are set to be equal to the number of channels, GN becomes IN . Likewise, if `G = 1` GN becomes LN .  References :  blog post medium article medium article2 "
    }, {
    "id": 29,
    "url": "http://localhost:4000/linear.html",
    "title": "An Intuitive Overview of Linear Algebra Fundamentals",
    "body": "2021/09/08 -  Introduction : Why Learn Linear Algebra? : This write up is an overview of some of the linear algebra fundamentals. It focuses on providing an intuitive/geometric review of some of the main concepts. The coverage is by no means comprehensive or complete. It‚Äôs meant to serve as review material for those who haven‚Äôt touched it in a very long time, or as a brief intro for those who want an over-the-top view of the landscape. Linear algebra is the study of vectors and linear functions Functions are computations/transformations taht take a set of inputs and produce an output. Functions of several variables are often presented in one line such as: [f(x,y) = 3x + 5y] In mathematics, the concept of linearity plays a very important role. Mathematically, a linear function, or linear map, or linear operator, $f$ is a function that satisfies:  $f(x,y) = f(x) + f(y)$ for any input $x$ and $y$ $f(cx) = cf(x)$ for any input $x$ and any scalar $c$Put into words, the first condition means that the output of a function acting on a sum of inputs is just equal to the sum of the individual outputs. The second implies that the output of a scaled input is just the scaled output of the original input. Linear functions are those whose graph is a straight line. A linear function has the following form: [y = f(x) = a+bx] Here, $x$ is our independent variable and $y$ is our dependent variable (since the result $y$ depends on the transformation we perform on $x$). Notice that although we classify one variable as being dependent on the other, in reality, both variables are linked and dependent on each other. We simply choose to classify the variable which takes an input as the ‚Äòindependent‚Äô one. In our example above, we can just as easily choose to exchange our variables and make $y$ the independent variable and $x$ the dependent one. Let‚Äôs take $y = f(x) = 3x+1$ as an example. We can choose to model the above function in terms of $y$ by taking the inverse, so we get:\(y = 3x+1y-1 = 3xx = f(y) = y/3 - 1/3\) In either case, you get the point. Our linear function can be viewed as representing a linear relationship between two or more variables. The relationship between these variables are the relationships we model in linear algebra. To make things easier to grasp, we usually represent our variable relationship visually by projecting input variable $x$ on a horizontal axis (which we call the $x$-axis) and showing its mapping to variable $y$ on a vertical axis (which we call the $y$-axis). This lets us get a visual intuition of how our function works in transforming inputs to outputs, and lets us model our linear transformations geometrically. Some graphical examples of linear functions are provided below: Of course, linear functions aren‚Äôt the only types of functions. Non-linear functions don‚Äôt have a linear mapping and are not represented by straight lines. Some examples of non-linear functions are provided below: Vectors : The key starting point of linear algebra is the concept of vectors. In high school physics, chances are you‚Äôve already seen that concept as being nothing more than ‚Äúa number and a direction‚Äù. This isn‚Äôt false, but it‚Äôs definitely not the whole story. It‚Äôs rather intuitive way of introducing vectors, hence why we‚Äôll use this analogy extensively. From a mathematical perspective, vectors are just a way of stacking numbers together in a column or a row. For example, \(\begin{bmatrix} 2 \\ 3 \end{bmatrix}\begin{bmatrix} i \\ 3 \\ -3 \end{bmatrix}\begin{bmatrix} 5 &amp; 2. 1 \end{bmatrix}\begin{bmatrix} -3 &amp; \frac{1}{2} &amp; 4 \end{bmatrix}\) The first two vector examples are naturally referred to as column vectors and the last two as row vectors. The amount of numbers is referred to as the dimension of the vector. Even if you‚Äôve never explicitly learned about vectors until now, you‚Äôve already seen them. A 2-dimensional vector of real numbers is analogous to the Cartesian coordinates. For example, a 2-dimensional vector looks like: A 3-dimensional vector of real numbers is analogous to the spatial coordinates in three dimensions. You can always think of a vector with $n$ numbers as a point in $n$-dimensional ‚Äúhyperspace‚Äù. For simplicity, we‚Äôll generally use 2 and 3 dimensional vectors from now on, but everything we explain below applies to vectors of arbitrary sizes.  Vector Addition : Since we just defined a new mathematical concept, it‚Äôs natural to ask the question: can we add and multiply vectors? "
    }, {
    "id": 30,
    "url": "http://localhost:4000/disparity.html",
    "title": "Depth from Disparity",
    "body": "2021/09/05 -  üåü 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images ? What cues in the image provide 3D information? Before looking at binocular, let‚Äôs consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R)}{2(x_L - x_R)} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 31,
    "url": "http://localhost:4000/camera.html",
    "title": "Dissecting the Camera Matrix (Part 1)",
    "body": "2021/09/05 -  Camera Calibration and Decomposition : Primarily, camera calibration is about finding the quantities internal to the camera that affect the imaging process. Here are some of the factors that will be taken care of:  image center : we need to find the position of the image center in the image. Wait, isn't the image center located at $(width/2, height/2)$? Well, no. Unless we calibrate the camera, the image will almost always appear to be off-center.  focal length : this is a very important parameter. Remember how people using DSLR cameras tend to focus on things before capturing the image? this parameter is directly related to the focus of the camera and it is very critical.  scaling factors : the scaling factors for row pixels and column pixels might be different. If we don't take care of this thing, the image will look stretched (either horizontally or vertically).  skew factor : this refers to shearing. the image will look like a parallelogram otherwise.  lens distortion : this refers to the pseudo zoom effect that we see near the center of any image.  shearing refers to a transformation in which all points along a given line $L$ remain fixed while other points are shifted parallel to $L$ by a distance proportional to their perpendicular distance from $L$.  Pinhole Camera Model : Before we jump into anything, let's see where this all began. When we capture an image, we are basically mapping the 3D scene to a 2D scene. It means that every point in the 3D world gets mapped to the 2D plane of our image. This is called the pinhole camera model . It basically describes the relationship between the coordinates of the 3D point and its projection on the 2D image. This, of course, is the ideal case where there is absolutely no distortion of any kind. Every camera is modeled based on this, and every camera aspires to simulate this as close as possible. But in the real world, we have to deal with things like geometric distortions, blurring, finite sized apertures, etc. The figure shown here depicts a pinhole camera model. The camera is placed at the origin $O$. The point $P$ represents a point in the real world. We are trying to capture that onto a 2D plane. The image plane represents the 2D plane that you get after capturing the image. The image plane actually contains the image that you see after capturing a picture. So basically, we are trying to map every 3D point to a point on the image plane. In this case, the point $P$ gets mapped to $P_c$. The distance between the origin $O$ and this image plane is called the focal length of the camera. This is the parameter you modify when you adjst the focus of the camera.  Intrinsic and Extrinsic Parameters : In the above figure, we want to estimate $(u,v)$ from $(X,Y,Z)$. Let's say the focal length is denoted by $f$. If you look at the triangle formed using the origin-$P_c$-and the $Z$-axis with the origin-$P$ and $Z$-axis, you will notice that they are similar triangles. This means that $u$ depends on the $f$, $X$ and $Z$. Similarly, $v$ depends on $f$, $Y$ and $Z$. $$\displaylines{u = fX/Z \\\v = fY/Z}$$Next, if the origin of the 2D image coordinate system does not coincide with where the $Z$-axis intersects the image plane, we need to translate $P_c$ into the desired origin. Let this translation be defined by $(t_u, t_v)$. So now, $u$ and $v$ are given by:$$\displaylines{u = fX/Z + t_u \\\v = fY/Z + t_v}$$So up until now, we have something that can translate $(X,Y,Z)$ to $(u,v)$. Let's denote this matrix $M$. So we can write:$$ P_c = MP$$Since this is a camera image, we need to express it in inches. For this, we will need to know the resolution of the camera in pixels/inch. If the pixels are square the resolution will be identical in both $u$ and $v$ directions of the camera image coordinates. However, for a more general case, we assume rectangular pixels with resolution $m_u$ and $m_v$ pixels/inch in $u$ and $v$ directions respectively. Therefore, to measure $P_c$ in pixels, its $u$ and $v$ coordinates should be multiplied by $m_u$ and $m_v$ respectively. So now, this new transformation matrix depends on $f, X, Y, Z, t_u, t_v, m_u, m_v$. Let's denote this by:$$P_c = KP$$Here, $K$ is called the intrinsic parameter matrix for the camera. Now, if the camera does not have its center of projection at $(0,0,0)$ and is oriented in an arbitrary fashion (not necessarily $z$-perpendicular to the image plane), then we need roation and translation to make the camera coordinate system coincide with the configuration in that pinhole camera figure. Let the rotation applied to coincide the principal axis with $Z$-axis given by a $3 \times 3$ rotation matrix $R$. Then the matrix is formed by first applying the translation followed by the rotation is given by the $3 \times 4$ matrix. $$E = \\( R|RT \\)$$This is called the extrinsic parameter matrix for the camera . So, the complete camera transformation can now be represented as: $$ K \\( R|RT \\) = KR \\( I|T \\)$$Hence, $P_c$ the projection of $P$ is given by:$$P\_c = KR \\( I|T \\) P = CP$$$C$ is a $3 \times 4$ matrix usually called the complete camera calibration matrix. So basically, camera calibration matrix is used to transform a 3D point in the real world to a 2D point on the image plane considering all the things like focal length of the camera, distortion, resolution, shifting of origin, etc.  References :  ksimek blog prateekvjoshi blog "
    }, {
    "id": 32,
    "url": "http://localhost:4000/intrinsic.html",
    "title": "Dissecting the Camera Matrix (Part 2)",
    "body": "2021/09/05 -  The Intrinsic Matrix : The intrinsic matrix is paramterized as \(K = \begin{bmatrix} f_x &amp; s&amp; x_o \\ 0 &amp; f_y &amp; y_o \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) Each intrinsic parameter describes a geometric property of the camera. Let‚Äôs examine each of these properties in defail.  Focal Length : The focal length is the distance between the pinhole and the film (a. k. a the image plane). For reasons that we will discuss later, the focal length is measured in pixels. In a true pinhole camera, both $f_x$ and $f_y$ have the same value, which is illustrated as $f$ in the above figure. In practice, $f_x$ and $f_y$ can differ for a number of reasons: the camera's lens introduces unintentional distortion the camera uses an anamorphic format, where the lens compresses a widescreen scene into a standard-sized sensor.  flaws in the digital camera sensor. In all of these cases, the resulting image has non-square pixels. Having two different focal lengths sadly isn‚Äôt intuitive, so some texts use a single focal length and an aspect ratio that describes the amount of deviation from a perfectly squared pixel. Such a parameterization nicely separates the camera geometry (i. e. focal length) from distortion (aspect ratio).  Principal Point Offset a. k. a $x_0$ and $y_0$ : The camera‚Äôs principal axis is the line perpendicular to the image plane that passes through the pinhole . Its intersection with the image plane is referred to as the principal point . The principal point offset is the location of the principal point relative to film‚Äôs origin . The exact definition dependson which convention is used for the location of the origin; the illustration above assumes it‚Äôs at the bottom-left off the film. Notice that the box surrounding the camera is irrelevant, only the pinholes position relative to the film matters.  Axis Skew $s$ and Other Geometric Properties : Axis skew causes shear distortion in the projected image. Apparently some digitalization processees can cause nonzero skew. The focal length and principal point offset amount to simple translation of the film relative to the pinhole. What about rotating or scaling the film? Rotating the film around the pinhole is equivalent to rotating the camera itself, which is handled by the extrinsic matrix. Rotating the film around any other fixed point $x$ is equivalent to rotating around the pinhole $P$, then translating by $(x-P)$. What about scaling? It should be obvious that doubling all camera dimensions (film size and focal length) has no effect on the captured scene. If, instead, you double the film size and not the focal length, it is equivalent to doubling both and then halving the focal length. Thus, representing the film‚Äôs scale explicitly would be redundant. It is captured by focal length.  The Camera Frustom : Focal Length: From Pixels to World Units : The above discussion of camera-scaling shows that there are infinite number of pinhole cameras that produce the same image. The intrinsic matrix is only concerened with the relationship between camera coordinates and image coordinates , so the absolute camera dimensions are irrelevant.  Using pixel units for focal length and principal point allows us to represent the relative dimensions fo the camera, namely, the film‚Äôs position relative to its size in pixels . Another way to say this is that the intrinsic camera transformation is invariant to uniform scaling of the camera geometry . By representing dimensions in pixel units, we naturally capture this invariance. You can use similar triangles to convert pixel units to world units if you know at least one camera dimension in world units. For example, if you know the camera‚Äôs film (or digital sensor) has a width of $W$ in millimiters, the image width in pixels $w$, you can convert the focal length $f_x$ to world units using [F_x:f_x = W: w  F_x = f_x \frac{W}{w}] Other parameters $f_y$, $x_0$ and $y_0$ can be converted to their world-unit counterpart using similar equations: [F_y = f_y \frac{H}{h} X_O = x_0 \frac{W}{w} Y_0 = y_0 \frac{H}{h}] The Camera Frustum: A Pinhole Camera Made Simple : To put it simply, camera frustum is a visual representation where we use virtual image that has the same properties as the film image. Unlike the true image, the virtual image appears in front of the camera, and the projected image is unflipped. So the pinhole has been replaced by the tip of the visibility cone, and the film, and the film is now represented by the virtual image plane.  Intrinsic Parameters as 2D Transformations : We can interpret these 3D-vectors as 2D homogeneous coordinates which are transformed to a new set of 2D points. This gives us a new view of the intrinsic matrix: a sequence of 2D affine transformations. It also emphasizes that the intrinsic camera transformation occurs post-projection .  References :  ksimek blog prateekvjoshi blog "
    }, {
    "id": 33,
    "url": "http://localhost:4000/inverse.html",
    "title": "Inverse Projection Transformation",
    "body": "2021/09/03 -  Depth and Inverse Projection : RGB + Depth = 3D Back-Projected Points When an image of a scene is captured by a camera, we lose depth information as objects and points in 3D space are mapped onto a 2D image plane. This is also known as projective transformation , in which points in the world are converted to pixels on a 2D plane. &lt;/p&gt;However, what if we want to do the inverse ? That is, we want to recover and reconstruct the scene given only 2D image. To do that, we would need to know the depth or $Z$-component of each corresponding pixels. Depth can be represented as an image as shown in the figure above, with brigther intensity denoting points further away.  In this blog post, we will take a tour and understand the mathematics and concepts of performing back-projection from 2D pixel coordinates to 3D points. We will assume that a depth map is provided to perform the 3D reconstruction. The concept that we will go through are camera calibration parameters, projective transformation using intrinsic and its inverse, and coordinate transformation between frames.  Central Projection of Pinhole Camera Model : First and foremost, understanding the geometrical model of the camera projection serves as the core idea. What we are ultimately interested in is the depth, parameter $Z$. Here, we consider the simplest pinhole camera model with no skew or distortion factor. 3D points are mapped to the image plane $(u,v) = f(X,Y,Z)$. The complete mathematical model that describes this transformation can be written as $p = K[R|t]*P$. $$s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \begin{bmatrix} f_x &amp; 0 &amp; u_0 \\ 0 &amp; f_y &amp; v_0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} r_{11} &amp; r_{12} &amp; r_{13} &amp; t{1} \\ r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2 \\ r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3\end{bmatrix} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}$$, where $p$ is the projected point on the image plane $K$ is the camera intrinsics matrix $[R|t]$ is the extrinsic parameters describing the relative transformation of the point in the world frame to the camera frame $P$ or $[X, Y, Z, 1]$ represents the 3D point expressed in a predefined world coordinate system in Euclidean space Aspect ratio scaling $s$ controls how pixels are scaled in the $x$ and $y$ direction as focal length chnanges Intrinsic Parameter Matrix :  Camera Projective Geometry The matrix $K$ is responsible for projecting 3D points to the image plane. To do that, the following quantities must be defined as:* Focal length $(f_x, f_y)$: measure the position of the image plane with respect to the camera center. * Principal point $(u_0, v_0)$; the optical center of the image plane* Skew factor: the misalignment from a square pixel if the image plane axes are not perpendicular. In our example, this is set to zero. The most common way of solving all the paramters is using the checkerboard method, where several 2D-3D correspondences obtained through matching and solving the unknown parameters by means of PnP, Direct Linear Transform or RANSAC to improve robustness. With all the unknowns determined, we can finally proceed to recover the 3D points $(X,Y,Z)$ by applying the inverse.  Back-Projection : Suppose $(X,Y,Z,1)$ is in the camera coordinate frame, i. e. we do not need to consider the extrinsic matrix $[R|t]$. ## References- MEDIUM "
    }, {
    "id": 34,
    "url": "http://localhost:4000/depth.html",
    "title": "Depth Estimation: Basics and Intuition",
    "body": "2021/09/02 - Introduction: In computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting epipolar geometry. This article will focus on giving readers a background into depth estimation and the problems associated with it. How We View the World: Let‚Äôs start with how humans perceive depth in general. This will give us some valuable insights on depth estimation since many of the methods were derived from our human vision system. Both machine and human vision share similarities in the way image is formed. Theoretically, when light rays from a source hit surfaces, it reflects off and directs towards the back of our retina, projecting them and our eye processes them as 2D just like how an image is formed on an image plane. So how do we actually measure distance and understand our environment in 3D when the projected scene is in 2D? The mechanism at work here is that our brain starts to reason about the incoming visual signals by recognizing patterns such as the size, texture and motion about the scene known as depth cues. There is no distance information about the image but somehow we can interpret and recover depth information effortlessly. These cues allow us to view objects and surfaces which are supposedly on flat images as 3D. How to Destroy Depth: Interpreting these depth cues begins with how scenes are projected to perspective view in humans and camera vision. On the other hand, an orthographic projection to front view or side view is one which destroys all depth information. Consider the above image. An observer could disentangle which aspect of the house is nearer to him/her as seen in the left image. However, it is totally impossible to distinguish relative distances from the right image. Even the background might be lying on the same plane as the house. Judging Depth Using Cues: There are basically 4 categories of depth cues: static monocular depth from motion, binocular and physiological cues. We subconsciously take advantage of these signals to perceive depth remarkably well. Pictorial Depth Cues: Our ability to perceive depth from a single image depends on the spatial arrangement of things in the scene. Below, I have summarized some of the hints that enable us to reason about the distance of different objects. It may already feel natural to you from your daily interaction.  Size of objects Texture Linear PerspectiveAn interesting study was conducted at UC, Berkeley and they show experimentally that when the horizon is viewable, there is an overwhelming tendency for us to exploit this property to quickly perceive depth. Depth Cues from Motion (Motion Parallax): This should not be surprising to you as well. When you, as an observer, is in motion, things around you pass by faster than the one that is farther away. The farther something appears, the slower it seems to pass away from the observer. Depth Cues from Stereo Vision (Binocular Parallax): The difference in view observed by your left and right eye is known as retina disparity. This phenomenon is also known as stereopsis: ability to perceive depth due to 2 different perspectives of the world. By comparing iamges from the retinas in the two eyes, the brain computes distance. The greater the disparity, the closer the things are around you. Depth Estimation in Computer Vision: The goal of depth estimation is to obtain a representation of the spatial structure of a scene, recovering the three-dimensional shape and appearance of objects in imagery. This is also known as the inverse problem, where we seek to recover some unknowns given insufficient information to fully specify the solution. So how do machines actually perceive depth? Can we somehow transfer some of the ideas discussed above? The earliest algorithm with impressive results begin with depth estimation using stereo vision back in the 90s. A lot of progress was made on dense stereo correspondence algorithm. Researchers were able to utilize geometry to constrain and replicate the idea of stereopsis mathemtically and at the same time running at real-time. As for monocular depth estimation, it recently started to gain popularity by using neural networks to learn representation that distills depth directly. Through neural networks, depth cues are implicitly learned through gradient-based methods. Besides this, there has been great advancement in self-supervised depth estimation. In this method, a model is trained to predict depth by means of optimizing a proxy signal. No ground truth label is needed in the training process. Most research either exploits geometrical cues such as multi-view geometry or epipolar geometry to learn depth. Depth Estimation from Stereo Vision: The main idea of solving depth using a stereo camera involves the concept of triangulation and stereo matching. The former depends on good calibration and rectification to constrain the problem so that it can be modelled on a 2D plane known as epipolar plane, which greatly reduces the latter (stereo matching) to a line search along the epipolar line.  Analogous to binocular parallax, once we are able to match pixel correspondences between the 2 views, the next task is to obtain a representation that encodes the difference. This representation is known as disparity. The formula to obtain depth from disparity can be worked out from similar triangles. To summarize the steps:  identify similar points from feature descriptors Match feature correspondence using a matching cost function Using epipolar geometry, find and match correspondence in one picture frame to the other. A matching cost function is used to measure the pixel dissimilarity.  Compute disparity from known correspondences $d = x1 - x2$ as shown in the above image.  Compute depth from the known disparity $ z = (f*b)/d$. Age of Deep Learning: Deep learning excels in high-level perceptual and cognitive task such as recognition, detection and scene understanding. Depth perception falls into this category and likewise should be a natural way forward. The seminal work of estimating depth directly from a monocular image started from Saxena. They learned to regress depth directly from monocular cues in 2D images via supervised learning, by minimizing a regression loss. Since then, many varieties of approaches have been proposed to improve the representation learning by proposing new architectures or loss functions. Self-Supervised Monocular Depth Estimation using Stereo: In this framework, the model will predict the disparities $d_l$ and $d_r$ only from the left RGB, $I_l$. Similar to the above method, a spatial transformer network warps the RGB image pair $I_l$, $I_r$ using the disparity. So the paired view can be synthesized and a reconstruction loss between the reconstructed views $I_{pred_l}$ and $I_{pred_r}$ and the target views $I_l$ and $Ir$ is used to supervise the training. Self-Supervised Depth Estimation using SfM Framework: This method frames the problem as learning to generate a novel view from a video sequence. The task of the neural network is to generate the target view $I_t$ from source view by taking image at different time step $I, I_{t-1}, I_{t+1}$ and applying a learnt transformation from a pose network to perform the image warping. Training was made possible by treating the warped view synthesis as supervision in a differentiable manner using a spatial transformer network. At inference time, the depth CNN would predict depth from a single RGB image. Do note that these methods do have some shortcomings such as unable to determine scale and modelling moving objects as described in the next section. CNN Depth Cues and Bias Learnt: Understanding and deciphering the black box has been an ongoing research in interpretable machine learning. In the context of depth estimation, a few works have started investigating what depth cues do neural network relies on or the inductive biased learnt from a particular dataset. They found out that:  Position of objects relative to ground contact point provides strong contextual information Shape does not matter but shadow does In an experiment, by placing an arbitrary object with artificial casted shadow, the model would estimate depth reasonably even if it is not available during training. Why is Measuring Depth So Difficult?: Let‚Äôs try to understand some of the fundamental problems of depth estimation. The main culprit lies in the projection of 3D views to 2D images where depth information is lost. Another problem is deeply seeded when there are motion and moving objects. Depth Estimation is Ill-Posed: On many depth estimation paper, many authors mention that the problem of estimating depth from a single RGB image is ill-posed inverse problem. What this means is that many 3D scenes observed in the world can indeed correspond to the same 2D plane. Scale-Ambiguity for Monocular Depth Estimatio: Recall that adjusting the focal length will proportionally scale the points on the image plane. Now, suppose we scale the entire scene, $X$ by some factor $k$ and at the same time, scale the camera matrices $P$ by the factor of $1/k$, the projections of the scene points in the image remains exactly the same. $$ x = PX = (1/k)P * kX $$That is to say, we can never recover the exact scale of the actual scene from the image alone! Note that this issue exists for monocular base techniques, as the scale can be recovered for a stero rig with a known baseline. Projection Ambiguity: Suppose we perform a geometric transformation of the scene. It is possible that after the transformation, these points will map to the same location on the plane. There exists not only projective ambiguity, but also affine and similarity ambiguity as well. Properties that Degrade Matching: For stereo based or multi-view depth estimation that requires triangulation, it usually involves the pipeline of Detect-Describe-Match. Matching becomes very difficult when the scene is taken from an extremely different viewpoint or varying changes in illumination between images. An extreme case is given in the above picture. These problematic cases include textureless regions (where many pixels will ahve the same pixel intensity), reflective surfaces, repetitive patterns or occlusions. Also. violating the Lambertian property (Lambertian surfaces refer to surfaces that appear to have teh same brightness no matter where it is viewed from). When images that show the same scene from 2 different view, the corresponding brightness intensity may not be equivalent due to non-ideal diffuse reflection. Moving Objects Violate the Static Assumption for SfM Methods: Dynamic objects in the scene further complicates the estimation process. Depth estimation via Structure from Motion involves a moving camera and consecutive static scenes. This assumption must hold for matching and aligning pxiels. This assumption breaks when there are moving objects in the scene. To this end, many researchers have looked into several methods to model moving objects in the scene by incorporating velocity information using optical flow or by using instance segmentation mask to model the object‚Äôs motion from one frame to another. References:  medium article"
    }, {
    "id": 35,
    "url": "http://localhost:4000/rigid2.html",
    "title": "3D Rigid Body Motion (Part 2)",
    "body": "2021/09/01 - Rotation Vectors and Euler AnglesRotation Vectors: With a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-freedom 3D rigid body motion? Obviously, the matrix representation has at least the following disadvantages:  $SO(3)$ has a rotation matrix of 9 qunatities, but a 3D rotation only has 3 degrees of freedom. Therefore, the matrix expression is redundant.  Similarly, the transformation matrix expresses 6 degree-of-freedom transformation with 16 quantities. So, is there a more compact representation? The rotation matrix itself has constraints: it must be an orthogonal matrix with a determinant of 1. The same is true for the transformation matrix. These constraints make the solution more difficult when you want to estimate or optimize a rotation matrix/transform matrix. Therefore, we hope that there is a way to describe rotation and translation more compactly. For example, is it feasible to express the rotation with a three-dimensional vector and express transformation with a six-dimensional vector?Obviously, a rotation can be described by a rotation axis and a rotation angle. Thus, we can use a vector whose direction is parallel with the axis of rotation, and the length is equal to the angle of rotation, which is called the rotation vector (or angle-axis/axis-angle). Only a three-dimensional vector is needed here to describe the rotation. Similarly, we may also use a rotation vector and a translation vector and a to express a transformation for a transformation matrix. The variable at this time is exactly six dimensions. Consider a rotation represented by $R$. If described by a rotation vector, assuming that the rotation axis is a unit-length vector $n$ and the angle is $\theta$, then the vector $\theta n$ can also describe this rotation. So, we have to ask, what is the connection between the two expressions? In fact, it is not difficult to derive their conversion relationship. The conversion from the rotation vector to the rotation matrix is shown by the Rodrigues‚Äô formula. Since the derivation process is a little complicated, it is not described here. Only the result of the conversion is given. $$ R = \cos \theta I + (1-\cos \theta) n n^T + \sin \theta n^{\wedge}$$. The symbol $\wedge$ is a vector to skew-symmetric conversion. Conversely, we can also calculate the conversion from a rotation matrix to a rotation vector. For the corner $\theta$, taking the trace of both sides, we have:$$\begin{split} &amp; tr(R) = \cos \theta tr(I) + (1-\cos \theta) tr(nn^T) + \sin \theta tr(n^{\wedge}) \\ &amp; = 3 \cos \theta + (1 - \cos \theta) \\ &amp; = 1+ 2 \cos \theta \end{split}$$. Therefore, $$\theta = \arccos (\frac{tr(R) - 1}{2})$$. Regarding the axis $n$, since the rotation axis does not change after the rotation, we have: $$Rn=n$$. So, the axis $n$ is the eigenvector corresponding to the amtrix $R$‚Äôs eigenvalue $1$. Solving this equation and normalizing it gives the axis of rotation. References:  Introduction to Visual SLAM"
    }, {
    "id": 36,
    "url": "http://localhost:4000/rigid.html",
    "title": "3D Rigid Body Motion (Part 1)",
    "body": "2021/08/31 - Introduction: In this article, I will introduce one of the fundamental problems of visual SLAM: How to describe a rigid body‚Äôs motion in 3-dimensional space ? Intuitively, we certainly know that this consists of one rotation plus one translation . The translation part does not really have any problems, but the rotation part is questionable. I will introduce the meaning of rotation matrices, quaternions, Euler angles and how they are computed and transformed. Rotation Matrix: Points, Vectors, and Coordinate Systems: The space of our daily life is 3-dimensional, so we are born to be used to 3D movements. The 3D space consists of three axes, so the position of one spatial point can be specified by three coordinates. However, we should now consider a rigid body , which has its position and orientation . The camera can also be viewed as a rigid body in three dimensions, so what we care about in Visual SLAM are the problem of the camera‚Äôs position and orientation. Combined, we can say, ‚Äúthe camera is at the $(0,0,0)$ point, facing the front‚Äù. Let‚Äôs describe this in a mathematical term. We start from the basic content: points and vectors. Points are the basic element in space, no length, no volume. Connecting the two points forms a vector. A vector can be thought of as an arrow pointing from one point to another. Here we need to warn you not to confuse the vector with its coordinates. A vector is one thing in space, such as $a$. Here, $a$ does not need to be associated with several real numbers. We can naturally talk about the plus or minus operation of two vectors, without relating to any real numbers. Only when we specify a coordinate system in this 3D space can we talk about the vector‚Äôs coordinates in this system, finding several real numbers corresponding to this vector. With the knowledge of linear algebra, the coordinates of a point in 3D space can be described as $\mathbb{R}^3$. How to do we describe this? Suppose that in this linear space, we fined a set of base $(e_1, e_2, e_3)$ , then, an arbitrary vector $a$ has a coordinate under this base: $$ a = \begin{bmatrix} e_1 &amp; e_2 &amp; e_3 \end{bmatrix} \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = a_1 e_1 + a_2 e_2 + a_3 e_3 $$. Here, $(a_1, a_2, a_3)^T$ is called $a$‚Äôs coordinates. The coordinates‚Äô specific values are related to the vector itself and the selection of the bases. In $\mathbb{R}^3$, the coordinate system usually consists of $3$ orthogonal coordinate axes (it can also be non-orthogonal, but it is rare in practice). For example, given $x$ and $y$ axis, the $z$ axis can be determined using the right-hand (or left-hand) rule. According to different definitions, the coordinate system is divided into left-handed and right-handed. The third axis of the left-hand rule is opposite to the right-hand rule. Most 3D libraries use right-handed coordinates. Based on basic linear algebra knowledge, we can talk about the operations between vectors/vectors, vectors/numbers, such as scalar multiplication, vector addition, subtraction, inner product, outer product and so on. For $a,b \in \mathbb{R}^3$, the inner product of $a,b$ can be written as: \$\$ a \cdot b = a^Tb = \sum_{i=1}^3 a_i b_i = |a\||b| \cos(&lt;a,b&gt;)\$\$where $ &lt;a. b&gt; $ refers to the angle between the vector $a, b$. The inner product can also describe the projection relationship between vectors. $$ a \times b = \begin{Vmatrix} e_1 &amp; e_2 &amp; e_3 \\ a_1 &amp; a_2 &amp; a_3 \\ b_1 &amp; b_2 &amp; b_3 \end{Vmatrix} = \begin{bmatrix} a_2b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1 \end{bmatrix} = \begin{bmatrix} 0 &amp; -a_3 &amp; a_2 \\ a_3 &amp; 0 &amp; -a_1 \\ -a_2 &amp; a_1 &amp; 0 \end{bmatrix} b = a \wedge b$$. The result of the outer product is a vector whose direction is perpendicular to the two vectors, and the length is $|a||b|\sin(&lt;a,b&gt;)$, which is also the area of the quadrilateral of the two vectors. From the outer product operation, we introduce the $\wedge$ operator here, which means writing $a$ as a skew-symmetric matrix . You can take $\wedge$ as a skew-symmetric symbol. It turns the outer product $a \times b$ into the multiplication of the matrix and the vector $a \wedge b$ is a linear operation. This symbol will be used frequently in the following sections. It is a one-to-one mapping, meaning that for any vector, it corresponds to a unique anti-symmetric matrix, and vice versa: [a \wedge = \begin{bmatrix} 0 &amp; -a_3 &amp; a_2 \ a_3 &amp; 0 &amp; -a_1 \ -a_2 &amp; a_1 &amp; 0 \end{bmatrix}] At the same time, note that the vector operations such as addition, subtraction, inner and outer products can be calculated even when we do not have their coordinates. For example, although the inner product can be expressed by the sum of the two vectors‚Äô product when we know the coordinates, the length and angle can also be calculated even if their coordinates are unknown. Therefore, the inner product result of the two vectors is independent of the selection of the coordinate system. Euclidean Transforms between Coordinate Systems: We often define a variety of coordinate systems in the real scene. In robotics, you define one coordinate system for each link and joint; in 3D mapping, we also define a coordinate system for each cuboid and cylinder. If we consider a moving robot, it is common practice to set a stationary inertial coordinate system (or world coordinate system), such as the $x_W, y_W, z_W$ defined in the picture above. Meanwhile, the camera or robot is a moving coordinate system, such as coordinate system defined by $x_C, y_C, z_C$. We might ask: a vector $p$ in the camera system may have coordinates $p_c$; and in the world coordinate system, its coordinates maybe $p_w$ .  Then what is the conversion between two coordinates? It is necessary to first obtain the coordinate values of the point in the camera system and then use the transform rule to do the coordinate transform. We need a mathematical way to describe this transformation. As we will see later, we can describe it with a transform matrix $T$. Intuitively, the motion between two coordinate systems consists of a rotation plus a translation , which is called rigid body motion . Obviously, the camera movement is rigid. During the rigid body motion, the length and angle of the vector will not change. Imagine that you throw your phone into the air and there may be differences in spatial position and orientation. But the length and the angle of each face will not change. At this point, we say that the phone‚Äôs motion is Euclidean. The Euclidean transform consists of rotation and translation . Let‚Äôs first consider the rotation. We have a unit-length orthogonal base $(e_1, e_2, e_3)$. After a rotation it becomes $(e_1‚Äô, e_2‚Äô, e_3‚Äô)$. Then, for the same vector $a$ (the vector does not move with the rotation of the coordinate system). its coordinates in these two coordinate systems are $[a_1, a_2, a_3]^T$ and $[a_1‚Äô, a_2‚Äô, a_3]^T$. Because the vector itself has not changed, according to the definition of coordinates, there are: [[e_1, e_2, e_3] \begin{bmatrix} a_1 \ a_2 \ a_3 \end{bmatrix} = [e_1‚Äô, e_2‚Äô, e_3‚Äô] \begin{bmatrix} a_1‚Äô \ a_2‚Äô \ a_3‚Äô \end{bmatrix}] To describe the relationship between the two coordinates, we multiply the left and right side of the above equation by $\begin{bmatrix} e_1^T \\ e_2^T \\ e_3^T \end{bmatrix}$, then the matrix on the left becomes an identity matrix, so: $$\begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} \triangleq Ra‚Äô$$. We take the intermediate matrix out and define it as a matrix $R$. This matrix consists of the inner product between the two sets of bases, describing the same vector‚Äôs coordinate transformation relationship before and after the rotation. It can be said that the matrix $R$ describes the rotation itself . So we call it the rotation matrix. Meanwhile, the components of the matrix are the inner product of the two coordinate system bases. Since the base vector‚Äôs length is $1$, it is actually the cosine of the angle between the base vectors. So this matrix is also called direction cosine matrix. The rotation matrix has some special properties.  In fact, it is an orthogonal matrix with a determinant of $1$. Conversely, an orthogonal matrix with a determinant of 1 is also a rotation matrix. So you can define a set of $n$ dimensional rotation matrices as follows:       $$SO(n) = \{ R \in \mathbb{R}^{n \times n}   RR^T = I, det(R) =1 \} $$   $SO(n)$ refers to the special orthogonal group. This set consists of a rotation matrix of $n$ dimensional space, in particular, $SO(3)$ refers to the rotation of the three-dimensional space. In this way, we can talk directly about the rotation transformation between the two coordinate systems without having to start from the bases. Since the rotation matrix is orthogonal, its inverse (i. e. , transpose) describes an opposite rotation. According to the above definition, there are: [a‚Äô = R^{-1}a = R^Ta] Obviously, the $R^T$ represents an opposite rotation. In the Euclidean transformation, there is a translation in addition to rotation. Consider the vector $a$ in the world coordinate system. After a rotation (depicted by $R$) and a translation of $t$, we get $a‚Äô$. Then we can put the rotation and translation together, and have:$$a‚Äô= Ra+t$$,where $t$ is called a translation vector. Compared to the rotation, the translation part simply adds the translation vector to the coordinates after the rotation , which si very simple. By the above formula, we completely describe the coordinate transformation relationship using a rotation matrix $R$ and a translation vector $t$. In practice, we may define the coordinate system 1 and 2, then the vector $a$ under the two coordinates is $a_1, a_2$. The relationship between the two systems should be: $$a_1 = R_{12} a_2 + t_{12}$$. Here, $R_{12}$ means the ‚Äúrotation of the vector from system 2 to system 1‚Äù. About $t_{12}$, readers may just take it as a translation vector without wondering about its physical meaning. In fact, it corresponds to a vector from the system 1‚Äôs origin pointing to system 2‚Äôs origin, and the coordinates are taken under tsystem 1. So I suggest you to understand it as ‚Äúa vector from 1 to 2‚Äù. But the reverse $t_{21}$, which is a vector from $2$‚Äôs origin to $1$‚Äôs origin, whose coordinates are taken in system $2$, is not equal to $-t_{12}$. It is also related to the rotation of the two systems. Therefore, when beginners ask the question ‚ÄúWhat are my coordinates?‚Äù, we need to clearly explain this sentence‚Äôs meaning. Here, ‚Äúmy coordinates‚Äù normally refers to the vector from the world system $W$ pointing to the origin of the camera system $C$, and then take the coordinates in the world‚Äôs base. Corresponding to the mathematical symbol, it should be the value of $t_{WC}$. For the same reason, it is not $-t_{CW}$ but actually $-R^T_{CW} t_{CW}$. Transform Matrix and Homogeneous Coordinates: The formula $a‚Äô = Ra+t$ fully expresses the rotation and the translation of Euclidean space, but there is still a small problem: the transformation relationship here is not a linear relationship. Suppose we made two transformations: $R_1,t_1$ and $R_2,t_2$:$$b = R_1 a + t_1, c = R_2 b + t_2$$. So the transformation from $a$ to $c$ is: $$c = R_2 (R_1 a + t_1) + t_2$$. This form is not elegant after multiple transformations. Therefore, we introduce homogeneous coordinates and transformation matrices, rewriting the formula:$$ \begin{bmatrix} a‚Äô \\ 1 \end{bmatrix} = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix} \begin{bmatrix} a \\ 1 \end{bmatrix} \triangleq T \begin{bmatrix} a \\ 1 \end{bmatrix}$$. This is a mathematical trick: we add $1$ at the end of the 3D vector and turn it into a 4D vector called homogeneous coordinates. For this four-dimensional vector, we can write the rotation and translation matrix, making the whole relationship a linear relationship. In this formula, the matrix $T$ is called transform matrix. We temporarily use $\tilde{a}$ to represent the homogeneous coordinates of $a$. Then, relying on homogeneous coordinates and transformation matrices, the superposition of the two transformations can have a good form: $$\tilde{b} = T_1 \tilde{a}, \tilde{c} = T_2 \tilde{b} \Rightarrow T_2 T_1 \tilde{a}$$. But the symbols that distinguish between homogeneous and non-homogeneous coordinates are annoying, because here we only need to add 1 at the end of the vector or remove 1 to turn it into a normal vector. So, without ambiguity, we will write it directly as $b = Ta$ and by default we just assume a homogeneous coordinate conversion is made if needed. The transformation matrix $T$ has a special structure: the upper left corner is the rotation matrix, the right side is the translation vector, the lower-left corner is $0$ vector, and the lower right corner is $1$. This set of transform matrix is also known as the special Euclidean group: $$SE(3) = \{ T = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix} \in \mathbb{R}^{4 \times 4}| R \in SO(3), t \in \mathbb{R}^3 \}$$. Like $SO(3)$, the inverse of the transformation matrix represents an inverse transformation: $$T^{-1} = \begin{bmatrix} R^T &amp; -R^T t \\ 0^T &amp; 1 \end{bmatrix}$$. Again, we use the notation of $T_{12}$ to represent a transformation from 2 to 1. Because the conversion between homogeneous and non-homogeneous coordinates is actually very easy, it is assumed that the conversion from homogeneous coordinates to normal coordinates is already done. Summary: First, we introduced the vector and its coordinate representation and introduced the operation between the vectors; then, the motion between the coordinate systems is described by the Euclidean transformation, which consists of translation and rotation. The rotation can be described by the rotation matrix $SO(3)$, while the translation is directly described by an $\mathbb{R}^3$ vector. Finally, if the translation and rotation are placed in a matrix, the transformation matrix $SE(3)$ is formed. References:  Introduction to Visual SLAM"
    }, {
    "id": 37,
    "url": "http://localhost:4000/homogeneous.html",
    "title": "Homogeneous Coordinates and Projective Geometry",
    "body": "2021/08/31 - Introduction to Projective Geometry: Most of the time when working with 3D, we are thinking in terms of Euclidean geometry-that is, coordinates in three-dimensional space ($X$, $Y$ and $Z$). However, there are certain situations where it is useful to think in terms of projective geometry instead. Projective geometry has an extra dimension, called $W$, in addition to the $X$, $Y$, and $Z$ dimensions. This four-dimensional space is called projective space and coordinates in projective space are called homogenous coordinates. For the purposes of 3D software, the terms projective and homogeous are basically interchangeable with 4D. Not Quaternions: Quaternions look a lot like homogeneous coordinates. Both are 4D vectors, commonly depicted as $(X,Y,Z,W)$. However, quaternions and homogeneous coordinates are different concepts, with different uses. An Analogy in 2D: First, let‚Äôs look at how projective geometry works in 2D, before we move on to 3D.  Imagine a projector that is projecting a 2D image onto a screen. It‚Äôs easy to identify the $X$ and $Y$ dimensions of the projected image.  Now, if you step back from the 2D image and look at the projector and the screen, you can see the $W$ dimension, too. The $W$ dimension is the distance from the projector to the screen .  So what does the $W$ dimension do, exactly? Imagine what would happen to the 2D image if you increased or decreased $W$-that is, if you increased or decreased the distance between the projector and the screen. If you move the projector closer to the screen, the whole 2D image becomes smaller. If you move the projector away from the screen, the 2D image becomes larger. As you can see, the value of $W$ affects the size (a. k. a scale) of the image . Applying it to 3D: There is no such thing as a 3D projector (yet), so its‚Äô harder to imagine projective geometry in 3D, but the $W$ value works exactly the same as it does in 2D. When $W$ increases, the coordinates expands (scales up). When $W$ decreases, the coordinates shrinks (scales down). The $W$ is basically a scaling transformation for the 3D coordinates . When $W = 1$: The usual advice for 3D programming beginners is to always set $W=1$ whenever converting a 3D coordinate to 4D coordinate. The reason for that is that when you scale a coordinate by a 1 it doesn‚Äôt shrink or grow, it just stays the same size. So, when $W=1$, it has no effect on the $X$, $Y$, or $Z$ values . For this reason, when it comes to 3D computer graphics, coordinates are said to be correct only when $W=1$. If you tried to render with $W=0$ your program would crash when it attempted to divide by zero. With $W&lt;0$ everything would flip unside-down and back-to-front. Mathematically speaking, there is no such thing as an incorrect homogeneous coordinate.  Using coordinates with $W=1$ is just a useful convention for the 3D computer grahics . The Math: Now, let‚Äôs look at some actual numbers, to see how the math works.  Let‚Äôs say that the projector is $3$ meters away from the screen, and there is a dot on the 2D image at the coordinate $(15, 21)$. This gives us the projective coordinate vector $(X,Y,W) = (15,21,3)$.  Now imagine that the projector was pushed closer to the screen so that the distance was $1$ meter. The closer the project gets to the screen, the smaller the image becomes. The projector has moved three times closer, so the image becomes three times smaller. If we take the original coordinate vector and divide all the values by three, we get the new vector where $W=1$:  $$(\frac{15}{3}, \frac{21}{3}, \frac{3}{3}) = (5,7,1)$$. The dot is now at coordinate $(5,7)$. This is how an incorrect homogeneous coordinate is converted to a correct coordinate: divide all the values by $W$. The process is exactly the same for 2D and 3D coordinates. Dividing all the values in a vector is done by a scalar multiplication with the reciprocal of the divisor. Here is a 4D example:  $$\frac{1}{5}(10, 20, 30, 5) = (\frac{10}{5}, \frac{20}{5}, \frac{30}{5}, \frac{5}{5}) = (2,4,6,1)$$ Uses of Homogeneous Coordinates in Computer Graphics: As mentioned earlier, in regard to 3D computer graphics, homogeneous coordinates are useful in certain situations. We will look at some of those situations here. Translation Matrices for 3D Coordinates:  A four-column matrix can only be multiplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors. Rotation and scaling transformation matrices only require three columns. But, in order to do translation, the matrices need to have at least four columns . This is why transformations are often $4 \times 4$ matices. However, a matrix with four columns cannot be multiplied by a 3D vector, due to the rules of matrix multiplication. A four-column matrix can only be mulitplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors. The 4th dimension $W$ is usually unchanged, when using homogeneous coordinates in matrix transformation. $W$ is set to $1$ when converting 3D coordinates into 4D, and is usually still $1$ after the transformation matrices are applied, at which point it can be converted back into a 3D coordinate by ignoring $W$. This is true for all translation, rotation, and scaling transformations, which by far are the most common types of transformations. The notable exception is projection matrices, which do affect the $W$ dimension. Perspective Transformation: In 3D, perspective is the phenomenon where an object appears smaller the further away it is from the camera. A far-away mountain can appear to be smaller than a cat, if the cat is close enough to the camera. Perspective is implemented in 3D computer graphics by using a transformation matrix that changes the $W$ element of each vertex. After the camera matrix is applied to each vertex, but before the projection matrix is applied, the $Z$ element of each vertex represents the distance away from the camera. Therefore, the larger $Z$ is, the more the vertex should be scaled down. The $W$ dimension affects the scale, so the projection matrix just changes the $W$ based on the $Z$ value. Here is an example of a perspective projection matrix being applied to a homogeneous coordinate:  $$ \begin{bmatrix} 1&amp;0&amp;0&amp;0 \\ 0&amp;1&amp;0&amp;0&amp; \\ 0&amp;0&amp;1&amp;0 \\ 0&amp;0&amp;1&amp;0 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1\end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1 \end{bmatrix} $$ Notice how the $W$ value is changed to $4$, which comes from the $Z$ value. After the perspective projection matrix is applied, each vertex undergoes perspective division. Perspective division is just a specific term for converting the homogeneous coordinate back to $W=1$, as explained earlier in the article. Continuing with the example above, the perspective division step would look like this:  $$\frac{1}{4}(2,3,4,4) = (0. 5,0. 75, 1,1)$$ After perspective division, the $W$ value is discarded, and we are left with a 3D coordinate that has been correctly scaled according to a 3D perspective projection. Positioning Directional Lights: One property of homogeneous coordinates is that they allow you to have points at infinity (infinite length vectors), which is not possible with 3D coordinates. Points at infinity occur when $W=0$. If you try to convert a $W=0$ homogeneous coordinate into a normal $W=1$ coordinate, it results in a bunch of divide-by-zero operations:  $$ \frac{1}{0}(2,3,4,0) = (\frac{2}{0}, \frac{3}{0}, \frac{4}{0}, \frac{0}{0})$$. This means that homogeneous coordinates with $W=0$ can not be converted back into 3D coordinates. What use does this have? Well, directional lights can be thought of as point lights that are infinitely far away. When a point light is infinitely far away, the rays of light become parallel, and all of the light travels in a single direction. This is basically the definition of a directional light. So, traditionally, in 3D graphics, directional lights are differentiated from point lights by the value of $W$ in the position vector of the light. If $W=1$, then it is a point light. If $W=0$, then it is a directional light. This is more of a traditional convention, rather than a useful way to write lighting code. Directional lights and point lights are usually implemented with separate code, because they behave differently. Summary: Homogeneous coordinates have an extra dimension called $W$, which scales the $X$, $Y$, and $Z$ dimensions. Matrices for translation and perspective projection can only be applied to homogeneous coordinates, which is why they are so common in 3D computer graphics. The $X$, $Y$, and $Z$ values are said to be correct when $W=1$. Any homogeneous coordinates can be converted to have $W=1$ by dividing all four dimensions by the $W$ value, except if $W=0$. When $W=0$, the coordinate represents a point at infinity (a vector with infinite length), and this is often used to denote the direction of directional lights. References:  Tomdalling‚Äôs Blog Post Image Processing and Computer Vision Lecture Notes"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




</ul>
<form class="bd-search hidden-sm-down" onsubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small" id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
    

    
    
    
<!-- Begin post excerpts, let's highlight the first 4 posts on top -->
<div class="row remove-site-content-margin">
    
    <!-- latest post -->
    
    <div class="col-md-6">
    <div class="card border-0 mb-4 box-shadow">   
    <a href="/gan.html">
    <div class="topfirstimage" style="background-image: url( /assets/images/gan.png); height: 200px;    background-size: cover;    background-repeat: no-repeat;"></div>     
    </a>
    <div class="card-body px-0 pb-0 d-flex flex-column align-items-start">
    <h2 class="h4 font-weight-bold">
    <a class="text-dark" href="/gan.html">All About Training GAN</a>
    </h2>
    <p class="excerpt">
         Generative Adversarial Networks Ultimately, if everything goes well, the generator learns the true distribution of the training data...
    </p>
    <div>
        <small class="d-block text-muted">
            In <span class="catlist">
                
                <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                
                </span>                   
        </small>
        <small class="text-muted">
            Sep 14, 2021
        </small>
    </div>
    </div>
    </div>
    </div>
    
    <div class="col-md-6">
        
        <!-- second latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/extrinsic.html">
                 <img class="w-100" src="http://localhost:4000/assets/images/extrinsic.png" alt="Dissecting the Camera Matrix (Part 2)">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/extrinsic.html">Dissecting the Camera Matrix (Part 2)</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Sep 14, 2021
                    </small>
                </div>
            </div>
        
        <!-- third latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/template.html">
                 <img class="w-100" src="/assets/images/depth/1.jpg" alt="Image Processing: the Basics">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/template.html">Image Processing: the Basics</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Sep 14, 2021
                    </small>
                </div>
            </div>
        
        <!-- fourth latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/pytorch.html">
                <img class="w-100" src="/assets/images/pytorch.png" alt="Advanced PyTorch: Things You Didn't Know">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/pytorch.html">Advanced PyTorch: Things You Didn't Know</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#pytorch">PyTorch</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Sep 12, 2021
                    </small>
                </div>
            </div>
        
    </div>
    
</div>
    
<!-- Sticky - add sticky tag to the post you want to highlight here - tags: [sticky] -->
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

<div class="jumbotron jumbotron-fluid jumbotron-home pt-0 pb-0 mt-3 mb-2rem bg-lightblue position-relative">
    <div class="pl-4 pr-0 h-100 tofront">
        <div class="row justify-content-between">
            <div class="col-md-6 pt-6 pb-6 pr-lg-4 align-self-center">
                <h1 class="mb-3">Homogeneous Coordinates and Projective Geometry</h1>
                <p class="mb-3 lead">
                    In this article, I'm going to explain homogeneous coordinates (a.k.a 4D coordinates) as simply as I can. We need projective geometry ...
                </p>
                <a href="/homogeneous.html" class="btn btn-dark">Read More</a>
            </div>
            <img class="col-md-6 d-none d-md-block pr-0" src="/assets/images/home.png" align="middle">	
            </div>
        </div>
    </div>
</div> 




    


 <!--endif page url is / -->
    


<!-- Now the rest of the posts with the usual loop but with an offset:4 on the first page so we can skeep the first 4 posts displayed above -->
    
<div class="row mt-3">
   
    <div class="col-md-8 main-loop">
        
        <h4 class="font-weight-bold spanborder"><span>All Stories</span></h4>
        

        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/gan.html">All About Training GAN</a>
	</h2>
	<p class="excerpt">
	    Generative Adversarial Networks Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 14, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/gan.html">
	<img height="150px" width="200px" src="/assets/images/gan.png" alt="All About Training GAN">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/extrinsic.html">Dissecting the Camera Matrix (Part 2)</a>
	</h2>
	<p class="excerpt">
	   This is the second part of our journey to master the camera matrix. In this blog post, we will study the extrinsic camera parameters. Extrinsic matrix describes the camera's location ...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 14, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/extrinsic.html">
	<img height="150px" width="200px" src="/assets/images/extrinsic.png" alt="Dissecting the Camera Matrix (Part 2)">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/template.html">Image Processing: the Basics</a>
	</h2>
	<p class="excerpt">
	    References &lt;a=href=""&gt; TheAILearner &lt;/a&gt;
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 14, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/template.html">
	<img height="150px" width="200px" src="/assets/images/depth/1.jpg" alt="Image Processing: the Basics">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/pytorch.html">Advanced PyTorch: Things You Didn't Know</a>
	</h2>
	<p class="excerpt">
	   This blog post is for those who know the basics of PyTorch but want to go a step further. We will be diving into principles and applications of deep learning via PyTorch.
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#pytorch">PyTorch</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 12, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/pytorch.html">
	<img height="150px" width="200px" src="/assets/images/pytorch.png" alt="Advanced PyTorch: Things You Didn't Know">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/no-bullshit.html">Guide to Linear Algebra (Part 1)</a>
	</h2>
	<p class="excerpt">
	   This article goes through the fundamentals of linear algebra. Linear algebra is the branch of mathematics concerning linear equations and their representations in vector spaces. This ...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#linear%20algebra">linear algebra</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 09, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/no-bullshit.html">
	<img height="150px" width="200px" src="/assets/images/linear.png" alt="Guide to Linear Algebra (Part 1)">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/point-cloud.html">Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision</a>
	</h2>
	<p class="excerpt">
	    Introduction In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is ...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 09, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/point-cloud.html">
	<img height="150px" width="200px" src="/assets/images/cnn.png" alt="Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/depth-intro.html">Depth Estimation - An Introduction</a>
	</h2>
	<p class="excerpt">
	    Paradigms for 3D Images Representation over a Plane As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the d...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 09, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/depth-intro.html">
	<img height="150px" width="200px" src="/assets/images/camera/camera.jpeg" alt="Depth Estimation - An Introduction">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/3d-packing.html">3D Packing for Self-Supervised Depth Estimation</a>
	</h2>
	<p class="excerpt">
	    Self-Supervised Scale-Aware SfM In self-supervised monocular SfM training, we aim to learn: &lt;ul&gt;&lt;li&gt; a monocular depth model $f_D = I \rightarrow D$ that predicts the sca...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#paper">paper</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 09, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/3d-packing.html">
	<img height="150px" width="200px" src="/assets/images/cnn.png" alt="3D Packing for Self-Supervised Depth Estimation">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/group.html">Batch Normalization and Group Normalization</a>
	</h2>
	<p class="excerpt">
	   Batch normalization is used in most state-of-the-art computer vision techniques to stabilize training, but it also suffers from drawbacks. Group normalization can be an awesome altern...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#deep%20learning">deep learning</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 08, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/group.html">
	<img height="150px" width="200px" src="/assets/images/bn.png" alt="Batch Normalization and Group Normalization">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/linear.html">An Intuitive Overview of Linear Algebra Fundamentals</a>
	</h2>
	<p class="excerpt">
	    Introduction  Why Learn Linear Algebra? This write up is an overview of some of the linear algebra fundamentals. It focuses on providing an intuitive/geometric review of some of the ...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#linear%20algebra">linear algebra</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 08, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/linear.html">
	<img height="150px" width="200px" src="/assets/images/linear.png" alt="An Intuitive Overview of Linear Algebra Fundamentals">
	</a>
	</div>

</div>

        
        
        
        <div class="mt-5">
         <!-- Pagination links -->
            
            <ul class="pagination"> 
              
                <li class="page-item disabled"><span class="prev page-link">¬´</span></li>
              

              
                
                <li class="page-item disabled"><span class="webjeda page-link">1</span></li>
                
              
                
                <li class="page-item"><a class="page-link" href="/page2">2</a></li>
                
              

              
                <li class="page-item"><a class="page-link" href="/page2">Next ¬ª</a></li>
              
            </ul>
                  
        </div>
        
    </div>
    
    <div class="col-md-4">
        <div class="sticky-top sticky-top-offset">
    <h4 class="font-weight-bold spanborder"><span>Featured</span></h4>  
    <ol class="list-featured">				
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/gan.html" class="text-dark">All About Training GAN</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/extrinsic.html" class="text-dark">Dissecting the Camera Matrix (Part 2)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/template.html" class="text-dark">Image Processing: the Basics</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/pytorch.html" class="text-dark">Advanced PyTorch: Things You Didn't Know</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#pytorch">PyTorch</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/no-bullshit.html" class="text-dark">Guide to Linear Algebra (Part 1)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#linear%20algebra">linear algebra</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/point-cloud.html" class="text-dark">Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/depth-intro.html" class="text-dark">Depth Estimation - An Introduction</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/3d-packing.html" class="text-dark">3D Packing for Self-Supervised Depth Estimation</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#paper">paper</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/group.html" class="text-dark">Batch Normalization and Group Normalization</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#deep%20learning">deep learning</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/linear.html" class="text-dark">An Intuitive Overview of Linear Algebra Fundamentals</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#linear%20algebra">linear algebra</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/disparity.html" class="text-dark">Depth from Disparity</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/camera.html" class="text-dark">Dissecting the Camera Matrix (Part 1)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/intrinsic.html" class="text-dark">Dissecting the Camera Matrix (Part 2)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/inverse.html" class="text-dark">Inverse Projection Transformation</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/depth.html" class="text-dark">Depth Estimation: Basics and Intuition</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/rigid2.html" class="text-dark">3D Rigid Body Motion (Part 2)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/rigid.html" class="text-dark">3D Rigid Body Motion (Part 1)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/homogeneous.html" class="text-dark">Homogeneous Coordinates and Projective Geometry</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
           
    </ol>
</div>     
    </div>
    
</div>





    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/custom.js"></script>
    

    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Seri Lee</strong></span>
                <span>Copyright ¬© <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                
            </div>
            </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
