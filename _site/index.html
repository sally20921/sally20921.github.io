<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Seri Lee Blog | Seri Lee Blog</title>

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Seri Lee Blog | This is where I write posts about my research field.</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Seri Lee Blog">
<meta property="og:locale" content="en_US">
<meta name="description" content="A Blog about Computer Vision and Deep Learning">
<meta property="og:description" content="A Blog about Computer Vision and Deep Learning">
<link rel="canonical" href="http://localhost:4000/">
<meta property="og:url" content="http://localhost:4000/">
<meta property="og:site_name" content="Seri Lee Blog">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Seri Lee Blog">
<script type="application/ld+json">
{"description":"A Blog about Computer Vision and Deep Learning","headline":"Seri Lee Blog","url":"http://localhost:4000/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"}},"@type":"WebSite","name":"Seri Lee Blog","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <link rel="stylesheet" href="/assets/css/custom.css">
    
    <!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">-->
    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="/assets/js/lib.js" defer></script>
    <script> 
	    MathJax = {
		    loader: {load: ['input/tex', 'ui/menu']},
		    tex: {
			    inlineMath: {'[+]': [['$','$']]},
			    displayMath: {'[+]':[['$$', '$$']]}
			}
	    }
   </script>
    <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
 <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>
  <script>
    function convert() {
      //
      //  Get the MathML input string, and clear any previous output
      //
      var input = document.getElementById("input").value.trim();
      output = document.getElementById('output');
      output.innerHTML = '';
      //
      //  Convert the MathMl to an HTML node and append it to the output
      //
      output.appendChild(MathJax.mathml2chtml(input));
      //
      //  Then update the document to include the adjusted CSS for the
      //    content of the new equation.
      //
      MathJax.startup.document.clear();
      MathJax.startup.document.updateDocument();
    }
  </script> 
</head>

<body class=" homefirstpage ">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Seri Lee Blog</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Author</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>




            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/_posts/2021-09-04-d3.html",
    "title": "Depth from Disparity",
    "body": "We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  üåü 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let‚Äôs consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 1,
    "url": "http://localhost:4000/_posts/2021-09-05-d2.html",
    "title": "Depth from Disparity",
    "body": "We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  üåü 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let‚Äôs consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 2,
    "url": "http://localhost:4000/_posts/2021-09-05-disparity.html",
    "title": "Depth from Disparity",
    "body": "We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  üåü 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let‚Äôs consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 3,
    "url": "http://localhost:4000/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 4,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": "Made by Seri @porfolio. "
    }, {
    "id": 5,
    "url": "http://localhost:4000/author-seri.html",
    "title": "Sal",
    "body": "                        Sal /span&gt;&lt;/h2&gt;        https://sites. google. com/snu. ac. kr/sally20921porfolio         Hi, I am Seri, the author of this blog.       &lt;/div&gt;                        &lt;/div&gt;    Posts by Sal:                   		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						            		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						            		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 04, 2021						        &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;: "
    }, {
    "id": 6,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "Authors:                                             Seri :       (View Posts)      Hi, I am Seri, the author of this blog.                           &nbsp;       &nbsp;                                      "
    }, {
    "id": 7,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories               computer vision:                                  		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 04, 2021						                                             Featured:    				                                          Depth from Disparity                          In                     computer vision,                                                                                           Depth from Disparity                          In                     computer vision,                                                                                           Depth from Disparity                          In                     computer vision,                                                                   "
    }, {
    "id": 8,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to Seri Lee Blog. We will reply as soon as possible!   "
    }, {
    "id": 9,
    "url": "http://localhost:4000/",
    "title": "Seri Lee Blog",
    "body": "    	"
    }, {
    "id": 10,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "‚Äù{{site. name}}‚Äù takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor‚Äôs preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google‚Äôs cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 11,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 12,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ ‚Äúsitemap. xml‚Äù   absolute_url }}   "
    }, {
    "id": 13,
    "url": "http://localhost:4000/disparity.html",
    "title": "Depth from Disparity",
    "body": "2021/09/05 - We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  üåü 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let‚Äôs consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 14,
    "url": "http://localhost:4000/d2.html",
    "title": "Depth from Disparity",
    "body": "2021/09/05 - We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  üåü 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let‚Äôs consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 15,
    "url": "http://localhost:4000/d3.html",
    "title": "Depth from Disparity",
    "body": "2021/09/04 - We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  üåü 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let‚Äôs consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




</ul>
<form class="bd-search hidden-sm-down" onsubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small" id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
    <!-- latest post -->
    
	<script>
    <div class="col-md-6">
    <div class="card border-0 mb-4 box-shadow">   
    <a href="/disparity.html">
    Depth from Disparity
    <img width="550px" height="200px" src="assets/images/depth/12.png" style="height: 210px;" align="middle">
    </a>
    </div>
    </div>
    </div>
        


<!-- Now the rest of the posts with the usual loop but with an offset:4 on the first page so we can skeep the first 4 posts displayed above -->
    



</div>

    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/custom.js"></script>
    

    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Seri Lee</strong></span>
                <span>Copyright ¬© <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                
            </div>
            </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</div></main>
</body>

</html>
