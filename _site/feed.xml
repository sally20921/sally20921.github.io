<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-04-05T20:20:33+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Seri Lee Blog</title><subtitle>This is where I write posts about my research field.</subtitle><entry><title type="html">3D Rigid Body Motion (Part 2)</title><link href="http://localhost:4000/rigid2.html" rel="alternate" type="text/html" title="3D Rigid Body Motion (Part 2)" /><published>2021-10-21T00:00:00+09:00</published><updated>2021-10-21T00:00:00+09:00</updated><id>http://localhost:4000/rigid2</id><content type="html" xml:base="http://localhost:4000/rigid2.html">&lt;!--more--&gt;

&lt;h2&gt; Rotation Vectors &lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.springer.com/gp/book/9789811649387&quot;&gt;Introduction to Visual SLAM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">The goal of this article is to introduce the rigid body geometry in 3-dimensional spaace: rotation matrix, transformation matrix, quaternion and Euler angle.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/rigid.png" /><media:content medium="image" url="http://localhost:4000/assets/images/rigid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Geometry</title><link href="http://localhost:4000/geometry.html" rel="alternate" type="text/html" title="Geometry" /><published>2021-09-16T00:00:00+09:00</published><updated>2021-09-16T00:00:00+09:00</updated><id>http://localhost:4000/geometry</id><content type="html" xml:base="http://localhost:4000/geometry.html">&lt;!--more--&gt;
&lt;h2&gt; Coordinate Systems &lt;/h2&gt;

&lt;h3&gt; Introducing Coordinate Systems &lt;/h3&gt;

&lt;p&gt;Coordinate systems play an essential role in the graphics pipeline. They are not complicated; However, learning a few things about them will make it easier to understand matrices.&lt;/p&gt;

&lt;p&gt;I mentioned that points and vectors (as used in computer vision/graphics) are represented with 3 real numbers. But what do these numbers mean? Each number represents &lt;span class=&quot;blue&quot;&gt; a signed distance &lt;/span&gt; from the origin of a line to the position of the point on that line.&lt;/p&gt;

&lt;p&gt;Consider drawing a line and putting a mark in the middle. We will call this mark origin. This mark becomes our point of reference: the position from which we will measure the distance to any other points. If a point lies to the right side of the origin, we take the signed distance to be greater than zero. On the other hand, if it is on the left side of the origin, the values will be negative.&lt;/p&gt;

&lt;p&gt;Now that we have a line and an origin, we add some additional marks at a regular interval (unit length) on each side of the origin, effectively turning our line into a ruler. With the ruler established, we can simply use it to measure the coordinate of a point from the origin (coordinate is another way of saying the signed distance from the origin to the point). In mathematics, the ruler defines what we call &lt;span class=&quot;red&quot;&gt; axis &lt;/span&gt;. We have just learned to define the coordinate of a point along an axis.&lt;/p&gt;

&lt;h3&gt; Dimensions and Cartesian Coordinate Systems &lt;/h3&gt;

&lt;p&gt;By placing two axes called $x$-axis and $y$-axis, we have defined a two dimensional space called a &lt;span class=&quot;neon&quot;&gt; plane &lt;/span&gt;. These two axes are said to define a coordinate system. &lt;span class=&quot;underline&quot;&gt; If these two rulers are perpendicular to each other, they define what we call a Cartesian coordinate system. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We now know how to make a two-dimensional Cartesian coordinate system and define the coordinates of a 2D point in that coordinate system. Note that &lt;mark class=&quot;gold&quot;&gt; the coordinates of points defined in a coordinate system are unique &amp;lt;/span&amp;gt;. The same point cannot be represented by two different sets of coordinates in one system. We are free to choose any coordinate system that we please.&lt;/mark&gt;&lt;/p&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/axis2d.png&quot; style=&quot;width: 300px !important;float: left !important; margin: 0px 0px 25px 25px !important;&quot; /&gt;&lt;img src=&quot;/assets/images/coor.png&quot; style=&quot;width: 300px !important; float: left !important; margin: 0px 0px 25px 25px !important;&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;In fact, we can choose to define infinitely many coordinate systems in a plane. Let’s assume we drew two Cartesian coordinate system like the figure above. On this paper, we place one point. &lt;span class=&quot;stroke&quot;&gt; The coordinates of that point will be different &lt;/span&gt; depending on which of the two coordinate system we consider.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;highlight-gradient&quot;&gt; If you know the coordinates of $P$ in coordinate system $A$, &lt;/span&gt; what do you need to do &lt;span class=&quot;underline-pink&quot;&gt; to find the coordinate of the same point in another coordinate system $B$? &lt;/span&gt; This represents an extremely important operation in computer vision, and we will soon learn why and how to find the map which translates the coordinates of a point from one coordinate system to another.&lt;/p&gt;

&lt;p&gt;Another common operation is to move the point in the coordinate system $A$ to another location in the same coordinate system. This is called a &lt;span class=&quot;gradient&quot;&gt; translation &lt;/span&gt; and is certainly one of the basic operations you can do on points.&lt;/p&gt;

&lt;p&gt;Note that all sorts of other linear operators can be applied to point coordinates. A multiplication of a real number to the coordinate of point produces &lt;span class=&quot;blue&quot;&gt; scale &lt;/span&gt;. A scale moves $P$ along the line that is going through the point and the origin. That is because when we are transforming a point we are actually transforming the vector going from origin to the point.&lt;/p&gt;

&lt;h3&gt; The Third Dimension &lt;/h3&gt;

&lt;p&gt;The 3D coordinate system is a simple extension of the 2D case. We will be adding a third axis orthogonal to both the $x$- and $y$-axis called the $z$-axis (representative of depth). In geometry, 3D coordinate system defines what is more formally known as &lt;span class=&quot;blink&quot;&gt; Euclidean Space &lt;/span&gt;. In linear algebra, the three axes from what we call the &lt;span class=&quot;flow&quot;&gt; basis &lt;/span&gt; of that coordinate system.&lt;/p&gt;

&lt;p&gt;A basis is a set of linearly independent vectors that, in a linear combination, can represent every vector (or point) in a given vector space (the coordinate system).&lt;/p&gt;

&lt;div class=&quot;sidenote&quot;&gt; Vectors from a set are said to be linearly independent if and only if none of the vectors in the set can be written as a linear combination of other vectors in that set. &lt;/div&gt;

&lt;p&gt;Change of basis, or change of coordinate system, is a common operation in mathematics and computer vision pipeline.&lt;/p&gt;

&lt;h3&gt; Left-Handed vs Right-Handed Coordinate Systems &lt;/h3&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/rhlh.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Unfortunately, due to various conventions concerning handedness, coordinate systems are not that simple. The problem can be illustrated in the following figure: when the up and forward vectors are oriented in the same way, an appropriate right vector can either point to the left or to the right.&lt;/p&gt;

&lt;p&gt;To differentiate the two conventions, we call the first coordinate system the left-handed coordinate system, and the other, the right-handed coordinate system.&lt;/p&gt;

&lt;p&gt;&lt;mark class=&quot;coral&quot;&gt; Remember that the middle finger always represent the right vector when checking the coordinate handedness. &lt;/mark&gt;&lt;/p&gt;

&lt;div class=&quot;blockquote&quot;&gt; The handedness of the coordinate system also plays a role in the orientation of normals computed from the edges of polygonal faces. If the orientation is right-handed, then polygons whose vertices were specified in counterclockwise order will be front-facing. This will be explained in the part on rendering polygon objects. &lt;/div&gt;

&lt;h3&gt; The Right, Up and Forward Vectors &lt;/h3&gt;

&lt;p&gt;The Cartesian coordinate system is only defined by three perpendicular vectors of unit length. As far as the mathematical notation is concerned, this coordinate system does not convey anything about what these three axes actually mean. The developer is the one that decides how these axes should be interpreted. It is thus very important to make a clear distinction between the handedness of the coordinate system and the conventions used to label the corresponding axes.&lt;/p&gt;

&lt;p&gt;The choice of coordinate system handedness also plays a critical role when it comes to rotation and the cross product of two vectors. It’s actually easy enough (but painful) to go from one coordinate system to another. All that is needed is to scale the point coordinates and the camera-to-world matrix by $(1,1,-1)$.&lt;/p&gt;

&lt;h3&gt; The World Coordinate System &lt;/h3&gt;

&lt;p&gt;In most 3D applications, each different type of coordinate system is defined with respect to a master coordinate system called the world coordinate system. It defines the origin and the main $x$,$y$,$z$-axes from which all other coordinate systems are defined.&lt;/p&gt;

&lt;h2&gt; Math Operations on Points and Vectors &lt;/h2&gt;

&lt;p&gt;I have explained the concept of (Cartesian) coordinate system. Now we can look at some of the most common operations which can be performed on points and vectors.&lt;/p&gt;

&lt;h3&gt; Vector Length &lt;/h3&gt;

&lt;p&gt;The vector itself indicates not only the direction of point $B$ from $A$ but also can be used to find out the distance between $A$ and $B$. This is given by the length of a vector which can easily be computed with the following formula:&lt;/p&gt;

&lt;p&gt;$$
\lVert V \rVert = \sqrt{x^2 + y^2 + z^2}
$$&lt;/p&gt;

&lt;p&gt;&lt;mark class=&quot;teal&quot;&gt; The vector's length is sometimes also called norm or magnitude. &lt;/mark&gt; Note that the axes of the three-dimensional Cartesian coordinate systems are unit vectors.&lt;/p&gt;

&lt;h3&gt; Normalizing a Vector &lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;red&quot;&gt; A normalized vector is a vector whose length is 1.&lt;/span&gt; Such a vector is also called a unit vector (it is a vector which has unit length). Normalizing a vector is very simple. We first compute the length of the vector and divide each one of the vector coordinates with this length.&lt;/p&gt;

&lt;p&gt;$$
\hat{V} = \frac{V}{\lVert V \rVert}
$$&lt;/p&gt;

&lt;p&gt;In mathematics, you will also find the term norm to define a function that assigns a length or size (or distance) to a vector. The function we have just described is called the Euclidean norm.&lt;/p&gt;

&lt;h3&gt; Dot Product &lt;/h3&gt;

&lt;p&gt;The dot product or scalar product requires two vectors $A$ and $B$ and can be seen as the projection of one vector onto the other. The result of the dot product is a real number. A dot product between two vectors is denoted with the dot sign: $A \cdot B$ (it can also sometimes be written as $&amp;lt;A,B&amp;gt;$).&lt;/p&gt;

&lt;p&gt;The dot product consists of multiplying each element of the $A$ vector with its counterpart from vector $B$ and taking the sum of each product. In the case of 3D vectors (they have three coefficients of elements which are $x$, $y$ and $z$), it consists of the following operation:&lt;/p&gt;

&lt;p&gt;$$
A \cdot B = A.x * B.x + A.y * B.y + A.z * B.z
$$&lt;/p&gt;

&lt;p&gt;Note that this is quite similar to the way we compute the length of a vector. If we take the square root of the dot product between two vectors ($\sqrt{A \cdot B}$) that are equal ($A=B$), then what we get is the length of the vector. We can write:&lt;/p&gt;

&lt;p&gt;$$
{\lVert V \rVert}^2 = V \cdot V
$$&lt;/p&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/dotproduct.png&quot; style=&quot;width: 300px !important;float: left !important; margin: 0px 0px 25px 25px !important;&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;The dot product between two vectors is an extremely important and common operation in any 3D application because the result of this operation relates to the cosine of the angle between the two vectors. In this example vector $A$ is projected in the direction of vector $B$.&lt;/p&gt;

&lt;ul&gt;&lt;li&gt; If $B$ is a unit vector then the product $A \cdot B$ gives $\lVert A \rVert \cos (\theta)$, the magnitude of the projection of $A$ in the direction of $B$, with a minus sign if the direction is opposite. This is called the scalar projection of $A$ onto $B$. &lt;/li&gt;

&lt;li&gt; When neither $A$ nor $B$ is a unit vector, we can write that $A \cdot B / \lVert B \rVert$, since $B$ as a unit vector is $B/\lVert B rVert$. &lt;/li&gt;

&lt;li&gt; In mathematics, $ {\cos}^{-1}$ is the inverse of the cosine function. When the two vectors are normalized ($A \cdot B / \lVert A \rVert \lVert B \rVert$), taking the arc cosine of the dot product gives you the angle $\theta$ between two vectors: $\theta = {\cos}^{-1}(A \cdot B / \lVert A \rVert \lVert B \rVert)$ or $\theta = {\cos}^{-1}(\hat{A} \cdot \hat{B})$. &lt;/li&gt;&lt;ul&gt;


&lt;div class=&quot;sidenote&quot;&gt; &lt;picture&gt;&lt;img src=&quot;/assets/images/dot.png&quot; style=&quot;width: 300px !important;float: left !important; margin: 0px 0px 25px 25px !important;&quot; /&gt;&lt;/picture&gt;
The dot product is a very important operation in 3D. It can be used for many things. As a test of orthogonality. When two vectors $A$,$B$ are perpendicular to each other, the result of the dot product between these two vectors is $0$. 

It is also used intensively to find out the angle between two vectors or compute the angle between a vector and the axis of a coordinate system. &lt;/div&gt; 

&lt;h3&gt; Cross Product &lt;/h3&gt;

The cross product is also an operation on two vectors. The difference between cross product and dot product is that the dot product returns a number, whereas the cross product returns a vector. 

The particularity of this operation is that &lt;mark class=&quot;blue&quot;&gt; the vector resulting from the cross product is perpendicular to the other two. &lt;/mark&gt; The cross product operation is written using the following syntax:

\$\$
C = A \times B
\$\$

&lt;picture&gt;&lt;img src=&quot;/assets/images/crossproduct.png&quot; style=&quot;width: 300px !important;&quot; /&gt;&lt;/picture&gt;

To compute the cross product we will need to implement the following formula:

\$\$
C_x = A_Y * B_Z - A_Z * B_Y
C_Y = A_Z * B_X - A_X * B_Z
C_Z = A_X * B_Y - A_Y * B_X
\$\$

The result of this cross product is another vector which is &lt;span class=&quot;reveal&quot;&gt;  orthogonal &lt;/span&gt; to the other two. The two vectors $A$ and $B$ define a plane and the resulting vector $C$ is perpendicular to that plane. 

$A$ and $B$ don't have to be perpendicular to each other, but when they are, the resulting $A$, $B$ and $C$ vectors form a Cartesian coordinate system (assuming the vectors have unit length).

If you need a mnemonic way of remembering this formula, we like to use the technique that consists of asking ourselves the question &quot;why z?&quot;. $y$ and $z$ being the coordinates of vector $A$ and $B$ used to compute the $x$ coordinates of the resulting vector $C$. 

It is important to note that the order of the vectors involved in the cross product has an effect on the resulting vector $C$. You can see that $A \times B$ doesn't give you the same result as $B \times A$. 

&lt;h2&gt; How Does Matrix Work &lt;/h2&gt;

&lt;h3&gt; Point-Matrix Multiplication &lt;/h3&gt;

Two matrices need to have compatible sizes in order to be multiplied with each other. For instance, the matrices of size $m \times p$ and $p \times n$ can be multiplied with each other. In computer vision, we would primarily deal with $4 \times 4$ matrices. 

Point can be written in a matrix form $P = [xyz]$. There is two things we need to remember. The first one is that a point multiplied by a matrix transforms the point to a new position. The result of a point multiplied by a matrix has to be a point.

The second thing we need to remember is that a $m \times p$ matrix multiplied by $p \times n$ matrix, gives a $m \times n$ matrix. Multiplying a $1 \times 3$ matrix by a $3 \times 3$ matrix gives as expected, a $1 \times 3$ matrix which is another point.

&lt;h3&gt; The Identity Matrix &lt;/h3&gt;

The identity matrix or unit matrix is a square matrix whose coefficients are all 0 except the coefficients along the diagonal which are set to $1$. 

\$\$
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\$\$

The result of $P$ multiplied by the identity matrix is $P$. 

&lt;h3&gt; The Scaling Matrix &lt;/h3&gt;

When these coefficients of the identity matrix are different than 1 (whether smaller or bigger than 1), then they act as a multiplier on the point's coordinates. In other words, the points coordinates are scaled up or down by some amount.

\$\$
\begin{bmatrix}
S_X &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; S_Y &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; S_Z
\end{bmatrix}
\$\$

Note that &lt;mark class=&quot;coral&quot;&gt; if either one of the scaling coefficiuent in the matrix are negative, then the point's coordinate for the corresponding axis will be flipped &lt;/mark&gt; (it will be mirrored to the other side of the matrix).

&lt;h3&gt; The Rotation Matrix &lt;/h3&gt;

We are now talking about building a matrix that will rotate a point or a vector around one axis of the cartesian coordinate system. And for doing so, we will need to use trigonometric functions. 

&lt;picture&gt;&lt;img src=&quot;/assets/images/rotation-mat.png&quot; style=&quot;width: 300px !important;float: left !important; margin: 0px 0px 25px 25px !important;&quot; /&gt;&lt;img src=&quot;/assets/images/rotation45.png&quot; style=&quot;width: 300px !important; float: left !important; margin: 0px 0px 25px 25px !important;&quot; /&gt;&lt;/picture&gt; 

Let's take a point $P$ defined in a three-dimensional coordinate system with coordinate $(1,0,0)$. Ignore the $z$-axis for a while and assume that the point lies in the $xy$ plane. What we want is to transform the point from $P$ to $P_T$ by the mean of a rotation (we could do this with a translation but using a rotation will be easier). $P_T$ coordinates are $(0,1,0)$. 

As you can see, this can be done by rotating the point around the $z$-axis by $90$ degrees counterclockwise. Let's assume now that we have a matrix $R$. When $P$ is multiplied by $R$ it transforms $P$ into $P_T$. 

\$\$
\begin{aligned}
P_T.x = P.x * R_{00} + P.y * R_{10} + P.z * R_{20} \\
P_T.y = P.x * R_{01} + P.y * R_{11} + P.z * R_{21} \\
P_T.z = P.x &amp;amp; R_{02} + P.y * R_{12} + P.z * R_{22}
\end{aligned}
\$\$

As we said, we don't care so much about $P_T.z$ for now which represents the $z$-coordinate of $P_T$. Let's concentrate instead on $P_T.x$ and $P_T.y$ which represent respectively the $x$ and $y$ coordinates of $P_T$. From $P$ to $P_T$, the $x$-coordinate goes from $1$ to $0$. It means that $R_{00}$ has to be equal to $0$. Considering that $P.y$ and $P.z$ are 0 anyway we don't care much about the values that $R_{10}$ and $R_{20}$ may be for now. 

What do we know about $P$? We know that $P.x$ is 1 and that all the other coordinates of $P$ are $0$. Which necessarily means that $R_{01}$ has to be 1. 

Let's recap. We know that $R_{00}$ is 0 and $R_{01}$ is 1. Let's write it down and see what $R$ looks like: (compare this matrix with the identity matrix)

\$\$
R_z = 
\begin{bmatrix}
0 &amp;amp; 1 &amp;amp; 0 \\
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1 \\
\end{bmatrix}
\$\$

If you use this matrix to transform $P=(1,0,0)$ you will get $P_T = (0,1,0)$.

&lt;h2&gt; How Does Matrix Work? &lt;/h2&gt;

&lt;h3&gt; Point-Matrix Multiplication &lt;/h3&gt;

We will start to put all the things we have learned on points, vectors, matrices and coordinate systems together. We mentioned that two matrices needed to have compatible sizes in order to be multiplied with each other. The matrices of size $m \times p$ and $p \times n$ can be multiplied with each other. In computer vision, we would primarily deal with $4 \times 4$ matrices. 

A point or a vector is a sequence of three numbers and for this reason they too can be written as a $1 \times 3$ matrix, a matrix that has one row and three columns. Point written in a matrix form $P=[xyz]$. 

The trick here is that, if we can write points and vectors as $1 \times 3$ matrices, we can multiply them by other matrices. Remember that the matrix $m \times p$ can be multiplied by the matrix $p \times n$ to give the matrix $m \times n$. So this implies that we can multiply something of the form $3 \times n$ where n can be any number greater than 1. 

There is two things we need to remember now to make sense of what we are going to explain. The first one is that a point multiplied by a matrix transforms the point to a new position. The result of a point multiplied by a matrix has to be a point. If it wasn't the case, we wouldn't be using matrices as a convenient way of transforming points. The second thing we need to remember is that a $m \times p$ matrix multiplied by a $p \times n$ matrix, gives a $m \times n$ matrix. 

If we look at our point as a $1 \times 3$ matrix, we need the result of the multiplication to be another point, that is a $1 \times 3$ matrix. Multiplying a $1 \times 3$ matrix by a $3 \times 3$ matrix gives as expected, a $1 \times 3$ matrix which is another point. 

&lt;h3&gt; The Identity Matrix &lt;/h3&gt;

The identity matrix or unit matrix is a square matrix whose coefficients are all $0$ except the coefficients along the diagonal which are set to $1$.

\$\$
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\$\$

The result of $P$ multiplied by the identity matrix is $P$.

&lt;h3&gt; The Scaling Matrix &lt;/h3&gt;

If you look at the code of the point-matrix multiplication, you can see that the coordinates of the point $P$ are respectively multiplied by the coefficients $R_{00}$ for $x$, $R_{11}$ for $y$, and $R_{22}$ for $z$. When these coefficients are set to $1$ (and all the other coefficients are set to $0$), we get the identity matrix. 

However, when these coefficients (along the diagonal) are different than $1$ (whether smaller or bigger than $1$), then they act as a multiplier on the point's coordinates (in other words, the points coordinates are scaled up or down by some amount). If you remember what we have said about coordinate systems, multiplying the coordinates ofa point by some real numbers result in scaling the point's coordinates. The scaling matrix can therefore be written as:

\$\$
\begin{bmatrix}
S_X &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; S_Y &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; S_Z
\end{bmatrix}
\$\$

Note that if either one of the scaling coefficients in the matrix are negative, then the point's coordinate for the corresponding axis will be flipped (it will be mirrored to the other side of the axis). 

&lt;h3&gt; The Rotation Matrix &lt;/h3&gt;

What we will be talking about now is about building a matrix that will rotate a point or a vector around one axis of the Cartesian coordinate system. And for doing so, we will need to use trigonometric functions. 

Let's take a point $P$ defined in a three-dimensional coordinate system with coordinate $(1,0,0)$. Let's ignore the $z$-axis for a while and assume that the point lies in the $xy$ plane. What we want is to transform the point from $P$ to $P_T$ by the mean of rotation (we could do this with a translation but using a rotation will make our demonstration easier). $P_T$ coordinates are $(0,1,0)$. This can be done by rotating the point around the $z$-axis by $90$ degrees counterclockwise. Let's assume now that we have a matrix $R$. When $P$ is multiplied by $R$ it transforms $P$ to $P_T$. Considering what we know about matrix multiplication let's see how we can re-write a point-matrix multiplication and isolate the computation of each of the transformed coordinates. 

\$\$
\begin{aligned}
P_T.x = P.x * R_{00} + P.y * R_{10} + P.z * R_{20} \\
P_T.y = P.x * R_{01} + P.y * R_{11} + P.z * R_{21} \\
P_T.z = P.x * R_{02} + P.y * R_{12} + P.z * R_{22}
\end{aligned}  
\$\$

As we said, we don't care so much about $P_T.z$ for now which represents the $z$-coordinate of $P_T$. Let's concentratte instead on $P_T.x$ and $P_T.y$ which represent respectively the $x$ and $y$ coordinates of $P_T$. From $P$ to $P_T$, the $x$-coordinate goes from $1$ to $0$. If we look at the first line of the code we wrote to compute $P_T$, it means that $R_{00}$ has to be equal to 0. Considering that $P.y$ and $P.z$ are $0$ anyway we don't care so much about the values that $R_{10}$ and $R_{20}$ may have for now. From P to $P_T$, the $y$-coordinate goes from $0$ to $1$. Let's have a look at the second line of code. What do we know about $P$? We know that $P.x$ is $1$ and that all the other coordinates of $P$ are $0$, which necessarily means that $R_{01}$ has to be $1$. Let's recap. We know that $R_{00}$ is $0$ and $R_{01}$ is $1$. Let's write it down and see what $R$ looks like (compare this matrix with the identity matrix):

\$\$
R_z = 
\begin{bmatrix}
0 &amp;amp; 1 &amp;amp; 0 \\
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1 
\end{bmatrix}
\$\$

Don't worry for now if you don't understand why the coefficients have the value they have. That will be explained soon. All you want to see, is that if you use this matrix to transform $P=(1,0,0)$ you will get $P_T=(0,1,0)$.

\$\$
\begin{aligned}
P_T.x = P.x * 0 + P.y * 1 + P.z * 0 = 0 \\
P_T.y = P.x * 1 + P.y * 0 + P.z * 0 = 1 \\
P_T.z = P.x * 0 + P.y * 0 + P.z * 1 = 0
\end{aligned}
\$\$

This is where our knowledge of trigonometric functions will become handy. If we look at a point on the unit circle we know that its $x$ and $y$ coordinates can be computed using the sine and cosine of the angle $\theta$. 

\$\$
\begin{aligned}
x = cos(\theta) = 0 \\
y = sin(\theta) = 1 \\
\theta = \frac{\pi}{2}
\end{aligned}
\$\$

When $\theta=0$, $x=1$ and $y=0$. When $\theta = 90$ degrees (or $\frac{\pi}{2}$), $x=0$ and $y=1$. That is interesting because you will notice that $x=0$ and $y=1$ are values of $R_{00}/R_{11}$ and $R_{01}/R_{10}$ respectively. So we could re-write the matrix $R$ as:

\$\$
R_z(\theta) = 
\begin{bmatrix}
cos(\theta) &amp;amp; sin(\theta) &amp;amp; 0 \\
sin(\theta) &amp;amp; cos(\theta) &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
= 
\begin{bmatrix}
0 &amp;amp; 1 &amp;amp; 0 \\
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1 
\end{bmatrix}
\$\$

&lt;h3&gt; Combining Rotation Matrices &lt;/h3&gt;

We have learned that multiplying matrices together combines their transformations. Now that we know how to rotate points around individual axis, it is possible to multiply $R_x$, $R_y$, $R_z$ together to create more complex rotations. If for instance you want to rotate a point around the $x$-axis, and then the $y$-axis, you can create two matrices using the matrices $R_x$ and $R_y$ and combine them using matrix multiplication $R_x * R_y$ to create a $R_{xy}$ matrix encoding the two individual rotations. 

Note that the order of the rotation is important and makes a difference. If you rotate a point around the $x$-axis first and then the $y$-axis second, you will end up with a result which is different from a rotation around the $y$-axis and then around the $x$-axis. 

&lt;h3&gt; Relation Between Matrices and Cartesian Coordinate System &lt;/h3&gt;

If you imagine that you have a point $P_x$ with coordinates $(1,0,0)$ and want to rotate this point around the $z$-axis by 10 degrees clockwise, we know that these new coordinates can be found using simple trigonometry. 

&lt;h3&gt; Orthogonal Matrices &lt;/h3&gt;
 
In fact, the type of matrices we ahve described (the rotation matrices) are called in linear algebra, orthogonal matrices. An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors.

We have mentioned previously that each row from the matrix represents an axis of a Cartesian coordinate system. If the matrix is a rotation matrix or the result of several rotation matrices multiplied with each other, then each row necessarily represents an axis of unit length (because the elements of the rows are constructed from the sine and cosine trigonometric functions which are used to compute the coordinates of points lying on the unit circle). You can see them as a Cartesian coordinate system which is originally aligned with the world coordinate system (the identity matrix's rows represent the axes of the world coordinate system) and rotated around one particular axis or a random axis. Orthogonal matrices have a few interesting properties but maybe the most useful one in computer vision, is that the transpose of an orthogonal matrix is equal to its inverse. Assuming $Q$ is an orthogonal matrix, we can write:

\$\$
Q^T = Q^{-1}
\$\$ 

which entials that $QQ^T = I$, where $I$ is the identity matrix.

&lt;h3&gt; Affine Transformations &lt;/h3&gt;

You will sometimes find the terms affine transformations used in place of matrix transformation. This technical term is actually more accurate to designate the transformations that you get from using the type of matrices we have described so far. 

In short, an affine transformation, is a transformation that preserves straight lines. The translation, rotation, shearing matrix are all affine transformations as are their combinations. The other type of transformation we will be studying in computer vision are called projective transformations (perspective projection is a projective transformation). As you may have guessed, such transformations do not necessarily preserve parallelism between lines. 

Not only you have learned how to create rotation matrices but we have also given you a way of visualizing what a matrix is: each row of the matrix represents one axis of a Cartesian coordinate system. The orientation (rotation), size (scale), and position (translation) of this coordinate system represents the transformation that will be applied to the points when they are multiplied by this matrix. 

The key idea is that points are originally defined in a certain coordinate system. If a point is attached to a local coordinate system $B$ (the matrix) and that we move, rotate, and translate that local coordinate system (i.e. the matrix), the point coordinates will not change in regard to the local coordinate system $B$. The point is somehow constrained to the transformation applied to the local coordinate system $B$. However, the coordinates of that point will change in the coordinate system $A$. 

Multiplying the point whose coordinates are expressed in regards to $A$ by the matrix $B$ will provide us with the point's new coordinates in the coordinate system $A$. What you need to remember is how to find the formula for the basic rotation matrices. That the order by which you multiply this basic matrices is important. And finally (and that's almost the most important) that a matrix can be seen as a local Cartesian system where each row of the matrix represents one axis of that local coordinate system. Such matrix is called an orientation matrix. 

&lt;h2&gt; Transforming Points and Vectors &lt;/h2&gt;

Even though translation seems to be the easiest linear operator that can be applied to point, we haven't mentioned it often in the previous chapter. Because to get the translation working with the theory of matrix multiplication, we need to make a change to the point structure that might confuse you slightly. 

As we mentioned in the last two chapters, a matrix-matrix multiplication can only work if the two matrices involved have a compatible size. That is if they have the size $m \times p$ and $p \times n$. Let's keep that in mind.

Let's start from a $3 \times 3$ identity matrix. We know that a point multiplied by this matrix have its coordinates unchanged. Let's see what changes we need to bring to that matrix to handle translation. Translation on a point is nothing more than adding a number to each of its coordinates (these numbers can be positive or negative). For instance if we want to move the point $(1,1,1)$ to the coordinate $(2,3,4)$ we need to add values $1$,$2$ and $3$ respectively to each of the points' $x$, $y$ and $z$ coordinates. It is very simple. Note that from now on, we will keep looking at points and vectors as matrix of size $1 \times 3$. 

Let's get back to the code that transforms a point using a matrix. What do we need to get the rotation matrix extended so that it handles translation as well? We would need to be able to have a fourth term to the right that would encode translation.

Now remember that we want to come up with a matrix that encodes scale, rotation, and translation. So, somehow we need to get $T_x$, $T_y$, $T_z$ to fit within the code of point-matrix multiplication (and store these three values somewhere in the matrix). 

Look at the first line for now. Note that to compute $x', we only use the coefficients ofthe matrix first column. If the column had four coefficients instead of three, then $T_x$ would be $M_{30}$. The same reasoning can be done with $T_y$ and $T_z$. 

However, this is assuming that our matrix now has the size $4 \times 3$ not $3 \times 3$ anymore. This is alright. We said that matrices could have any size. However, we know that matrix multiplication can be valid if their sizes are compatible. 

We try to multiply a point $1 \times 3$ matrix with a $4 \times 3$ matrix and theory tells us that this is not possible. What shall we do? The solution is simple. We will add one additional column to the point to turn it into a $1 \times 4$ matrix and set the fourth coefficient of this point to $1$. Our point now looks like this $(x,y,z,1)$. In computer vision, it is called a homogeneous point (or a point with homogeneous coordinates). With such a point we can easily encode translation in our matrix. 

This is the theory. In order to encode translation, scale and rotation in a matrix we need to deal with points that have homogeneous coordinates. But because the fourth value is always $1$ we never really explicitly define it in the code. We only define $x$,$y$,$z$ and assume that there is a fourth value. 

Our matrix is now a $4 \times 3$ matrix. So you may wonder, how do we go from $4 \times 3$ matrix to our final $4 \times 4$ matrix which is the form that is the most commonly used in computer vision? The fourth columns plays a role in perspective projection and for some other type of transformations that are not very common (such as the shear transformation), but generally it is simply set to $(0,0,0,1)$. 

What happens though when the coefficients of this column have different values than the default (we said its uncommon but it happens sometimes)? Before we can answer this question, we first need to learn a few things more about homogeneous points. 

&lt;h3&gt; The Trick About Homogenous Points &lt;/h3&gt;

Presenting a point as an homogeneous point is necessary to allow point-multiplication by $4 \times 4$ matrices however, in the code, this is only done implicitly, since as we have explained, $w$ is always $1$. 

But as we mentioned briefly, this fourth column is not always set to $(0,0,0,1)$ particularly when you deal with projection matrices (matrices that can project points to the screen). In these special cases, the result for $w'$ can be different than $1$ (which is intentional) but for this point to be usable as a Cartesian point, we need to normalize $w'$ back to $1$ by dividing it by itself which requires to divide the other coordinates ($x'$,$y'$ and $z'$) by $w'$ as well. 

As you can see we don't need to declare a $w$ coordinate in the Point's type. We can just compute a value for $w'$ on the fly as we assume implicitly that the point we are transforming is a Cartesian point which you can see as a homogeneous point whose $w$ coordinate is not declared explicitly (because it's always equal to 1). However, if the matrix we are multiplying the point with is a projection matrix for instance, the result of $w'$ might be different than $1$. In this particular caase, we need to normalize all the coordinates of $P'$ to set it back to $1$. Once this is done, we get a point which we can use in our Cartesian coordinate system again.

All you need to remember is that generally, you will never have to care about homogeneous coordinates, expect when points are multiplied by a perspective projection matrix. However, you will actually probably not come across this issue if you work on a ray tracer, as this special type of matrix is not used in ray tracing.

&lt;h3&gt; Transforming Vectors &lt;/h3&gt;

Vectors somehow are simpler to transform than points. Vectors, as we said in the preamble of this lesson, represent direction whereas points represent position in space. As such vectors do not need to be translated because their position is in fact meaningless. With vectors we are only interested in the direction in which they point and eventually their length which is sometimes an information we need to solve geometric or shading problems. 

Vectors can be transformed like we transformed point but we can remove the part of the code that is responsible for the translation bit. 

Now that the matrix has size $4 \times 4$ we need to extend the size of the point by adding an extra coordinate. We can do this by implicitly treating points as homogeneous points but to continue using them in a Cartesian coordinate system (as Cartesian points) we need to be sure that $w$, this fourth coordinate is always set to $1$. Most of the time the matrices we use to transform a point will have their fourth column set to $(0,0,0,1)$ and with these matrices, the value of $w'$ should always be $1$. However, in special cases (projection matrix, shear transform) the value of $w'$ might be different than $1$ in which case you will need to normalize it (we divide $w'$ by itself) which requires to also divide the other transformed coordinates $x'$, $y'$, and $z'$ by $w'$.

Matrices are not the only method to encode or store transformations. You can also for instance represent a rotation using a method proposed initially by Euler. The idea is to define a rotation in this casae as a vector and an angle representing a rotation around the vector. 

While uncommon, both techniques are used to solve problems in computer vision from time to time. Rotations in computer vision are also commonly done using quaternions. Matrices themselves have certain limitations especially when it comes to rotation by an angle greater than $360$ degrees. This can lead to a problem known as the gimbal lock. Matrices are also hard to interpolate which is often needed in rendering to compute the motion blur of objects. For this particular reason, quaternions are generally preferred though they are considered to be generally harder to understand. 

&lt;h2&gt; Row Major and Column Major Vector &lt;/h2&gt;

Earlier in this lession, we have explained that vectors (or points) can be written down as $1 \times 3$ matrices (one row, three columns). Note however that we could have also written them down as $3 \times 1$ matrices (three rows, one column). Technically, these two ways of expressing points and vectors as matrices are perfectly valid and choosing one mode or the other is just a matter of convention.

In the first example, we have expressed our vector or point in what we call the row-major order: the vector (or point) is written as a row of three numbers. In the second example, we say that points or vectors are written in column-major order: we write the three coordinates of the vector or point vertically, as a column.

Remember that we express points and vectors as matrices to multiply them by $3 \times 3$ transformation matrices (for the sake of simplicity we will work with $3 \times 3$ rather than $4 \times 4$ matrices).

So what do we do? We move the point or vector to the right side of the multiplication. Note that the result of this operation is a transformed poitn written in the form of a $3 \times 1$ matrix. So we get a point to start with and we finish with a transformed point which is what we want. Problem solved. To summarize, when by convention we decide to express vectors or points in row-major order, we need to put the point on the left side of the multiplication and the $3 \times 3$ on the right inside of the multiplication sign. This is called in mathematics, a left or pre-multiplication. If you decide to write the vectors in column-major order, matrix needs to be on the left side of the multiplication and the vector or point on the right side. This is called a right or post-multiplication. 

Multiplying a point or a vector by a matrix should give us the same result whether we use row- or column-major order. If you use a 3D application to rotate a point by a certain angle around the $z$-axis, you expect the point to be in certain position after the rotation no matter what internal convention the developer used to represent points and vectors. 

Multiplying a row-major and column-major point (or vector) by the same matrix clearly wouldn't give us the same result. We would actually need to transpose the $3 \times 3$ matrix used in the column-major multiplication to be sure that $x'$,$y'$, and $z'$ are the same.

In conclusion, going from row-major order to column-major order not only involves to swap the point or vector and the matrix in the multiplication but also to transpose the $3 \times 3$ matrix, to guarantee that both conventions give the same result (and vice versa).

From these observations, we can see that any series of transformations applied to a point or a vector when a row-major convention is used can be written in sequential order. Imagine for instance that you wnat to translate point $P$ with matrix $T$ then rotate it around the $z$-axis with $R_z$ then around the $y$-axis with $R_y$. You can write:

\$\$
P' = P * T * R_z * R_y
\$\$

If you were to use a column-major notation you would need to call the transform in reverse order (which one might find counter-intuitive):

\$\$
P' = R_y * R_z * T * P
\$\$

Order of transformation when we use column-major matrices is more similar in mathematics to the way we write function evaluation and composition.

&lt;h3&gt; Implication in Coding &lt;/h3&gt;

There is another potentially very important aspect to take into consideration if you need to choose between row-major and column-major, but this has nothing to do really with the conventions themselves and how practical one is over the other. It has more to do with the computer and the way it works. 

Typically the implementation of a matrix are laid out contiguouosly in memory. In the world of computing, accessing elements from an array in a non-sequential order, is not necessarily a good thing. It actually potentially degrades the cache performance of the CPU. 

Applied to our matrix problem, accessing the coefficients of the matrix in non-sequential order can therefore be a problem. Assuming the CPU loads the requested float in the cache plus 3 floats next to it, our current implementation might lead to many cache misses, since the coefficients used to compute $x'$, $y'$ and $z'$ are 5 floats apart in the array. 

On the other hand, if you use a column-major order notation, computing $x'$ for instance requires to access the 1st, 2nd and 3rd element of the matrix. In conclusion, we can say that form a programming point of view, implementing our point- or vector-matrix multiplication using a column-major order convention might be better, performance wise, than the version using the row-major order convention. 

&lt;h2&gt; Matrix Operations &lt;/h2&gt;
&lt;h3&gt; Transpose &lt;/h3&gt;

The transpose of a matrix $M$ is another matrix which we write using the following convention: $M^T$. We can describe the process of transposing a matrix in different ways. It can be seen as: reflecting $M$ over its main diagonal to obtain $M^T$, writing the rows of $M$ as the columns of $M^T$ or reciprocally, writing the columns of $M$ as the rows of $M^T$. 

The idea is to swap the rows and columns and since the operation can't be done in place we need to assign the result to a new matrix which is returned by the function. Transposing matrices can be useful when you want to convert matrices from a 3D application using row-major matrices to another using a column-major convention (and vice versa).

&lt;h3&gt; Inverse &lt;/h3&gt;

If the multiplying point $A$ by the matrix $M$ gives point $B$, multiplying a point $B$ the inverse of the matrix $M$ gives point $A$. In mathematics, a matrix inversion is usually written using the following notation:

\$\$
M^{-1}
\$\$

From this observation, we can write that:

\$\$
MM^{-1} = I
\$\$

where $I$ is the identity matrix. Multiplying a matrix by its inverse gives the identity matrix. We have mentioned the case of the orthogonal matrix which inverse can easily be obtained from computing its transpose. An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors. 

Matrix inversion is an important process in 3D. We know that we can use point- or vector-matrix multiplication to convert points and vectors but it is sometimes useful to be able to move the transformed points or vectors back into the coordinate system in which they were originally defined into. It is often necessary for instance, to transform the ray direction and origin in object space to test for a primitive-ray intersection. If there is an intersection resulting hit point is in object space and needs to be converted back into world space to be usuable.

&lt;h2&gt; Spherical Coordinates and Trigonometric Functions &lt;/h2&gt;

Besides points, vectors, normals and matrices the last useful technique from linear algebra we will need to render images is to express vectors in terms of spherical coordinates. We could certainly render images without using them, but you will see that using them simplifies many problems especially when it comes to shading. This chapter is also a good opportunity to review trigonometric functions.

&lt;h3&gt; Trigonometric Functions &lt;/h3&gt;

Rendering a computer generated images is almost entirely a geometric problem so not understanding or using trigonometry for creating such images (and the phythagorean theorem) would be hard. Let's start to review the sine and cosine function as well as the way angles can be computed from 2D coordinates. 

Usually these functions are defined in regards to the unit circle (a circle of radius 1). When we draw a point $P$ on this unit circle, the $x$-coordinate of the point can be computed using the cosine of the angle subtended by the $x$-axis and a line that goes from the origin of the coordinate system to $P$. This angle is usually called $\theta$. Similarly, the sine of this angle gives the $y$-coordinate of the point $P$. Note that the angle $\theta$ is defined in radians. It will be easier to define the angles in degrees, but we will need to convert them internally to radians to use them in the C++ trigonometric functions $\theta_{radians} = \frac{\pi}{180} \theta_{degrees}$. Remember that a complete turn around the unit circle represents 360 degrees of $2\pi$. 

It is also important to remember that the cosine, sine and tangent functions are defined from a simple relationship between the edges of a right triangle (right-angle triangle). The tangent formula is interesting because to come back to our example using the unit circle, you can see that it can be computed using the ratio of $y$ over $x$. 

Another very useful function in computer vision is the arctangent which is the tangent inverse function. In other words, if you feed the arctangent function with the result of the tangent function you get $\theta$. In programming you can use the `atan` function but this function doesn't take the sign of the parameters $x$ and $y$ into account. To fix the issue, you need to use the C/C++ function `atan2` instead which takes into account the sign of the point's coordinates in the computation of the angle. Similarly to `atan2`, you can compute the inverse function of sine and cosine using `asin` and `acos`. Let's summarize all the functions we have talked about so far. 

\$\$
\begin{aligned}
\theta = \acos(P_x)
\theta = \asin(P_y)
\theta = \atan(P_y, P_x)
\end{aligned}
\$\$

The interesting thing to note is that the angle returned by the `atan2` function is positive for counter-clockwise angles (upper half-plane, $y&amp;gt;0$) and negative for clockwise angles (lower half-plane, $y&amp;lt;0$). It produces results in the range $[-\pi, \pi]$. 

&lt;h3&gt; Representing Vectors with Spherical Coordinates &lt;/h3&gt;

So far we have learned hyow to represent vectors (as in directions) using Cartesian coordinates (with three values, one for each axis). It is also possible though to represent the same vectors with only two values. One to represent the angle between the vector and the vertical axis and one to represent the angle between the vector projected onto the horizontal plane and the right vector from the Cartesian coordinate system. The vertical angle is always called $\theta$ and the horizontal angle is always called $\phi$. No matter what you do and what you see in textbooks, we advise you to follow these rules which is about the only convention unanimously followed by the computer vision community. These angles should be expressed in radians. 

Note that $\theta$ lies within the range $[0:\pi]$ while $\phi$ varies in the range $[0: 2\pi]$. As such $\theta$ and $\phi$ can also be seen as coordinates and are called spherical coordinates. $V_r$, $V_u$ and $V_f$ correspond to the Cartesian coordinates of the vector in the Cartesian coordinate systems defined by the right, up and forward axes. Note that we haven't used the names $x$, $y$, $z$ for the axis for a reason we will explain soon. Also, we have always represented a normalized vector (unit length) but any vector of arbitrary length can be represented using spherical coordinates. The formal definition of spherical coordinates includes an additional term (usually denoted $r$ for radial distance) to represent the length of the vector combined with $\theta$ and $\phi$ which can also be called the polar and aximuth angles. Spherical coordinates are just another way of encoding vectors. They make this representation compact as only two numbers are used instead of three (if you don't care about the length of the vector) with the Cartesian coordinates (it can save memory in your program) and they will become most useful when we will talk about shading. The question now is how we convert a vector represented in Cartesian coordinates to spherical coordinates. 

&lt;h3&gt; Conventions Again: $Z$ is Up! &lt;/h3&gt;

The convention when it comes to represent vectors in mathematics and physics is to name the up vector as the $z$-axis and the right and forward vector respectively the $x$- and $y$-axis. And to make things easier, the convention is also to use a left-hand coordinate system. Having $z$-axis representing the up vector, is something we have already briefly mentioned. As you can see, this convention is different from the one we normally use (where the up axis is the $y$-axis) but unfortunately this notation is the norm and we will have to stick to it. 

&lt;h3&gt; Creating an Orientation Matrix or Local Coordinate System &lt;/h3&gt;

In this chapter, we will use what we have learned so far about coordinate systems and what they represent to build a local coordinate system (or frame) from a vector which can also be a normal. This technique is often used in the rendering pipeline as a way of converting point and vectors which are defined in one coordinate system to another. 

The idea is to let the normal at that point to become one of the axis of that local coordinate system (often aligned with the up vector, and let the tangent and bi-tangent of that point to become )

Often a coordinate system is helpful because it can be easier to manipulate the coordinates of a vector rather than manipulating its magnitude and direction directly. When we express a vector in a coordinate system, we identify a vector with a list of numbers, called coordinates or components, that specify the geometry of the vector in terms of the coordinate system. 

We assume that you are familiar with the standard $(x,y)$ Cartesian coordinate system in the plane. Each point $p$ in the plane is identified with its $x$ and $y$ components: $p=(p_1,p_2)$.

To determine the coordinates of a vector $a$ in the plane, the first step is to translate the vector so that its tail is at the origin of the coordinate system. Then, the head of the vector will be at some point $(a_1, a_2)$ in the plane. We call $(a_1, a_2)$ the coordinates or the components of the vector $a$. We often write $a \in \mahthbb{R}^2$ to denote that it can be described by two real coordinates. 

&lt;h3&gt; Vectors in Three-Dimensional Space &lt;/h3&gt;

If you switched the locations of the positive $x$-axis and positive $y$-axis, then you would end up having a left-handed coordinate system. With these axes any point $p$ in space can be assigned three coordinates $p=(p_1,p_2,p_3)$. 

Just as in two-dimensions, we assign of a vector $a$ by translating its tail to the origin and finding the coordinates of the point at its head. In this way, we can write the vector as $a=(a_1,a_2,a_3)$. We often write $a \in \mathbb{R}^3$ to denote that it can be described by three real coordinates. 

A representation of a vector $a=(a_1,a_2,a_3)$ in the three-dimensional Cartesian coordinate system. The vector $a$ is drawn as a green arrow with tail fixed at the origin. You can drag the head of the green arrow with tail fixed at the origin. 

Just as in two dimensions, we can also denote three-dimensional vectors in terms of the standard unit vectors, $i$, $j$, and $k$. These vectors are the unit vectors in the positive $x$, $y$, and $z$ direction, respectively. We can express any three-dimensional vector as a sum of scalar multiples of these unit vectors $a=(a_1,a_2,a_3) = a_1 i + a_2 j + a_3 k$. 

We can easily visualize two or three dimensions by drawing pictures of a plane or space. Going to higher dimension is easy with lists of numbers, though of course high-dimensional vectors are not easy to visualize. 

For many of us, this metaphor may hamper our efforts to delve into higher dimensions, as we are restrained by our experience of just three spatial dimensions in the world around us. It may be easier to think of a high dimensional vector as simply describing quantities of distinct objects. 

At first, it may seem that going beyond three dimensions is an exercise in pointless mathematical abstraction. to describe even the simplest objects, we will typically need more than three dimensions. It is challenging to develop mathematical models that cna realistically describe a physical system and yet keep the number of dimensions from becoming incredibly large. 

However, even if the center of a rigid object is specified, the object could also rotate. In fact, it can rotate in three different directions, such as the roll, pitch and yaw of an airplane. Consequently, we need six dimensions to specify the position of a rigid object: three to specify the location of the center of the object, and three to specify the direction in which the object is pointing. 

&lt;h4&gt; The Dot Product &lt;/h4&gt;
 
The dot product between two vectors is based on the projection of one vector onto another. Let's imagine we have two vectors $a$ and $b$, and we want to calculate how much of $a$ is pointing in the same direction as the vector $b$. We want a quantity that would be positive if the two vectors are pointing in similar directions, zero if they are perpendicular, and negative if the two vectors are pointing in nearly opposite directions. We will define the dot product between the vectors to capture these quantities. 

The answer to this question should not depend on the magnitude of $b$, only its direction. To sidestep any confusion caused by the magnitude of $b$, let's scale the vector so that it has length one. In other words, let's replace $b$ with the unit vector that points in the same direction as $b$. We'll call this vector $u$, which is defined by:

\$\$
y = \frac{b}{lVert b \rVert}
\$\$

The dot product of $a$ with unit vector $u$, denoted $a \cdot u$ is defined to be the projection of $a$ in the direction of $u$, or the amount that $a$ is pointing in the same direction as unit vector $u$. Let's assume for a moment that $a$ and $u$ are pointing in similar directions. Then, you can imagine $a \cdot y$ as the length of the shadow of $a$ onto $u$ if their tails were together and the sun was shining from a direction perpendicular to $u$. 

\$\$
a \cdot y = \lVert a \rVert \cos \theta
\$\$

If $a$ and $u$ were perpendicular, there would be no shadow. That corresponds to the case when $\cos\theta = \cos \frac{\pi}{2} = 0$ and $a \cdot u = 0$. If the angle $\theta$ between $a$ and $u$ were larger than $\frac{\pi}{2}$, then the shadow wouldn't hit $u$. Since in this case $\cos\theta &amp;lt; 0$, the dot product $a \cdot u$ is also negative. 

But we need to get back to the dot product $a \cdot b$, where $b$ may have a magnitude different than one. This dot product $a \cdot b$ should depend on the magnitude of both vectors $\lVert a \rVert$ and $\lVert b \rVert$, and be symmetric in those vectors. Hence, we don't want to define $a \cdot b$ to be exactly the projection of $a$ on $b$; we want it to reduce to this projection for the case when $b$ is a unit vector. We can accomplish this very easily: just plug the definition $u = \frac{b}{\lVert b \rVert}$ into our dot product definition of the equation above. This leads to the definition that the dot product $a \cdot b$ divided by the magnitude $\lVert b \rVert$ of $b$ is the projection of $a$ onto $b$.

Then, we get a nice symmetric definition for the dot product $a \cdot b$.

\$\$
a \cdot b = \lVert a \rVert \lVert b \rVert \cos \theta
\$\$

The dot product of the vectors $a$ and $b$ when divided by the magnitude of $b$, is the projection of $a$ onto $b$. Notice how the dot product is positive for acute angles and negative for obtuse angles. 

The formula demonstrates that the dot product grows linearly with the length of both vectors and is commutative, $a \cdot b = b \cdot a$. 

&lt;h4&gt; The Formula for the Dot Product in terms of Vector Components &lt;/h4&gt;

The geometric definition of the dot product says that the dot product between two vectors $a$ and $b$ is 

\$\$
a \cdot b = \lVert a \rVert \lVert b \rVert \cos \theta
\$\$

Although this formula is nice for understanding the properties of the dot product, a formula for the dot product in terms of vector components would make it easier to calculate the dot product between two given vectors. 

Since the standard unit vectors are orthogonal, we immediately conclude that the dot product between a pair fo distinct standard unit vectors is zero.

The dot product between a unit vector and itself is also simple to compute. In this case, the angle is zero and $\cos\theta = 1$. Given that the vectors are all of length one, the dot products are 

\$\$
i \cdot i = j \cdot j = k \cdot k = 1
\$\$

The second step is to calculate the dot product between two three-dimensional vectors 

\$\$
a = (a_1, a_2, a_3) = a_1 i + a_2 j + a_3 k 
b = (b_1, b_2, b_3) = b_1 i + b_2 j + b_3 k
\$\$

To do this, we simply assert that for any three vectors $a$, $b$ and $c$, and any scalar $\lambda$, 

\$\$
(\lambda a ) \cdot b = \lambda (a \cdot b) = a \cdot (\lambda b)

(a+b) \cdot c = a \cdot c + b \cdot c
\$\$

These properties mean that the dot product is linear. 

Given this properties and the fact that the dot product is commutative, we can expand the dot product $a \cdot b$ in terms of components,

\$\$
a \cdot b = (a_1 i + a_2 j + a_3 k) \cdot (b_1 i + b_2 j + b_3 k)
\$\$

Since we know the dot product of unit vectors, we can simplify the dot product formula to

\$\$ 
a \cdot b = a_1 b_1 + a_2 b_2 + a_3 b_3
\$\$

Given the geometric definition of the dot product along with the dot product formula in terms of components, we are ready to calculate the dot product of any pair of two- or three-dimensional vectors.

&lt;h4&gt; The Cross Product &lt;/h4&gt;

 There are two ways to take the product of a pair of vectors. One of these methods of multiplication is the cross product, which is the subject of this page. The other multiplication is the dot product, which we discuss on another page. 

 The cross product is defined only for three-dimensional vectors. If $a$ and $b$ are two three-dimensional vectors, then their cross product, written as $a \times b$ and pronounced &quot;a cross b&quot; is another three-dimensional vector. We define the cross product vector $a \times b$ by the following three requirements:

&lt;ul&gt;&lt;li&gt; $a \times b$ is a vector that is perpendicular to both $a$ and $b$. &lt;/li&gt;
&lt;li&gt; The magnitude (or length) of the vector $a \times b$, written as $\lVert a \times b \rVert$ is the area of the parallelogram spanned by $a$ and $b$. &lt;/li&gt;
&lt;li&gt; The direction of $a \times b$ is determined by the right-hand rule. (This means that if we curl the fingers of the right hand from $a$ to $b$, then the thumb points in the direction of $a \times b$.) &lt;/li&gt;&lt;/ul&gt;

The below figure illustrates how, using trigonometry, we can calculate that area of the parallelogram spanned by $a$ and $b$ is 

\$\$
\lVert a \rVert \lVert b \rVert \sin \theta
\$\$

where $\theta$ is the angle between $a$ and $b$. This formulat shows that the magnitude of the cross product is largest when $a$ and $b$ are perpendicular. On the other hand, if $a$ and $b$ are parallel or if either vector is the zero vector, then the cross product is the zero vector (It is a good thing that we get the zero vector in these cases so that the above definition still makes sense. If the vectors are parallel or one vector is the zero vector, then there is not a unique line perpendicular to both $a$ and $b$. But since there is only one vector of zero length, the definition still uniquely determines the cross product.)

You can also verify that the applet demonstrates $b \times a = - a \times b$ and $a \times a = 0$, which are important properties of the cross product.

The geometric definition of the cross product is good for understanding the properties of the cross product. However, the geometric definition isn't so useful for computing the cross product of vectors. For computations, we will want a formula in terms of the components of vectors. We start by using the geometric definition to compute the cross product of the standard unit vectors. 

The parallelogram spanned by any two of these standard unit vectors is a unit square, which has area one. Hence, by the geometric definition, the cross product must be a unit vector. Since the cross product must be perpendicular to the two unit vectors, it must be equal to the other unit vector or the opposite of that unit vector. 

\$\$
i \times j = k
j \times k = i
k \times i = j
\$\$

By remembering that $b \times a = - a \times b$, you can infer that 

\$\$
j \times i = -k 
k \times j = -i
i \times k = -j
\$\$

Finally, the cross product of any vector with itself is the zero vector ($a \times a = 0$). In particular, the cross product of any standard unit vector with itself is the zero vector. 

With the exception of the two special properties mentioned above ($b \times a = - a \times b$, and $a \times a = 0$), we'll just assert that the cross product behaves like regular multiplication. It obeys the following properties:

\$\$
(ya) \times b = y (a \times b) = a \times (yb)
a \time (b+c) = a \times b + a \times c
(b+c) \times a = b \times a + c \times a
\$\$

where $a$, $b$, and $c$ are vectors in $\mathbb{R}^3$ and $y$ is a scalar. (These properties mean that the cross product is linear). We can use these properties, along with the cross product of the standard unit vectors, to write the formula for the cross product in terms of components. 

We write the components of $a$ and $b$ as:

\$\$
a = (a_1, a_2, a_3) = a_1 i + a_2 j + a_3 k
b = (b_1, b_2, b_3) = b_1 i + b_2 j + b_3 k
\$\$

First, we'll assume that $a_3 = b_3 = 0$. (Then, the manipulations are much easier.)

\$\$
a \times b = (a_1b_2 - a_2 b_1)k 
\$\$

Writing the result as a determinant, as we did in the last step, is a handy way to remember the result. 

&lt;h3&gt; Matrices &lt;/h3&gt;

The structure of a matrix allows us to define a fundamental operation on matrices: multiplication. This multiplication forms the basis of linear algebra. In particular, this matrix multiplication allows matrices to represent linear transformations (or linear functions) that transform vectors into other vectors. (A simple example of a linear transformation is the rotation of a vector.) Other uses of matrices involve calculating their determinant. 

&lt;h4&gt; Vectors as Matrices &lt;/h4&gt;

When we view vectors as matrices, we actually view them as a rotated version of the standard form, writing an $n$-dimensional vector as a $n \times 1$ column matrix. 

&lt;h4&gt; Matrix-Vector Product &lt;/h4&gt;

To define multiplication between a matrix $A$ and a vector $x$ (i.e., the matrix-vector product), we need to view the vector as a column matrix. 

One takes the dot product of $x$ with each rows of $A$. The first component of the matrix-vector product is the dot product of $x$ with the first row of $A$. In fact, $A$ has only one row, the matrix-vector product is really a dot product in disguise.

&lt;h4&gt; Matrix-Matrix Product &lt;/h4&gt;

Since we view vectors as column matrices, the matrix-vector product is simply a special case of the matrix-matrix product (i.e., a product between two matrices). Just like for the matrix-vector product, the product $AB$ between matrices $A$ and $B$ is defined only if the number of columns in $A$ equals the number of rows in $B$. Each column of $C$ is the matrix-vector product of $A$ with the respective column of $B$. 

&lt;h4&gt; The Transpose of a Matrix &lt;/h4&gt;

The transpose of a matrix is simply a flipped version of the original matrix. We can transpose a matrix by switching its rows with its columns. We denote the transpose of matrix $A$ by $A^T$. 

We can take a transpose of a vector as a special case. Since an $n$-dimensional vector $x$ is represented by an $n \times 1$ column matrix, the transpose of $x^T$ is a $1 \times n$ row matrix.

Although we won't typically write dot product as $x^T y$, we end up with a matrix multiplication equivalent to the familiar dot product $x \cdot y$. Moreover, you can view this dot product as forming the building block for the general matrix multiplication. 

&lt;h4&gt; Linear Transformations &lt;/h4&gt;

A linear transformation (or a linear map) is a function $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ that satisfies the following properties:

\$\$
T(x+y) = T(x) + T(y)
T(ax) = aT(x)
\$\$

for any vectors $x,y \in \mathbb{R}^n$ and any scalar $a \in \mathbb{R}$. 

It is simple enought to identify whether or not a given function $f(x)$ is a linear transformation. Just look at each component of $f(x)$. If each of these terms is a number times one of the components of $x$, then $f$ is a linear transformation.

The condition for a linear transformation is stronger than the condition one learns in grade school for a funciton whose graph is a line. A single variable function $f(x) = ax+b$ is not a linear transformation unless its $y$-intercept $b$ is zero. 

A useful feature of a linear transformation is that there is a one-to-one correspondence between matrices and linear transformations, based on matrix-vector multiplication. 

&lt;h4&gt; Matrices and Linear Transformations &lt;/h4&gt;

Given any $m \times n$ matrix $B$, we can define a function $g: \mathbb{R}^n \rightarrow \mathbb{R}^m (note the order of $m$ nad $n$ switched) by $g(x) = Bx$, where $x$ is an $n$-dimensional vector. 

In this way, we can associate with every matrix a function. What about going the other way around? Given some function, say $g: \mathbb{R}^n \rightarrow \mathbb{R}^m$, can we associate with $g(x)$ some matrix? We can only if $g(x)$ a a special kind of function called a linear transformation. The function $g(x)$ is a linear transformation if each term of the component of $g(x)$ is a number of times on of the variables. The important conclusion is that every linear transformation is associated with a matrix and vice versa. 

&lt;h4&gt; Determinants &lt;/h4&gt;

Many of the main uses for matrices in multivariate calculus involve calculating something called the determinant. It's useful, for example, to calculate the cross product as well as a change of variables. 

The determinant of a matrix is defined only for square matrices, i.e., $n \times n$ matrices with the same number of rows and columns. The determinant can be viewed as a funciton whose input is a square matrix and whose output is a number. The determinant of a $1 \times 1$ matrix is that number itself. 

We calculate the determinant of a $3 \times 3$ matrix in the exact same way. We proceed along the first row and multiply each component by the determinant of the submatrix formed by ignoring that component's row andcolumn. Through this procedure we calculate three terms, one for $a$, one for $b$, and one for $c$. Each of these terms is added together, only with alternating signs (the first term - the second term + the third term). The above procedure generalizes to large determinants, but $3 \times 3$ determinants will be enough for multivariate calculus. 

However, note that in this case, the vertical lines do not mean absolute value. The determinant can be negative. In mathematics, we like to use the same symbols to mean different things, which is okay as long as it's clear from context. Since the absolute value of an array of numbers is meaningless, the notation is unambiguous. 

&lt;h4&gt; The Relationships between $ and Area or Volume &lt;/h4&gt;

From the properties of the geometric definition of the cross product, we can discover a link between $2 \times 2$ determinants and area and a link between $3 \times 3$ determinants and volume.

&lt;h4&gt; Determinants and Linear Transformations &lt;/h4&gt;

A linear transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a mapping from $n$-dimensional space to $m$-dimensional space. Such a linear transformation can be associated with an $m \times n$ matrix. 

If we restrict ourselves to mappings within the same space, such as $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$, then $T$ is associated with a square $n \times n$ matrix. One can calculate the determinant of such a square matrix, and such determinants are related to area or volume. It turns out that the determinant of a matrix tells us important geometrical properties of its associated linear transformation. 

&lt;h4&gt; One-Dimensional Linear Transformations &lt;/h4&gt;

A one-dimensional linear transformation is a function $T(x) = ax$ for some scalar $a$. To view the one-dimensional case in the same way we view higher dimensional linear transformations, we can view $a$ as a $1 \times 1$ matrix. The determinant of the $1 \times 1$ matrix is just the numbwer $a$ itself. Although this case is very simple, we can gather some intuition about linear maps by first looking at this case. 

An example one-dimensional linear transformation is the function $T(x) = 3x$. We could visualize this function by its graph, which is a line through the origin with slope 3. However, instead, let's view it as a mapping from the real line $R$ back on to the real line $R$. In this case, we can think of the function as $x' = T(x)$, which maps the number $x$ on the $x$-axis to the new number $T(x)$ on the $x'$-axis. $T$ takes the number 1 and maps it to $3$. We also use the language that $3$ is the image of $1$ under the mapping $T$.

We can summarize this mapping by looking how $T$ maps an interval of numbers. For example, $T$ maps the interval $[0,1]$ to the interval $[0,3]$, as illustrated by the following figure. The fact that the determinant of the matrix associated with $T$ is 3 means that $T$ stretches objects so that their length is increased by a factor of 3. $T$ preserves the orientation of objects: in both the interval $[0,1]$ and its image $[0,3]$, red points are to the right of green points. 

In general, the linear transformation $T(x) = ax$ stretches objects to change their length by a factor of $\lvert a \rvert$. If $a$ is positive, $T$ preserves the orientation; if $a$ is a negative, $T$ reverse orientaiton. We will obtain similar conclusions for higher-dimensional lilnear transformation in terms of the determinant of the associated matrix. 

&lt;h4&gt; Two-Dimensional Linear Transformations &lt;/h4&gt;

A two-dimensional linear transformation is a function $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ of the form 

\$\$
T(x,y) = (ax+by, cx+dy)
\$\$

where $a,b,c$, and $d$ are numbers defining linear transformation. We can write this more succinctly as $T(x) = Ax$, where $x = (x,y)$ and $A$ is the $2 \times 2$ matrix containing the constants that define the linear transformation.

We will view $T$ as mapping objects from the $xy$-plane onto an $x'y'$-plane: (x',y') = T(x,y)$. as in the one-dimensional case, the geometric properties of this mapping will be reflected in the determinant of the matrix $A$ associated with $T$. 

To begin, we look at the linear transformation $T(x,y)$. As with all linear transformations, it maps the origin $x=(0,0)$ back to the origin $(0,0)$. We can get a feel for the behavior of $T$. We can get a feel for the behavior of $T$ by looking at its actions on the standard unit vectors, $i=(1,0)$ and $j=(0,1)$. 



&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.scratchapixel.com&quot;&gt; scratchpixel &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/ul&gt;&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/pointvec.png" /><media:content medium="image" url="http://localhost:4000/assets/images/pointvec.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">All About Training GAN</title><link href="http://localhost:4000/gan.html" rel="alternate" type="text/html" title="All About Training GAN" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/gan</id><content type="html" xml:base="http://localhost:4000/gan.html">&lt;!--more--&gt;

&lt;h2&gt; 1.  Generative Adversarial Networks &lt;/h2&gt;

&lt;p&gt;Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. The discriminator should not be able to distinguish between real and fake images.&lt;/p&gt;

&lt;p&gt;Another way to look at the GAN setup is that the discriminator is trying to guide the generator by telling what real images look like. The two networks try to achieve what is called the Nash Equilibrium with respect to each other.&lt;/p&gt;

&lt;h2&gt; 2.  Training GANs &lt;/h2&gt;

&lt;p&gt;GAN networks are a dynamic system where the optimization process is seeking not a minimum, but a equilibrium between two forces. There are no good objective metrics for evaluating whether a GAN is performing well during training, e.g. reviewing the loss is not sufficient. Instead the best approach is to visually inspect the generated examples and use subjective evaluation.&lt;/p&gt;

&lt;p&gt;Other quantitative measures, such as Inception Score (IS) or Frechet Inception Distance (FID) rely on pretrained models with a specific set of object classes. They lack an upper bound (which means hypothetically the highest possible score is infinity).&lt;/p&gt;

&lt;h3&gt; 2.1  Look at the Loss &lt;/h3&gt;

&lt;p&gt;In a discriminative model,the loss measures the accuracy of the prediction and we use it to monitor the progress of training. However, the loss in GAN measures how well we are doing compared with our opponent. Often, the generator cost increases but the image quality is actually improving.&lt;/p&gt;

&lt;p&gt;If you see the discriminator loss rapidly approaching, there is probably no chance of recovery and it is time to change something.&lt;/p&gt;

&lt;h3&gt; 2.1  Look at the Gradients &lt;/h3&gt;
&lt;p&gt;Monitor the gradients along with the losses in the networks. These can give you a good idea about the progress of training and can even help in debugging if things are not really working well.&lt;/p&gt;

&lt;p&gt;Ideally, the generator should receive large gradients early in the training because it needs to learn how to generate real-looking data. The discriminator on the other hand does not always get large gradients early on, because it can easily distinguish real and fake images.&lt;/p&gt;

&lt;p&gt;If the gradients at the layer of generator are too small, learning might be slow or not happening at all. The generator should get large gradients early on and the discriminator getting consistently high gradients at the top layer once the generator has been trained enough.&lt;/p&gt;

&lt;h2&gt; 3. Detecting GAN Failure Modes &lt;/h2&gt;
&lt;p&gt;The reason why GANs are difficult to train is that both generator and the discriminator are trained simultaneously in a zero-sum game. This means that improvements to one model come at the expense of the other model. 
The goal of training two models involves finding a point of equilibrium between the two competing concerns. It also means that everytime the parameters of one model are updated the nature of the optimization problem that is being solved is updated as well. The technical challenge of training two competing neural networks at the same time is that they can fail to converge.&lt;/p&gt;

&lt;ul&gt;&lt;li class=&quot;highlight&quot;&gt; &lt;b&gt; Convergence Failure &lt;/b&gt;&lt;/li&gt;
&lt;div class=&quot;indent&quot;&gt; The fact that GANs are composed by two networks, and each of them has its loss function leads to GANs unstability. In GAN architecture, the discriminator tries to minimize a cross-entropy while the generator tries to maximize it. When discriminator confidence is high and the discriminator starts to reject the samples that are produced by the generator, generator's gradient vanishes. 

This scenario happens when the generator score reaches near zero and the discriminator score reaches near one. The discriminator is overpowering the generator. If the score does not recover from these values for many iterations, it is better to stop training. &lt;/div&gt;

&lt;li class=&quot;highlight&quot;&gt;&lt;b&gt; Mode Collapse &lt;/b&gt; &lt;/li&gt;
&lt;div class=&quot;indent&quot;&gt; Mode collapse is when the GAN produces a small variety of images with many duplicates. This happens when the generator is unable to learn a rich feature representation because  it learns to associate similar outputs to multiple different inputs. The most promising way to check for mode collapse is to inspect the generated images. If there is little diversity in the output and some of them are almost identical, there is likely mode collapse. If you observe this happening, you should try to increase the ability of the generator to create more diverse outputs or impair the discriminator by randomly giving false labels to real images.

Another type of behavior you should look out for is when the generator oscillates between generating specific examples in the domain. They progress from generating one kind of sample to generating another kind of sample without eventually reaching equilibrium.
&lt;/div&gt;

&lt;li class=&quot;highlight&quot;&gt; &lt;b&gt; Diminisheed Gradient &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;
&lt;div class=&quot;indent&quot;&gt; This situation happens when the discriminator gets too successful that the generator gradient vanishes and learns nothing.


&lt;h2&gt; Lessons I Learned &lt;/h2&gt;
&lt;div class=&quot;three&quot;&gt; Use a batch size smaller than or equal to 64.&lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; In my experience, using bigger batch sizes often hurt the performance. I suspect it fuels the problem of discriminator getting too good at discriminating the real and fake images, since large batch size means providing a lot of examples to train on. &lt;/div&gt;

&lt;div class=&quot;three&quot;&gt; Add noise to both real and synthetic data. &lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; It is well known that making the training of discriminator more difficult is beneficial for the overall stability. Adding noise increases the complexity of the discriminator training and stabilizes the data distribution of the two competing networks. &lt;/div&gt;

&lt;div class=&quot;three&quot;&gt; Use Label Smoothing &lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; If the label for real images is set to 1, change it to a lower value like 0.9. This solution discourages the discriminator from being overconfident. &lt;/div&gt;

&lt;div class=&quot;three&quot;&gt; Different learning rates for the generator and discriminator a.k.a. Two Time-Scale Update Rule &lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; In my experience, choosing a higher learning rate for the discriminator(i.e. 0.0004) and a lower one(i.e. 0.0001) for the generator works well in practice. I guess the reason is that the generator has to make small steps to fool the discriminator so it does not choose fast but not precise solutions to win the adversarial game. &lt;/div&gt;

&lt;div class=&quot;three&quot;&gt; Use some kind of normalization method &lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; For me, applying Spectral Normalization, a particular kind of normalization applied on the convolutional kernels, greatly helped the stability of training.&lt;/div&gt;

&lt;blockquote&gt; I learned that hyperparameter tuning takes a lot of time and patience especially for training GANs. &lt;/blockquote&gt;



&lt;/div&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">I wrote a short article about what I learned training GANs. GAN is well known for its instability in training and there are pitfalls worth knowing.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/gan.png" /><media:content medium="image" url="http://localhost:4000/assets/images/gan.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dissecting the Camera Matrix (Part 2)</title><link href="http://localhost:4000/extrinsic.html" rel="alternate" type="text/html" title="Dissecting the Camera Matrix (Part 2)" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/extrinsic</id><content type="html" xml:base="http://localhost:4000/extrinsic.html">&lt;!--more--&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/cali.png&quot; /&gt;&lt;/picture&gt;
&lt;div class=&quot;caption&quot;&gt; Overview of the Camera Calibration Parameters &lt;/div&gt;

&lt;h2&gt; The Extrinsic Camera Matrix &lt;/h2&gt;

&lt;p&gt;The extrinsic matrix takes the form of a rigid transformation matrix: a $3 \times 3$ rotation matrix in the left-block, and $3 \times 1$ translation column-vector in the right.&lt;/p&gt;

&lt;p&gt;$$&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
\begin{array}{ccc|c}
  r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; t_1 &lt;br /&gt;
  r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; t_2 &lt;br /&gt;
  r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; t_3
\end{array}
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;$$&lt;/p&gt;

&lt;p&gt;It is common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation:&lt;/p&gt;

&lt;p&gt;$$&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
\begin{array}{ccc|c}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; t_1 &lt;br /&gt;
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; t_2 &lt;br /&gt;
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; t_3 &lt;br /&gt;
\hline
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{array}
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;\times&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
\begin{array}{ccc|c}
r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; 0 &lt;br /&gt;
r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; 0 &lt;br /&gt;
r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; 0 &lt;br /&gt;
\hline
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{array}
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;$$&lt;/p&gt;

&lt;p&gt;The matrix describes how to transform points in world coordinates to camera coordinates. The important thing to remember about the extrinsic matrix is that it describes how the &lt;span class=&quot;blue&quot;&gt; world &lt;/span&gt; is transformed &lt;span class=&quot;blue&quot;&gt; relative to the camera &lt;/span&gt;. This if often counter-intuitive, because we usually want to specify how the &lt;span class=&quot;red&quot;&gt; camera &lt;/span&gt; is transformed &lt;span class=&quot;red&quot;&gt; relative to the world &lt;/span&gt;.&lt;/p&gt;

&lt;h2&gt; Building the Extrinsic Matrix from Camera Pose &lt;/h2&gt;

&lt;p&gt;Like I said before, it is often more natural to &lt;span class=&quot;highlight-yellow&quot;&gt; specify the camera’s pose directly &lt;/span&gt; rather than specifying &lt;span class=&quot;highlight-pink&quot;&gt; how world points should transform to camera coordinates &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Luckily, building an extrinsic camera matrix this way is easy: just &lt;span class=&quot;highlight-green&quot;&gt; build a rigid transformation matrix that describes the camera’s pose &lt;/span&gt; and then &lt;span class=&quot;rainbow&quot;&gt; take its inverse &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;$$&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
\begin{array}{c|c}
R &amp;amp; t &lt;br /&gt;
0 &amp;amp; 1 
\end{array}
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;=&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
\begin{array}{c|c}
R_c &amp;amp; C &lt;br /&gt;
0 &amp;amp; 1 
\end{array}
\end{bmatrix}^{-1}&lt;/p&gt;

&lt;p&gt;$$&lt;/p&gt;

&lt;p&gt;Let $C$ be a column vector describing the location of the camera-center in world coordinates, and let $R_c$ be the rotation matrix describing the camera’s orientation with respect to the world coordinate axes. Then extrinsic matrix is obtained by inverting the camera’s pose matrix.&lt;/p&gt;

&lt;blockquote&gt; Algebraically a rotation matrix in $n$-dimensions is a $n \times n$ special orthogonal matrix, i.e. an orthogonal matrix whose determinant is 1. &lt;/blockquote&gt;

&lt;div class=&quot;sidenote&quot;&gt; We can define matrix $R$ that rotates in the $xy$-Cartesian plane counterclock-wise through an angle $\theta$ about the origin of the Cartesian system as follows:

\$\$
R = \begin{bmatrix}
\cos\theta &amp;amp; -\sin\theta \\
\sin\theta &amp;amp; \cos\theta
\end{bmatrix}
\$\$

&lt;/div&gt;

&lt;div class=&quot;sidenote&quot;&gt; The set of all rotation matrices form a group, known as the special orthogonal group. The inverse of a rotation matrix is its transpose, which is also a rotation matrix. 

\$\$
\displaylines{
R^T = R^{-1} \\
det(R) = 1
}
\$\$
&lt;/div&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/rotation.png&quot; /&gt;&lt;/picture&gt;
&lt;div class=&quot;caption&quot;&gt; the extrinsic matrix is obtained by inverting the camera's pose matrix &lt;/div&gt;

&lt;p&gt;We here use the fact that the inverse of a rotation matrix is its transpose, and inverting a translation matrix simply negates the translation vector. Relationship between the extrinsic matrix parameters and the camera’s pose is straightforward:&lt;/p&gt;

&lt;p&gt;$$
\displaylines{
R = R^T_c &lt;br /&gt;
t = -RC
}
$$&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://ksimek.github.io/2012/08/14/decompose/&quot;&gt; ksimek blog &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://prateekvjoshi.com/2014/05/31/understanding-camera-calibration/&quot;&gt; prateekvjoshi blog &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/extrinsic.png" /><media:content medium="image" url="http://localhost:4000/assets/images/extrinsic.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Advanced PyTorch: Things You Didn’t Know</title><link href="http://localhost:4000/pytorch.html" rel="alternate" type="text/html" title="Advanced PyTorch: Things You Didn’t Know" /><published>2021-09-12T00:00:00+09:00</published><updated>2021-09-12T00:00:00+09:00</updated><id>http://localhost:4000/pytorch</id><content type="html" xml:base="http://localhost:4000/pytorch.html">&lt;!--more--&gt;

&lt;h2&gt; Flatten Operation for a Batch of Image Inputs to a CNN 
&lt;/h2&gt;
&lt;p&gt;Flattening specific tensor axis is often required with CNNs because we work with batches of inputs opposed to single inputs. A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flattened out so that the fully connected layer can accept them as the input. A flatten operation is a specific type of reshaping operation where by all of the axes are smooshed or squashed together.&lt;/p&gt;

&lt;p&gt;To flatten a tensor, we need to have at least two axes. This makes it so that we are starting with something that is not already flat. For example, in the MNIST dataset, we will look at an handwritten image of eight. This image has 2 distinct dimensions, height and width.&lt;/p&gt;

&lt;p&gt;The height and width are $18 \times 18$ respectively. These dimensions tell use that this is a cropped image becaue the MNIST dataset contains $28 \times 28$ images. Let’s see how these two axes of height and width are flattened out into a single axis of length 324 (c.f. 324 what we get when multiplying 18 with 18).&lt;/p&gt;

&lt;h3&gt; Flattening Specific Axes of a Tensor &lt;/h3&gt;

&lt;p&gt;Tensor inputs to a convolutional neural network typically have 4 axes, one for batch size, one for color channels, and one each for height and width.&lt;/p&gt;

&lt;p&gt;$$
[B,C,H,W]
$$&lt;/p&gt;

&lt;p&gt;Suppose we have the following three tensors:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Each of these has a shape of $4 \times 4$, so we have three rank-2 tensors. For our purpose, we’ll consider these to be three $4 \times 4$ images that we will use to create a batch that can be passed to a CNN. Batches are represented using a single tensor, so we’ll need to combine these three tensors into a single larger tensor that has 3 axes instead of 2.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here, we used the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stack()&lt;/code&gt; method to concatenate our sequence of tensors along a new axis. Since we have three tensors along a new axis, we know that the length of this axis should be 3. At this point, we have a rank-3 tensor that contains a batch of three $4 \times 4$ images. All we need to do now to get this tensor into a form that a CNN expects is add an axis for the color channels. We basically have an implicit single color channel for each of these image tensors, so in practice, these would be grayscale images.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Notice how the additional axis of length 1 doesn’t change the number of elements in the tensor. This is because the product of the components values doesn’t change when we multiply by one.&lt;/p&gt;

&lt;p&gt;The first axis has 3 elements. Each element of the first axis represents an image. For each image, we have a single color channel on the channel axis. Each of these channels contain 4 arrays that contain 4 numbers or scalar components.&lt;/p&gt;

&lt;h3&gt; Flattening the Tensor Batch &lt;/h3&gt;

&lt;p&gt;Let’s see how to flatten images in this batch. Remember the whole batch is a single tensor that will be passed to the CNN, &lt;span class=&quot;underline&quot;&gt; we don’t want to flatten the whole thing &lt;/span&gt; We only want to &lt;span class=&quot;glow&quot;&gt; flatten the image tensors &lt;/span&gt; within the batch tensor.&lt;/p&gt;

&lt;p&gt;For example, if we do the following operations on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# this is the same operation as t.flatten()
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What I want you to notice about this output is that we have flattened the entire batch, and this smashes all the batches together into a single axis. The flattened batch won’t work well inside our CNN because we need individual predictions for each image within our batch tensor, and now we have a flattened mess.&lt;/p&gt;

&lt;p&gt;The solution here, is to flatten each image while &lt;span color=&quot;blink&quot;&gt; still maintaining the batch axis &lt;/span&gt;. This means we want to &lt;span class=&quot;underline&quot;&gt; flatten only part of the tensor &lt;/span&gt;. We want to flatten the color channel axis with the height and width axes.&lt;/p&gt;

&lt;blockquote&gt; The Axes that Need to be Flattened: $[C,H,W]$ &lt;/blockquote&gt;
&lt;p&gt;This can be done with PyTorch’s built in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flatten()&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Notice how we specified the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start_dim&lt;/code&gt; parameter.This tells the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flatten()&lt;/code&gt; method which axis it should start the flatten operation. Now we have a rank-2 tensor with three single color channel images that have been flattened out into 16 pixels.&lt;/p&gt;

&lt;h3&gt; Flattening an RGB Image &lt;/h3&gt;
&lt;p&gt;If we flatten an RGB image, what happens to the color? Each color channel will be flattened first, then the flattened channels will be lined up side by side on a single axis of the tensor.&lt;/p&gt;

&lt;p&gt;For example, we build an RGB image tensor like the following code:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;By flattening the image tensor, this is how it is going to look like.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2&gt; Broadcasting and Element-Wise Operations with PyTorch &lt;/h2&gt;

&lt;blockquote&gt; Remember, all these rules apply to PyTorch Tensors! Python built-in types such as list will not behave this way.&lt;/blockquote&gt;
&lt;h3&gt; Element-Wise Operations &lt;/h3&gt;
&lt;p&gt;An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two elements are said to be corresponding if the two elements occupy the same position within the tensor. The position is determined by the indexes used to locate each element. Therefore, we can deduce that tensors must have the same shape in order to perform an element-wise operation.&lt;/p&gt;

&lt;h3&gt; Broadcasting Tensors &lt;/h3&gt;
&lt;p&gt;Broadcasting describes how tensors with different shapes are treated during element-wise operations. For example, suppose we have the following two tensors:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What will be the result of this two tensors’ element-wise addition operation? Even though these two tensors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t2&lt;/code&gt; will be transformed via broadcasting to match the shape of the higher rank tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t1&lt;/code&gt;, and the element-wise operation will be performed as usual.&lt;/p&gt;

&lt;p&gt;The concept of broadcasting is the key to understanding how this operation will be carried out. We can check the broadcast transformation using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcast_to()&lt;/code&gt; numpy function.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;broadcast_to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;When do we actually use broadcasting? We often need to use broadcasting when we are preprocessing and especially during normalization routines.&lt;/p&gt;

&lt;h3&gt; Element-Wise Operation Applies to Comparision and Functions &lt;/h3&gt;
&lt;p&gt;Comparison operations are also element-wise operations. For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element containing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.bool&lt;/code&gt; value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt;. It is also fine to assume that the function is applied to each element of the tensor.&lt;/p&gt;

&lt;div class=&quot;sidenote&quot;&gt; there are other ways to refer to element-wise operations, such as component-wise or point-wise &lt;/div&gt;

&lt;h2&gt; Argmax and Reduction Operations for Tensors &lt;/h2&gt;
&lt;p&gt;Now, we will focus in on the frequently used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;argmax()&lt;/code&gt; function, and we’ll see how to access the data inside our tensors.&lt;/p&gt;

&lt;h3&gt; Tensor Reduction Operation &lt;/h3&gt;

&lt;blockquote&gt; A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.&lt;/blockquote&gt;

&lt;p&gt;Reduction operations allow us to perform operations on element within a single tensor. Let’s look at an example. Suppose we have the following rank-2 tensor:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;8.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The sum of our tensor’s scalar components is calculated using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum()&lt;/code&gt; tensor method. The result of this call is a &lt;span class=&quot;rainbow&quot;&gt; scalar-valued tensor &lt;/span&gt;. Since the number of elements have been reduced by the operation, we can conclude that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum()&lt;/code&gt; method is a reduction operation.&lt;/p&gt;

&lt;p&gt;Other common reduction functions include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.sum()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.prod()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.mean()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.std()&lt;/code&gt;. All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the tensor’s elements.&lt;/p&gt;

&lt;p&gt;Reduction operations in general allow us to compute aggregate values across data structures. But do reduction operations always reduce to a tensor with a single element? The answer is no. In fact, we often reduce specific axes at a time. This process is important.&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://deeplizard.com/learn/video/K3lX3Cltt4c&quot;&gt; deeplizard &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content><author><name>seri</name></author><category term="PyTorch" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/pytorch.png" /><media:content medium="image" url="http://localhost:4000/assets/images/pytorch.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Guide to Linear Algebra (Part 1)</title><link href="http://localhost:4000/no-bullshit.html" rel="alternate" type="text/html" title="Guide to Linear Algebra (Part 1)" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>http://localhost:4000/no-bullshit</id><content type="html" xml:base="http://localhost:4000/no-bullshit.html">&lt;!--more--&gt;

&lt;h2&gt; Computational Linear Algebra &lt;/h2&gt;

&lt;p&gt;This chapter covers the computational aspects of performing matrix calculations. Understanding matrix computations is important because all later chapters depend on them. Suppose we’re given a huge matrix $ A \in R^{n \times n} $ with $ n=1000 $. Hidden behind the innocent-looking mathematical notation of the matrix inverse $A^{-1}$, the matrix product $AA$, and the matrix determinant $ | A |$, lie monster coputations involving all the $1000 \times 1000 = 1$ million entries of the matrix $A$. Millions of arithmetic operations must be performed, so I hope you have at least a thousand pencil ready!&lt;/p&gt;

&lt;p&gt;Okay, calm down. I won’t actually make you calculate millions of arithmetic operations. In fact, to learn linear algebra, it is sufficient to know how to carry out calculations with $3 \times 3$ and $4 \times 4$ matrices. Even for such moderately sized matrices, computing products, inverses, and determinants by hand are serious computational tasks. If you’re ever required to take a linear algebra final exam, you need to make sure you can do these calculations quickly. Even if no exam looms in your imminent future, it’s important to practice matrix operations by hand to get a feel for them.&lt;/p&gt;

&lt;p&gt;This chapter will introduce you to the following computational tasks involving matrices:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span class=&quot;blue&quot;&gt;Gauss-Jordan elimination &lt;/span&gt; Suppose we're trying to solve two equations in two unknowns $x$ and $y$:

\$\$ 
\displaylines{
ax+by = c \\\
dx+ ey= f
}
\$\$

If we add $\alpha$\times the first equation to the second equation, we obtain an equivalent system of equations:

\$\$
\displaylines{
ax + by = c \\\
(d + \alpha a)x + (e + \alpha b)y = f + \alpha c
}
\$\$

This is called a &lt;span class=&quot;highlight-sketch&quot;&gt; row operation &lt;/span&gt;: we added $\alpha$-times the first row to the second row. Row operations change the coefficient of the system of equations, but leave the solution unchanged. Gauss-Jordan elimination is a systematic procedure for solving systems of linear equations using row operations. &lt;/li&gt;
&lt;li&gt; &lt;span class=&quot;blue&quot;&gt; Matrix product &lt;/span&gt; The product $AB$ between matrices $A \in \mathbb{R}^{m \times l}$ and $B \in \mathbb{R}^{l \times n}$ is the matrix $C \in \mathbb{R}^{m \times n}$ whose coefficients $c_{ij}$ are defined by the formula $c_{ij} = \sum_{k=1}^{l}a_{ik}b_{kj}$ for all $i \in \lbrack 1, \dots, m \rbrack $ and $j \in \lbrack 1, \dots, n \rbrack $. We'll soon unpack this formula and learn about its intuitive interpretation: that computing $C = AB$ is computing all the dot products between the rows of $A$ and the columns of $B$. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;blue&quot;&gt; Determinant &lt;/span&gt; The determinant of a matrix $A$, denoted $|A|$ is an operation that gives us useful information about the linear independence of the rows of the matrix. The determinant is connected to many notions of linear algebra: linear independence, geometry of vectors, solving systems of equations, and matrix invertibility. We'll soon discuss these aspects. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;blue&quot;&gt; Matrix inverse &lt;/span&gt; We'll build upon our knowledge of Gauss-Jordan elimination, matrix products, and determinants to derive three different procedures for computing the matrix inverse $A^{-1}$.&lt;/li&gt;&lt;/ul&gt;

&lt;h3&gt; Reduced Row Echelon Form &lt;/h3&gt;
&lt;p&gt;In this section, we’ll learn to solve systems of linear equations using the Gauss-Jordan elimination procedure. A system of equations can be represented as a matrix of coefficients. The Gauss-Jordan elimination procedure converts any matrix into its &lt;span class=&quot;highlight-sketch&quot;&gt; &lt;b&gt; reduced row echelon form (RREF) &lt;/b&gt;&lt;/span&gt;. We can easily find the solution (or solutions of the system of equations from the RREF.&lt;/p&gt;

&lt;p&gt;Listen up: the material covered in this section requires your full on, caffeinated attention, as the procedures you’ll learn are somewhat tedious. Gauss-Jordan elimination involves many repetitive mathematical manipulations of arrays of numbers. It’s important you hang in there and follow through the step-by-step manipulations, as well as verify each step I present on your own with pen and paper.&lt;/p&gt;

&lt;h4&gt; Solving Equations &lt;/h4&gt;
&lt;p&gt;Suppose you’re asked to solve the following system of equations:&lt;/p&gt;

&lt;p&gt;$$
\displaylines{
1x_1 + 2x_2  = 5 \&lt;br /&gt;
3x_1 + 9x_2 = 21}
$$&lt;/p&gt;

&lt;p&gt;The standard approach is to use one of the equation-solving tricks we learned to combine the equations and find the values of the two unknowns $x_1$ and $x_2$.&lt;/p&gt;

&lt;p&gt;Observe that the names of the two unknowns are irrelevant to the solution of the system of equations. Indeed, the solution $(x_1, x_2)$ to the above system of equations is the same as the solution $(s,t)$ to the system of equations&lt;/p&gt;

&lt;p&gt;$$
\displaylines{
1s+ 2t = 5 \&lt;br /&gt;
3s+ 9t = 21}
$$&lt;/p&gt;

&lt;p&gt;The important parts of a system of linear equations are the &lt;span class=&quot;highlight-yellow&quot;&gt; coefficients &lt;/span&gt; in front of the variables and the constants on the right-hand side of each equation.&lt;/p&gt;

&lt;h4&gt; Augmented Matrix &lt;/h4&gt;
&lt;p&gt;The system of linear equations can be written as an &lt;span class=&quot;highlight-green&quot;&gt; augmented matrix &lt;/span&gt;:&lt;/p&gt;

&lt;p&gt;$$
\begin{pmatrix} 1 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 5 \&lt;br /&gt;
                3 &amp;amp; 9 &amp;amp;\bigm | &amp;amp; 21
\end{pmatrix}
$$&lt;/p&gt;

&lt;p&gt;The first column corresponds to the coefficients of the first variable, the second column is for the second variable, and the last column corresponds to the constants of the right-hand side. It is customary to draw a vertical line where the equal signs in the equations would normally appear. This line helps distinguish the coefficients of the equations from the column of constants on the right-hand side.&lt;/p&gt;

&lt;p&gt;Once we have the augmented matrix, we can simplify it by using &lt;span class=&quot;highlight-yellow&quot;&gt; row operations &lt;/span&gt; (which we’ll discuss shortly) on its entries. After simplification by row operations, the augmented matrix will be transformed to&lt;/p&gt;

&lt;p&gt;$$
\begin{pmatrix}
1 &amp;amp; 0 &amp;amp;\bigm | &amp;amp; 5 \&lt;br /&gt;
0 &amp;amp; 1 &amp;amp;\bigm | &amp;amp; 2
\end{pmatrix}&lt;/p&gt;

&lt;p&gt;$$&lt;/p&gt;

&lt;p&gt;which corresponds to the system of equations&lt;/p&gt;

&lt;p&gt;$$
\displaylines{
x_1 = 1 \&lt;br /&gt;
x_2 = 2}
$$&lt;/p&gt;

&lt;p&gt;This is a &lt;span class=&quot;monospace&quot;&gt; trivial &lt;/span&gt; system of equations; there is nothing left to solve and we can see that the solutions are $x_1 = 1$ and $x_2 = 2$. This example illustrates the general idea of the Gauss-Jordan elimination procedure for solving the system of equations by manipulating an augmented matrix.&lt;/p&gt;

&lt;h4&gt; Row Operations &lt;/h4&gt;
&lt;p&gt;We can manipulate the rows of an augmented matrix without changing its solutions. We’re allowed to perform the following three types of row operations:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt; Add a multiple of one row to another row &lt;/li&gt;
&lt;li&gt; Swap the position of the two rows &lt;/li&gt;
&lt;li&gt; Multiply a row by a constant &lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Let’s trace the sequence of row operations needed to solve the system of equations&lt;/p&gt;

&lt;p&gt;$$
\displaylines{x_1 + 2x_2 = 5 \&lt;br /&gt;
3x_1 + 9x_2 = 21}
$$&lt;/p&gt;

&lt;p&gt;starting from its augmented matrix:&lt;/p&gt;

&lt;p&gt;$$
\begin{pmatrix}
1 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 5 \&lt;br /&gt;
3 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 21
\end{pmatrix}
$$&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As a first step, we eliminate the first variable in the second row by subtracting three times the first row from the second row.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$ 
\begin{pmatrix}
1 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 5 \&lt;br /&gt;
0 &amp;amp; 3 &amp;amp;\bigm | &amp;amp; 6
\end{pmatrix}
$$&lt;/p&gt;

&lt;p&gt;We denote this row operation as $R_2 \leftarrow R_2 - 3R_1$.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To simplify the second row, we divide it by 3 to obtain&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$ 
\begin{pmatrix}
1 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 5 \&lt;br /&gt;
0 &amp;amp; 1 &amp;amp;\bigm | &amp;amp; 2 
\end{pmatrix}
$$&lt;/p&gt;

&lt;p&gt;This row operation is denoted $R_2 \leftarrow \frac{1}{3}R_2$.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The final step is to eliminate the second variable from the first row. We do this by subtracting two times the second row from the first row $R_1 \leftarrow R_1 - 2 R_2$:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$
\begin{pmatrix}
1 &amp;amp; 0 &amp;amp;\bigm | &amp;amp; 1 \&lt;br /&gt;
0 &amp;amp;  1 &amp;amp;\bigm | &amp;amp; 2
\end{pmatrix}
$$&lt;/p&gt;

&lt;p&gt;We can now read off the solution: $x_1 =1$ and $x_2 = 2$.&lt;/p&gt;

&lt;p&gt;Note how we simplified the augmented matrix through a specific procedure: we followed the &lt;span class=&quot;rainbow&quot;&gt; Gauss-Jordan elimination algorithm &lt;/span&gt; to bring the matrix into its reduced row echelon form.&lt;/p&gt;

&lt;p&gt;The reduced row echelon form (RREF) is the simplest form for an augmented matrix. Each row contains a &lt;span class=&quot;circle-sketch-highlight&quot;&gt;&lt;b&gt; leading one &lt;/b&gt;&lt;/span&gt; (a numeral 1) also known as a &lt;span class=&quot;circle-sketch-highlight&quot;&gt; &lt;b&gt;pivot &lt;/b&gt;&lt;/span&gt;. Each column’s pivot is used to eliminate the numbers that lie below and above it in the same column. The end result of this procedure is the reduced row echelon form:&lt;/p&gt;

&lt;p&gt;$$
\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; \ast &amp;amp; 0 &amp;amp;\bigm | \ast \&lt;br /&gt;
0 &amp;amp; 1 &amp;amp; \ast &amp;amp; 0 &amp;amp;\bigm | \ast \&lt;br /&gt;
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp;\bigm | \ast
\end{pmatrix}
$$&lt;/p&gt;

&lt;p&gt;Note the matrix contains only zero entries below and above the pivots. The asterisks $\ast$ denote arbitrary numbers that could not be eliminated because no leading one is present in these columns.&lt;/p&gt;

&lt;div class=&quot;definition&quot;&gt;The solution to a system of linear equations in the variables $x_1, x_2, \dots, x_n$ is the set of values $\{(x_1, x_2, \dots, x_n)\}$ that satisfy all the equations.&lt;/div&gt;

&lt;div class=&quot;definition&quot;&gt; Gaussian elimination is the process of bringing a matrix into row echelon form. &lt;/div&gt;
&lt;div class=&quot;definition&quot;&gt; A matrix is said to be in row echelon form (REF) if all entries below the leading ones are zero. This form can be obtained by adding or subtracting the row with the leading one from the rows below it. &lt;/div&gt;
&lt;div class=&quot;definition&quot;&gt; Gaussian-Jordan elimination is the process of bringing a matrix into reduced row echelon form. &lt;/div&gt;
&lt;div class=&quot;definition&quot;&gt; A matrix is said to be in reduced row echelon form (RREF) if all the entries below and above the pivots are zero. Starting from the REF, we obtain the RREF by subtracting the row containing the pivots from the rows above them.&lt;/div&gt;
&lt;div class=&quot;definition&quot;&gt; the rank of the matrix $A$ is the number of pivots in the RREF of $A$. &lt;/div&gt;

&lt;h3&gt; Number of Solutions &lt;/h3&gt;

&lt;p&gt;A system of linear equations in three variables could have:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span class=&quot;rainbow&quot;&gt; one solution &lt;/span&gt; If the RREF of a matrix has a pivot in each row, we can read off the values of the solution by inspection. 

\$\$

\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; c_1 \\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; c_2 \\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; c_3
\end{bmatrix}

\$\$

The unique solution is $x_1 = c_1$, $x_2 = c_2$, and $x_3 = c_3$. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;rainbow&quot;&gt; Infinitely many solutions 1 &lt;/span&gt; If one of the equations is redundant, a row of zeros will appear when the matrix is brought to the RREF. This happens when one of the original equations is a linear combination of the other two. In such cases, we're really solving two equations  in three variables, so can't pin down one of the unknown variables. We say the solution contains &lt;span class=&quot;underline&quot;&gt; a free variable &lt;/span&gt;. For example, consider the following RREF:

\$\$
\begin{bmatrix} 
1 &amp;amp; 0 &amp;amp; a_1 &amp;amp; c_1 \\\
0 &amp;amp; 1 &amp;amp; a_2 &amp;amp; c_2 
\end{bmatrix}
\$\$

The column that doesn't contain a leading one corresponds to the free variable. To indicate that $x_3$ is a free variable, we give it a special label $x_3 \equiv t$. The variable $t$ could be any number $t \in \mathbb{R}$. In other words, when we say $t$ is free, it means $t$ can take on any value from $-\infty$ to $+\infty$. The information in the augmented matrix can now be used to express $x_1$ and $x_2$ in terms of the right-hand constants and the free variable $t$:

\$\$
\begin{Bmatrix} x_1 = c_1 - a_1 t \\\ x_2 = c_2 - a_2 t \\\ x_3 = t, \forall t \in \mathbb{R} \end{Bmatrix} = \begin{Bmatrix} \begin{bmatrix} c_1 \\\ c_2 \\\ 0 \end{bmatrix} + t \begin{bmatrix} -a_1 \\\ -a_2 \\\ 1 \end{bmatrix}, \forall t \in \mathbb{R}\end{Bmatrix}
\$\$.

The solution corresponds to the equation of a line passing through the point $(c_1, c_2, 0)$ with direction vector $(-a_1, -a_2, 1)$. We'll discuss the geometry of lines in the next section. For now, it's important that you understand that a system of equations can have more than one solution; any point on the line $l \equiv \{(c_1, c_2, 0) + t(-a_1, -a_2, 1), \forall t \in \mathbb{R}\}$ is a solution to the above system of equations. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;rainbow&quot;&gt; Infinitely many solutions 2 &lt;/span&gt; It's also possible to obtain a two-dimensional solution space. This happens when two of the three equations are redundant. In this case, there will be a single leading one, and thus two free variables. For example, in the RREF

\$\$
\begin{bmatrix} 
0 &amp;amp; 1 &amp;amp; a_1 &amp;amp; c_1 \\\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 
\end{bmatrix}
\$\$

the variables $x_1$ and $x_3$ are free. As in the previous infinitely-many-solutions case, we define new labels for the free variables $x_1 \equiv s$ and $x_3 \equiv t$, where $ s \in \mathbb{R}$ and $t \in \mathbb{R}$ are two arbitrary numbers. The solution to this system of equations is 

\$\$ 
\begin{Bmatrix} 
x_1 = s \\\ x_2 = c_2 - a_2 t \\\ x_3 = t , \\\ \forall s,t \in \mathbb{R} \end{Bmatrix} = \begin{Bmatrix} \begin{bmatrix} 0 \\\ c_2 \\\ 0 \end{bmatrix} + s \begin{bmatrix} 1 \\\ 0 \\\ 0 \end{bmatrix} + t \begin{bmatrix} 0 \\\ -a_2 \\\ 1 \end{bmatrix}, \forall s,t \in \mathbb{R} 
\end{Bmatrix} 
\$\$

This solution set corresponds to the parametric equation of a plane that contains the point $(0,c_2, 0)$ and the vectors $(1,0,0)$ and $(0, -a_2, 1)$. 

The general equation for the solution plane is $0x+1y+a_2z = c_2$, as can be observed from the first row of the augmented matrix. In the next section, we'll learn more about the geometry of planes and how to convert between their general and parametric forms. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;rainbow&quot;&gt; no solutions &lt;/span&gt; If there are no numbers $(x_1, x_2, x_3)$ that simultaneously satisfy all three equations, the system of equations has no solution. An example of a system of equations with no solution is the pair $ s+t = 4$ and $s+t = 44$. There are no numbers $(s,t)$ that satisfy both these equations. 

A system of equations has no solution if its reduced row echelon form contains a row of zero coefficients with a nonzero constant in the right-hand side:

\$\$
\begin{Bmatrix}
\begin{array}{ccc|c}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; c_1 \\\\ 
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; c_2 \\\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; c_3 
\end{array}
\end{Bmatrix}
\$\$

If $c_3 \neq 0$ this system of equations is impossible to satisfy. There is no solution because there are no numbers $(x_1, x_2, x_3)$ such that $0x_1 + 0x_2 + 0x_3 = c_3$. 

Dear reader, we've reached the first moment in this book where you'll need to update your math vocabulary. The solution to an individual equation is a finite set of points. The solution to a system of equations can be an entire space containing infinitely many points, such as a line or a plane. 

The solution set of a system of three linear equations in three unknowns could be either the empty set $\{0\}$ (no solution), a set with one element $\{(x_1, x_2, x_3)\}$, or a set with infinitely many elements like a line $\{p_o + t \overrightarrow{v}, t \in \mathbb{R}\}$ or a plane $\{p_o + s \overrightarrow{v} + t \overrightarrow{w}, s,t \in \mathbb{R}\}$. Another possible solution set is all of $\mathbb{R}^3$; every vector $ \overrightarrow{x} \in \mathbb{R}^3 $ is a solution to the equation 

\$\$
\begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} \begin{bmatrix} x_1 \\\ x_2 \\\ x_3 \end{bmatrix} = \begin{bmatrix} 0 \\\ 0 \\\ 0 \end{bmatrix}
\$\$

Note the distinction between the three types of infinite solution sets. A line is one-dimensional, a plane is two-dimensional, and $\mathbb{R}^3$ is three-dimensional. Describing all points on a line requires one parameter, describing all points on a plane takes two parameters, and-of course-describing a point in $\mathbb{R}^3$ takes three parameters. 

&lt;h3&gt; Geometric Interpretation &lt;/h3&gt;
We can gain some intuition about solution sets by studying the geometry of the intersections of lines in $\mathbb{R}^2$ and planes in $\mathbb{R}^3$. 

&lt;h4&gt; Lines in two dimensions &lt;/h4&gt;
Equations of the form $ax+by = c$ corresponds to lines in $\mathbb{R}^2$. Solving systems of linear equations of the form

\$\$
\displaylines{
a_1 x + b_1 y = c_1 \\\
a_2 x + b_2 y = c_2
}
\$\$

requires finding the point $(x,y) \in \mathbb{R}^2$ where these lines intersect. There are three possibilities for the solution set:
- &lt;span class=&quot;highlight-sketch&quot;&gt; one solution &lt;/span&gt; if the two lines intersect at a point. 
- &lt;span class=&quot;highlight-sketch&quot;&gt; infinitely many solutions &lt;/span&gt; if the lines are superimposed.
- &lt;span class=&quot;highlight-sketch&quot;&gt; no solution &lt;/span&gt; if the two lines are parallel and never intersect.

&lt;h4&gt; Planes in three dimensions &lt;/h4&gt;
Equations of the form $ax+by+cz = d$ correspond to planes in $\mathbb{R}^3$. When solving three such equations, 

\$\$ 
\displaylines{
a_1 x + b_1 y + c_1 z = d_1 \\\
a_2 x + b_2 y + c_2 z = d_2 \\\
a_3 x + b_3 y + c_3 z = d_3
}
\$\$

we want to find a set of points $(x,y,z)$ that satisfy all three equations simultaneously. There are four possibilities for the solution set:

1. &lt;span class=&quot;highlight-green&quot;&gt; one solution &lt;/span&gt; three non-parallel planes intersect at a point. 
2. &lt;span class=&quot;highlight-yellow&quot;&gt; infinitely many solutions 1 &lt;/span&gt; if only one of the plane equations is redundant, the solution corresponds to the intersection of two planes which is a line.
3. &lt;span class=&quot;highlight-green&quot;&gt; infinitely many solutions 2 &lt;/span&gt; if two of the equations are redundant, then the solution space is a two-dimensional space. 
4. &lt;span class=&quot;highlight-yellow&quot;&gt; if two (or more) of the planes are parallel, they will never intersect. &lt;/span&gt;

&lt;h3&gt; Determinants &lt;/h3&gt;
&lt;h4&gt; Overview &lt;/h4&gt; 
What is the volume of a rectangular box of length $1m$, width $2$ and height $3m$? It's easy to compute the volume of this box because its shape is right rectangular prism. The volume of this prism is $V = l \times w \times h = 6m^3$. What if the shape of the box was a parallelpiped instead? A parallelpiped is a box whose opposite faces are parallel but whose sides are slanted. How do we compute the volume of a parallelpiped? The determinant operation, specifically the $3 \times 3$ determinant, is the perfect tool for this purpose. 

The determinant of a matrix, denoted $det(A)$ or $|A|$, is a particular way to multiply the entries of the matrix to produce a single number. We use determinants for all kinds of tasks: to compute areas and volumes, to solve systems of linear equations, to check whether a matrix is invertible or not, etc.

We can interpret the determinant of a matrix intuitively as a geometrical calculation. The &lt;span class=&quot;blue&quot;&gt; determinant &lt;/span&gt; is the &lt;span class=&quot;blue&quot;&gt; volume &lt;/span&gt; of the geometric shape whose edges are the rows of the matrix. For $2 \times 2$ matrices, the determinant corresponds to the area of a parallelogram. For $3 \times 3$ matices, the determinant corresponds to the volume of a parallelpiped. For dimensions $d &amp;gt; 3$, we say the determinant measures a &lt;span class=&quot;blue&quot;&gt; $d$-dimensional hyper-volume &lt;/span&gt;.

Consider the linear transformation $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined through the matrix-vector product with a matrix $A_T: T(\overrightarrow{x}) \equiv A_T \overrightarrow{x}$. The determinant of the matrix $A_T$ is the &lt;span class=&quot;blue&quot;&gt; scale factor &lt;/span&gt; associated with the linear transformation $T$. The scale factor of the linear transformation $T$ describes &lt;mark&gt; how the area of a unit square in the input space (a square with dimensions $1 \times 1$) is transformed by $T$ &lt;/mark&gt;. After passing through $T$, the unit square is transformed to a parallelogram with with area $det(A_T)$. Linear transformations that shrink areas have $det(A_T) &amp;lt; 1$, while linear transformations that enlarge areas have $det(A_T) &amp;gt; 1$. A linear transformation that is area preserving has $det(A+T) = 1$. 

The determinant is also used to check &lt;span class=&quot;blue&quot;&gt; linear independence &lt;/span&gt; for a given set of vectors. We &lt;span class=&quot;underline&quot;&gt; construct a matrix using the vectors as the matrix rows &lt;/span&gt;, and compute its determinant. 

The determinant of a matrix tells us whether or not that matrix is &lt;span class=&quot;blue&quot;&gt; invertible &lt;/span&gt;. If $det(A) \neq 0$, then $A$ is invertible; if $det(A) = 0$, $A$ is not invertible. 

The determinant shares a connection with the &lt;span class=&quot;blue&quot;&gt; vector cross product &lt;/span&gt;, and is used in the definition of the &lt;span class=&quot;blue&quot;&gt; eigenvalue equation &lt;/span&gt;.

&lt;h4&gt; Formulas &lt;/h4&gt;
The determinant of a $2 \times 2$ matrix is 

\$\$
det(\begin{bmatrix} a_{11} &amp;amp; a_{12} \\\ a_{21} &amp;amp; a_{22} \end{bmatrix}) = \begin{vmatrix} a_{11} &amp;amp; a_{12} \\\ a_{21} &amp;amp; a_{22} \end{vmatrix} = a_{11}a_{22} - a_{12}a{21}
\$\$

The formulas for the determinants of larger matrices are defined recursively. For example, the determinant of $3 \times 3$ matirx is defined in terms of $2 \times 2$ determinants:

\$\$ 
\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{vmatrix} = a_{11} 

= a_{11} \begin{vmatrix}a_{22} &amp;amp; a_{23} \\\ a_{32} &amp;amp; a_{33} \end{vmatrix} -a_{12} \begin{vmatrix} a_{21} &amp;amp; a_{23} \\\ a_{31} &amp;amp; a_{33} \end{vmatrix}+ a_{13} \begin{vmatrix} a_{21} &amp;amp; a_{22} \\\ a_{31} &amp;amp; a_{32} \end{vmatrix}
\$\$ 

There's a neat computational trick for computing $3 \times 3$ determinants by hand. The trick consists of extending the matrix $A$ into a $3 \times 5$ array that contains copies the columns of $A$: the $1^{st}$ column of $A$ is copied to the $4^{th}$ column of the extended array, and the $2^{nd}$ column of $A$ is copied to the $5^{th}$ column. The determinant is then computed by summing the products of the entries on the three positive diagonals and subtracting the products of the entries on the three negative diagonals. 

&lt;picture&gt;&lt;img src=&quot;/assets/images/3x3.png&quot; /&gt;&lt;/picture&gt;

The general formula for the determinant of an $n \times n$ matrix is

\$\$
det(A)  = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} M_{1j}
\$\$

where $M_{ij}$ is called the &lt;mark&gt; minor &lt;/mark&gt; associated with the entry $a_{ij}$. The minor $M_{ij}$ is the determinant of the submatrix obtained by removing the $i^{th}$ row and the $j^{th}$ column of the matrix $A$. Note the alternating factor $(-1)^{i+j}$ that changes value between $+1$ and $-1$ for different terms in the formula.

The determinant of a $4 \times 4$ matrix $B$ is 

\$\$ 
det(B) = b_{11}M_{11} - b_{12}M_{12} + b_{13}M_{13} - b_{14}M_{14}
\$\$

The general formula for determinants $det(A) = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} M_{1j}$ assumes we're expanding the determinant along the first row of the matrix. In fact, a determinant formula can be obtained by expanding the determinant along anyrow or column of the matrix.

The expand-along-any-row-or-column nature of determinants can be very handy: if you need to calculate the determinant of a matrix with one row (or column) containing many zero entries, it makes sense to expand along that row since many of the terms in the formula will be zero. If a matrix contains a row (or column) consisting entirely of zeros, we can immediately tell its determinant is zero. 

&lt;h4&gt; Geometric interpretation &lt;/h4&gt;
&lt;h5&gt; Area of a parallelogram &lt;/h5&gt;
Suppose we're given vectors $overrightarrow{v} = (v_1, v_2)$ and $\overrightarrow{w} = (w_1, w_2)$ in $\mathbb{R}^2$ and we construct a parallelogram with corner points $(0,0), \overrightarrow{v}, \overrightarrow{w}$ and $\overrightarrow{v} + \overrightarrow{w}$.

The area of this parallelogram is equal to the determinant of the matrix that contains $(v_1, v_2)$ and $(w_1, w_2)$ as rows:

\$\$
area = \begin{vmatrix} v_1 &amp;amp; v_2 \\\ w_1 &amp;amp; w_2 \end{vmatrix} = v_1 w_2 - v_2 w_1 
\$\$

&lt;h2&gt; Theoretical Linear Algebra &lt;/h2&gt;
The things we are going to learn now are less concerned with calculations and more about mind expansion. This section extends what we know about the vector space $\mathbb{R}^n$ to the realm of &lt;span class=&quot;blue&quot;&gt; abstract vector spaces &lt;/span&gt; of &lt;u&gt;vector-like mathematical objects. &lt;/u&gt;

&lt;h3&gt; Eigenvalues and Eigenvectors &lt;/h3&gt;
The set of &lt;u&gt; eignevectors &lt;/u&gt; of a matrix is a special set of input vectors for which the action of the matrix is described as &lt;u&gt; simple scaling &lt;/u&gt;. 

Decomposing a matrix in terms of its eigenvalues and its eigenvectors gives valuable insights into the properties of the matrix. Certain matrix calculations, like &lt;b&gt; computing the power of the matrix&lt;/b&gt;, become much easier when use the &lt;u&gt; eigendecomposition of the matrix &lt;/u&gt;

For example, suppose we're given a square matrix $A$ and want to compute $A^7$. To make this example more concrete, we'll analyze the matrix 
\$\$
A = \begin{bmatrix} 9 &amp;amp; -2 \\\ -2 &amp;amp; 6 \end{bmatrix}
\$\$

We want to compute $A^7$. That would be an awful lot of matrix multiplications. Every matrix corresponds to some linear operation. This means it's legit to ask, &lt;u&gt; &quot;what does the matrix A do?&quot; &lt;/u&gt; Once we figure out this part, we can compute $A^{77}$ by simply doing what $A$ does $77$ times. 

The best way to see what a matrix does is to look inside it and see what it's made of. To understand the matrix $A$, you must find it its eigenvectors and its eigenvalues. The eigenvectors of a matrix are a &lt;span class=&quot;rainbow&quot;&gt; natural basis &lt;/span&gt; for describing the action ofthe matrix. The eigendecomposition is a change-of-basis operation that expresses the matrix $A$ with respect to its eigenbasis (own-basis). The eigendecompositions of the matrix $A$ is a product of three matrices:






&lt;/li&gt;&lt;/ul&gt;</content><author><name>seri</name></author><category term="linear algebra" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/linear.png" /><media:content medium="image" url="http://localhost:4000/assets/images/linear.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Batch Normalization and Group Normalization</title><link href="http://localhost:4000/group-norm.html" rel="alternate" type="text/html" title="Batch Normalization and Group Normalization" /><published>2021-09-08T00:00:00+09:00</published><updated>2021-09-08T00:00:00+09:00</updated><id>http://localhost:4000/group-norm</id><content type="html" xml:base="http://localhost:4000/group-norm.html">&lt;!--more--&gt;

&lt;h2&gt; Batch Normalization: the Principles &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/bn2.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Batch Normalization is an algorithmic method which makes the training of Deep Neural Networks &lt;span class=&quot;blue&quot;&gt; faster and more stable &lt;/span&gt;. Batch normalization is computed differently during the training and the testing phase.&lt;/p&gt;

&lt;p&gt;At &lt;span class=&quot;circle-sketch-highlight&quot;&gt; training &lt;/span&gt;, the BN layer determines the mean and standard deviation of the activation values across the batch. It then &lt;span class=&quot;underline&quot;&gt; normalizes the activation vector &lt;/span&gt; with $\mu$ and $\sigma$. That way, each neuron’s output follows a standard &lt;span class=&quot;blue&quot;&gt; normal distribution &lt;/span&gt; across the &lt;span class=&quot;blue&quot;&gt; batch &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;It finally applies a &lt;span class=&quot;blue&quot;&gt; linear transformation &lt;/span&gt; with $\gamma$ and $\beta$ which are the two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trainable parameters&lt;/code&gt;. Such step allows the model to choose the optimum distribution for each hidden layer. $\gamma$ allows to adjust the standard deviation while $\beta$ allows to adjust the bias, shifting the curve on the right or on the left side.&lt;/p&gt;

&lt;p&gt;At each iteration, the network computes the mean $\mu$ and the standard deviation $\sigma$ corresponding to the current batch. Then it trains $\gamma$ and $\beta$ through &lt;mark&gt; gradient descent &lt;/mark&gt; using an &lt;mark&gt; Exponential Moving Average (EMA) &lt;/mark&gt; to give more importance to the latest iterations.&lt;/p&gt;

&lt;div class=&quot;sidenote&quot;&gt; We mostly use &lt;mark&gt; Exponential Moving Average algorithm &lt;/mark&gt; to reduce the noise or to smooth the data. The weight of each element decreases progressively over time, meaning &lt;span class=&quot;underline&quot;&gt; the EMA gives greater weight to recent data points &lt;/span&gt;. EMA reacts faster to changes compared to Simple Moving Average. &lt;/div&gt;

&lt;p&gt;At the &lt;span class=&quot;rainbow&quot;&gt; evaluation phase &lt;/span&gt;, we may not have a full batch to feed into the model. To tackle this issue, &lt;span class=&quot;underline&quot;&gt; we compute $\mu_{pop}$ and $\sigma_{pop}$ as the estimated mean and standard deviation of the studied population &lt;/span&gt;. Those values are computed using all the $\mu_{batch}$ and $\sigma_{batch}$ during training, and directly fed during the evaluation phase.&lt;/p&gt;

&lt;h2&gt; Why Normalization? &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/bn4.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;What we can conclude from the original Batch Normalization paper is that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Adding BN layers leads to &lt;b&gt; better convergence and higher accuracy &lt;/b&gt;&lt;/li&gt;
  &lt;li&gt;Adding BN layers allows us to &lt;b&gt; use higher learning rate &lt;/b&gt; without compromising convergence.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To quote Ian Goodfellow about the use of batch normalization:&lt;/p&gt;
&lt;blockquote&gt; Before BN, we thought it was almost &lt;span class=&quot;underline&quot;&gt; impossible to efficiently train deep models using sigmoid &lt;/span&gt; in the hidden layers. Batch Normalization makes those unstable networks trainable. &lt;/blockquote&gt;

&lt;p&gt;In practice, it is widely admitted that:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt; For &lt;span class=&quot;highlight-green&quot;&gt; CNNs, Batch Normalization is better &lt;/span&gt; &lt;/li&gt;&lt;li&gt; &lt;span class=&quot;highlight-yellow&quot;&gt; For Recurrent Networks, Layer Normalization is better &lt;/span&gt; &lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;While BN uses the current batch to normalize every single value, LN uses all the current layer to do so. &lt;span class=&quot;underline&quot;&gt; The normalization is performed using other features from a single example &lt;/span&gt; instead of using the same feature across all current batch examples.&lt;/p&gt;

&lt;p&gt;The best way to understand why BN works is to understand &lt;span class=&quot;highlight-sketch&quot;&gt; the optimization landscape smoothness &lt;/span&gt;. BN &lt;u&gt; reparameterizes the underlying optimization problem &lt;/u&gt;, making the training faster and easier. In additional recent studies, researchers observed that this effect is not unique to BN, but applies to other normalization methods (i.e. L1 normalization or L2 normalization).&lt;/p&gt;

&lt;h2&gt; The Drawbacks of BN &lt;/h2&gt;

&lt;p&gt;For BN to work, the &lt;span class=&quot;rainbow&quot;&gt; batch size is required to be sufficiently large &lt;/span&gt;, usually at least &lt;span class=&quot;circle-sketch-highlight&quot;&gt; 32 &lt;/span&gt;. However, there are situations when we have to settle for a small batch size. For example, when each &lt;span class=&quot;underline&quot;&gt; data sample is highly memory consuming &lt;/span&gt; or when we train a &lt;span class=&quot;underline&quot;&gt; very large neural network &lt;/span&gt; which leaves little GPU memory for processing data. For computer vision applications other than image classification, the restriction on batch sizes are more demanding and it is difficult to have higher batch sizes.&lt;/p&gt;

&lt;h2&gt; Comparisions of Normalization Methods &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/bn3.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Group Normalization is one of the latest normalization methods that &lt;u&gt; avoids exploiting the batch dimension&lt;/u&gt;, thus is &lt;u&gt;independent of batch size&lt;/u&gt;. But there are other normalization methods as well.&lt;/p&gt;

&lt;h3&gt; Layer Normalization &lt;/h3&gt;

&lt;p&gt;Layer Normalization computes $\mu_i$ and $\sigma_i$ along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(C,H,W)&lt;/code&gt; axes. The computation for an input feature is entirely independent of other input features in a batch.&lt;/p&gt;

&lt;h3&gt; Instance Normalization &lt;/h3&gt;

&lt;p&gt;Instance Normalization computes $\mu_i$ and $\sigma_i$ along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(H,W)&lt;/code&gt; axes. Since the computation of IN is the same as that of BN with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size=1&lt;/code&gt;, IN actually makes the situation worse in most cases. However, for &lt;span class=&quot;highlight-pink&quot;&gt; style transfer tasks &lt;/span&gt;, IN is better at discarding contrast information of an image, thus having superior performance than BN.&lt;/p&gt;

&lt;h3&gt; Group Normalization &lt;/h3&gt;

&lt;p&gt;Also notice that IN can be viewed as applying Layer Normalization to each channel individually as if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_channels = 1&lt;/code&gt;. Group Normalization is the middle ground between IN and LN. It &lt;span class=&quot;gif&quot;&gt; organizes the channels into different groups &lt;/span&gt; and &lt;span class=&quot;highlight-yellow&quot;&gt; computes $\mu_i$ and $\sigma_i$ along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(H,W)&lt;/code&gt; axes and along a group of channels.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;First, the batch with dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(N,C,H,W)&lt;/code&gt; is reshaped to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(N,G,C//G,H,W)&lt;/code&gt;. The number of group &lt;span class=&quot;underline&quot;&gt; $G$ is a pre-defined hyperparameter &lt;/span&gt;. Then we normalize along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(C//G,H,W)&lt;/code&gt; dimension and return the result after reshaping the batch back to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(N,C,H,w)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Group Normalization is better than Layer Normalization as GN allows different distribution to be learned for each group of channels. GN is also thought to be better than IN because GN can exploit the dependence across channels. If &lt;mark&gt;`C = G`&lt;/mark&gt;, that is, if the number of groups are set to be equal to the number of channels, &lt;mark&gt; GN becomes IN &lt;/mark&gt; . Likewise, if &lt;mark&gt;`G = 1`&lt;/mark&gt; &lt;mark&gt; GN becomes LN &lt;/mark&gt;.&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://amaarora.github.io/2020/08/09/groupnorm.html&quot;&gt; blog post &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/what-is-group-normalization-45fe27307be7&quot;&gt; medium article &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338&quot;&gt; medium article2 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="deep learning" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/bn.png" /><media:content medium="image" url="http://localhost:4000/assets/images/bn.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dissecting the Camera Matrix (Part 1)</title><link href="http://localhost:4000/camera.html" rel="alternate" type="text/html" title="Dissecting the Camera Matrix (Part 1)" /><published>2021-09-05T00:00:00+09:00</published><updated>2021-09-05T00:00:00+09:00</updated><id>http://localhost:4000/camera</id><content type="html" xml:base="http://localhost:4000/camera.html">&lt;!--This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices, and and I will try to untangle the issues that can crop-up with different coordinate conventions.--&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;h2&gt; Camera Calibration and Decomposition &lt;/h2&gt;
&lt;p&gt;Primarily, camera calibration is about finding the quantities internal to the camera that affect the imaging process. Here are some of the factors that will be taken care of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; &lt;span class=&quot;blue&quot;&gt; image center &lt;/span&gt;: we need to find the position of the image center in the image. Wait, isn't the image center located at $(width/2, height/2)$? Well, no. Unless we calibrate the camera, the image will almost always appear to be off-center.&lt;/li&gt;
&lt;li&gt; &lt;span class=&quot;blue&quot;&gt; focal length &lt;/span&gt;: this is a very important parameter. Remember how people using DSLR cameras tend to &lt;mark&gt; focus &lt;/mark&gt; on things before capturing the image? this parameter is directly related to the &lt;mark&gt; focus &lt;/mark&gt; of the camera and it is very critical.&lt;/li&gt;
&lt;li&gt; &lt;span class=&quot;blue&quot;&gt; scaling factors &lt;/span&gt;: the scaling factors for row pixels and column pixels might be different. If we don't take care of this thing, the image will look stretched (either horizontally or vertically).&lt;/li&gt;
&lt;li&gt; &lt;span class=&quot;blue&quot;&gt; skew factor &lt;/span&gt;: this refers to shearing. the image will look like a parallelogram otherwise.&lt;/li&gt;
&lt;li&gt; &lt;span class=&quot;blue&quot;&gt; lens distortion &lt;/span&gt;: this refers to the pseudo zoom effect that we see near the center of any image. &lt;/li&gt; 

&lt;small class=&quot;sidenote&quot;&gt; &lt;span class=&quot;highlight-sketch&quot;&gt;shearing &lt;/span&gt; refers to a transformation in which all points along a given line $L$ remain fixed while other points are shifted parallel to $L$ by a distance proportional to their perpendicular distance from $L$. &lt;/small&gt;
&lt;br /&gt;

&lt;h2&gt; Pinhole Camera Model &lt;/h2&gt;
&lt;picture&gt;&lt;img src=&quot;/assets/images/pinhole.png&quot; /&gt;&lt;/picture&gt;

Before we jump into anything, let's see where this all began. When we capture an image, we are basically mapping the 3D scene to a 2D scene. It means that every point in the 3D world gets mapped to the 2D plane of our image. This is called the &lt;span class=&quot;red&quot;&gt; pinhole camera model &lt;/span&gt;. It basically describes the relationship between the coordinates of the 3D point and its projection on the 2D image. This, of course, is the ideal case where &lt;span class=&quot;highlight-yellow&quot;&gt; there is absolutely no distortion &lt;/span&gt; of any kind. Every camera is modeled based on this, and every camera aspires to simulate this as close as possible. But in the real world, we have to deal with things like geometric distortions, blurring, finite sized apertures, etc. 

&lt;picture&gt;&lt;img src=&quot;/assets/images/pinhole2.png&quot; /&gt;&lt;/picture&gt;

The figure shown here depicts a pinhole camera model. The camera is placed at the origin $O$. The point $P$ represents a point in the real world. We are trying to capture that onto a 2D plane. The &lt;span class=&quot;rainbow&quot;&gt; image plane &lt;/span&gt; represents the 2D plane that you get after capturing the image. The image plane actually contains the image that you see after capturing a picture. So basically, we are trying to map every 3D point to a point on the image plane. In this case, the point $P$ gets mapped to $P_c$. The distance between the origin $O$ and this image plane is called the &lt;span class=&quot;highlight-sketch&quot;&gt; focal length &lt;/span&gt; of the camera. This is the parameter you modify when you adjst the &lt;mark&gt; focus &lt;/mark&gt; of the camera. 

&lt;h2&gt; Intrinsic and Extrinsic Parameters &lt;/h2&gt; 
In the above figure, we want to estimate $(u,v)$ from $(X,Y,Z)$. Let's say the focal length is denoted by $f$. If you look at the triangle formed using the origin-$P_c$-and the $Z$-axis with the origin-$P$ and $Z$-axis, you will notice that they are similar triangles. This means that $u$ depends on the $f$, $X$ and $Z$. Similarly, $v$ depends on $f$, $Y$ and $Z$. 

\$\$

\displaylines{
u = fX/Z \\\
v = fY/Z
}

\$\$

Next, if the origin of the 2D image coordinate system does not coincide with where the $Z$-axis intersects the image plane, we need to translate $P_c$ into the desired origin. Let this translation be defined by $(t_u, t_v)$. So now, $u$ and $v$ are given by:

\$\$
\displaylines{
u = fX/Z + t_u \\\
v = fY/Z + t_v
}
\$\$

So up until now, we have something that can translate $(X,Y,Z)$ to $(u,v)$. Let's denote this matrix $M$. So we can write:

\$\$ 
P_c = MP
\$\$


Since this is a camera image, we need to express it in inches. For this, we will need to know the resolution of the camera in pixels/inch. If the pixels are square the resolution will be identical in both $u$ and $v$ directions of the camera image coordinates. However, for a more general case, we assume rectangular pixels with resolution $m_u$ and $m_v$ pixels/inch in $u$ and $v$ directions respectively. Therefore, to measure $P_c$ in pixels, its $u$ and $v$ coordinates should be multiplied by $m_u$ and $m_v$ respectively. So now, this new transformation matrix depends on $f, X, Y, Z, t_u, t_v, m_u, m_v$. Let's denote this by:

\$\$
P_c = KP
\$\$

Here, $K$ is called the &lt;span class=&quot;typewriter&quot;&gt; intrinsic parameter matrix &lt;/span&gt; for the camera.

Now, if the camera does not have its center of projection at $(0,0,0)$ and is oriented in an arbitrary fashion (not necessarily $z$-perpendicular to the image plane), then we need roation and translation to make the camera coordinate system coincide with the configuration in that pinhole camera figure. Let the rotation applied to coincide the principal axis with $Z$-axis given by a $3 \times 3$ rotation matrix $R$. Then the matrix is formed by first applying the translation followed by the rotation is given by the $3 \times 4$ matrix.

\$\$
E = \\( R|RT \\)
\$\$

This is called the &lt;span class=&quot;typewriter&quot;&gt; extrinsic parameter matrix for the camera &lt;/span&gt;. So, the complete camera transformation can now be represented as: 

\$\$ 
K \\( R|RT \\) = KR \\( I|T \\)
\$\$

Hence, $P_c$ the projection of $P$ is given by:

\$\$
P\_c = KR \\( I|T \\) P = CP
\$\$

$C$ is a $3 \times 4$ matrix usually called the complete camera calibration matrix. So basically, camera calibration matrix is used to transform a 3D point in the real world to a 2D point on the image plane considering all the things like focal length of the camera, distortion, resolution, shifting of origin, etc. 

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://ksimek.github.io/2012/08/14/decompose/&quot;&gt; ksimek blog &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://prateekvjoshi.com/2014/05/31/understanding-camera-calibration/&quot;&gt; prateekvjoshi blog &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/camera2.png" /><media:content medium="image" url="http://localhost:4000/assets/images/camera2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Depth from Disparity</title><link href="http://localhost:4000/disparity.html" rel="alternate" type="text/html" title="Depth from Disparity" /><published>2021-09-05T00:00:00+09:00</published><updated>2021-09-05T00:00:00+09:00</updated><id>http://localhost:4000/disparity</id><content type="html" xml:base="http://localhost:4000/disparity.html">&lt;!--We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.--&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2 class=&quot;glow&quot;&gt;🌟 3D Reconstruction from 2D Signals &lt;/h2&gt;&lt;/div&gt;

&lt;p&gt;We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology.&lt;/p&gt;

&lt;p&gt;How can we automatically &lt;span class=&quot;highlight-sketch&quot;&gt; compute 3D geometry from images &lt;/span&gt;? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics.&lt;/p&gt;

&lt;p&gt;Well, we humans do so naturally. Here are several cues we use to infer depth information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; shading &lt;/li&gt;
&lt;li&gt; texture &lt;/li&gt; 
&lt;li&gt; focus &lt;/li&gt;
&lt;li&gt; motion &lt;/li&gt; 
&lt;li&gt; perspective &lt;/li&gt;
&lt;li&gt; occlusion &lt;/li&gt; 
&lt;li&gt; symmetry &lt;/li&gt;
&lt;/ul&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/single.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i.e., camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.&lt;/p&gt;

&lt;h2&gt; The Stereo Problem &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/stereo.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images.&lt;/p&gt;

&lt;p&gt;The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences.&lt;/p&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/epipolar.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Furthermore,  &lt;span class=&quot;rainbow&quot;&gt; the Epipolar Constraint &lt;/span&gt; reduces the correspondence problem to $1D$ search along &lt;span class=&quot;frozen&quot;&gt; conjugate epipolar lines &lt;/span&gt; shown in the above figure.&lt;/p&gt;

&lt;p&gt;Thus, &lt;span class=&quot;rainbow&quot;&gt; Epipolar Constraint &lt;/span&gt; assumes that stereo pairs are rectified images, meaning the same &lt;span class=&quot;frozen&quot;&gt; epipolar line &lt;/span&gt; aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades.&lt;/p&gt;

&lt;p&gt;From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as:&lt;/p&gt;

&lt;p&gt;$$
I(x,y) = D(x+d, y)
$$&lt;/p&gt;

&lt;p&gt;This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair.&lt;/p&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/disparity.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore,&lt;/p&gt;

&lt;p&gt;$$
\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}
$$&lt;/p&gt;

&lt;p&gt;and the world coordinate can be expressed as&lt;/p&gt;

&lt;p&gt;$$
X = \frac{b(x_L+x_R)}{2(x_L - x_R)} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}
$$&lt;/p&gt;

&lt;blockquote class=&quot;black&quot;&gt; $d = x_L - x_R$ is the &lt;span class=&quot;neon-green&quot;&gt;disparity&lt;/span&gt; between corresponding left and right image points &lt;/blockquote&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/depth-from-disparity-via-deep-learning-part-0-458827141b23&quot;&gt; article from medium &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/depth/12.png" /><media:content medium="image" url="http://localhost:4000/assets/images/depth/12.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Depth Estimation: Basics and Intuition</title><link href="http://localhost:4000/depth.html" rel="alternate" type="text/html" title="Depth Estimation: Basics and Intuition" /><published>2021-09-02T00:00:00+09:00</published><updated>2021-09-02T00:00:00+09:00</updated><id>http://localhost:4000/depth</id><content type="html" xml:base="http://localhost:4000/depth.html">&lt;!--more--&gt;

&lt;h2&gt; Depth Estimation in Computer Vision &lt;/h2&gt;

&lt;p&gt;Depth estimation is also known as &lt;span class=&quot;blue&quot;&gt; the inverse problem &lt;/span&gt;, where we seek to recover unknowns given insufficient information.&lt;/p&gt;

&lt;p&gt;So how do machines actually perceive depth? The earliest algorithm with impressive results began with depth estimation using stereo vision back in the 90s. A lot of progress was made on &lt;span class=&quot;highlight-green&quot;&gt; dense stereo correspondence algorithm &lt;/span&gt; Researchers were able to &lt;u&gt; utilize geometry &lt;/u&gt; to constrain and replicate the idea of &lt;b&gt; stereopsis &lt;/b&gt; mathematically.&lt;/p&gt;

&lt;p&gt;As for monocular depth estimation, it recently started to gain popularity by using neural networks. We will talk about this shortly after discussing stereo vision.&lt;/p&gt;

&lt;h3&gt; Depth Estimation from Stereo Vision &lt;/h3&gt;

&lt;picture&gt;&lt;img src=&quot;/asset/images/rect.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;The main idea of &lt;u&gt; solving depth using a stereo camera &lt;/u&gt; involves the concept of &lt;span class=&quot;rainbow&quot;&gt; triangulation and streo matching &lt;/span&gt;. The former depends on &lt;b&gt; good calibration and rectification &lt;/b&gt; to constrain the problem so that it can be modelled on a 2D plane known as &lt;span class=&quot;circle-sketch-highlight&quot;&gt; epipolar plane &lt;/span&gt;. Epipolar plane greatly reduces the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stereo matching problem&lt;/code&gt; to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;line search&lt;/code&gt; along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;epipolar line&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Once we are able to &lt;span class=&quot;gif&quot;&gt; match pixel correspondences &lt;/span&gt; between 2 views, the next task is to &lt;span class=&quot;shine&quot;&gt; obtain a representation that encodes the difference &lt;/span&gt;. &lt;u&gt; This representation is known as &lt;span class=&quot;frozen&quot;&gt; disparity &lt;/span&gt;&lt;/u&gt;. The formula to obtain depth from disparity can be worked out from similar triangles.&lt;/p&gt;

&lt;h2&gt; Why is Measuring Depth So Difficult? &lt;/h2&gt;

&lt;p&gt;Before we move on, let’s try to understand some fundamental problems of depth estimation. The main culprit lies &lt;u&gt; in the projection of 3D views to 2D images where depth information is lost &lt;/u&gt;. Another problem is deeply seeded when &lt;u&gt; there are motion and moving objects &lt;/u&gt;.&lt;/p&gt;

&lt;h3&gt; Depth Estimation is Ill-Posed &lt;/h3&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/ill.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Estimating depth from a single RGB image is an ill-posed inverse problem. What this means is that &lt;span class=&quot;highlight-yellow&quot;&gt; many 3D scenes observed in the 3D world can indeed correspond to the same 2D image plane.&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt; Scale Ambiguity for Monocular Depth Estimation &lt;/h3&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/scale.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;The uncertain scale factor require current methods to have an additional sensor like LiDAR to provide depth ground-truth or stereo camera as additional training input, which makes them difficult to implement.&lt;/p&gt;

&lt;p&gt;Note that this issue exists only for &lt;u&gt; monocular-based techniques &lt;/u&gt;, as the scale can be recovered for a stereo rig with a known baseline.&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/depth-estimation-1-basics-and-intuition-86f2c9538cd1&quot;&gt; medium article&lt;/a&gt; &lt;/li&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/self-supervised-depth-estimation-breaking-down-the-ideas-f212e4f05ffa&quot;&gt; medium article 2 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/inverse-projection-transformation-c866ccedef1c&quot;&gt; medium article 3 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ras.papercept.net/images/temp/IROS/files/0898.pdf&quot;&gt; paper about absolute depth &lt;/a&gt;&lt;/li&gt;
&lt;ul&gt; 


&lt;/ul&gt;&lt;/ul&gt;&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">This article focuses on giving readers a background into depth estimation and problem associated with it. We cover both methodolgies used to extract the depth information, namely 'depth from monocular images' (static or sequential) and 'depth from stereo images' by exploiting epipolar geometry.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/depth/1.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/depth/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>