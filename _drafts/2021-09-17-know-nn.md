---
layout: post
title:  "Neural Networks: Back to the Basics"
author: seri
categories: [ deep learning ]
image: assets/images/neuron.png
tags: featured
---

<!--more-->
<h2> Introduction </h2>
<h2> Perceptron </h2>
What is a neural network? To get started, I'll explain a type of artificial neuron called a <span class="blue"> perceptron </span>. Today, it's more common to use other models of artificial neurons. The main neural model used is called the <span class="red"> sigmoid neuron </span>. We will get to sigmoid neurons shortly. But to understand why sigmoid neurons are defined the way they are, it's worth taking the time to first understand perceptrons. 

So who do perceptrons work? A perceptron takes several binary inputs $x_1, x_2, \dots $ and produces a single binary output:

<picture><img src="{{site.baseurl}}/assets/svg/perceptron.svg" class="img-small-center"></picture>

In the example shown, the perceptron has three inputs $x_1, x_2, x_3$. In general, it could have more or fewer inputs. Rosenblatt, the inventor of perceptron, introduced <span class="highlight"> weights </span>, $w_1, w_2, \dots$, real numbers expressing the importance of the respective inputs to the outputs. 

The neuron's output, 0 or 1, is determined by whether the weighted sum $\sum_{j}w_j x_j$ is less or greater than some <span class="shadow-blue"> threshold value </span>. Just like the weights, the threshold is a real number which is a parameter of the neuron. 

To put it in more precise algebraic terms:

$$
\text{output} = \begin{cases} 0 & \text{if $\sum_j w_j x_j \leq threshold$} \\ 1 & \text{if $\sum_j w_j x_j > threshold$}\end{cases}
$$

That's all there is to how a perceptron works!

<div class="textbox"> A way you can think about the perceptron is that it's a device that <u> makes decisions </u> by <u> weighing up evidence </u>. </div>

By varying the weights and the threshold, we can get different models of decision-making. 

<small> Obviously, the perceptron isn't a complete model of human decision-making. </small> But what the example illustrates is <u> how a perceptron can weigh up different kinds of evidence </u> in order to make decisions. 

So it should seem plausible that <span class="highlight-yellow"> a complex network of perceptrons could make quite subtle decisions </span>.

<picture><img class="img-small-center" src="{{site.baseurl}}/assets/images/mlp.png"> </picture>

In this network, the first layer of perceptrons is making three very simple decisions by weighing the input evidence. The perceptrons in the second layer is making a decision by <u> weighing up the results from the first layer of decision-making </u>. In this way, a perceptron in the second layer <span highlight="highlight-skew"> can make a decision at a more complex and abstract level </span> than perceptrons in the first layer. 

Even more complex decisions can be made by the perceptron in the third layer.  In this way, <span class="underline-pink"> many layers of perceptrons can engage in sophisticated decision making. </span>.

Let's simplify the way we describe perceptrons. The condition $\sum_j w_j x_j > \text{threshold}$ is cumbersome, so we can make a change to simplify it. We will move the threshold to the other side of the inequality, and replace it by what is known as the perceptron's <span class="highlight-sketch"> bias </span>, $ b \equiv - \text{threshold} $. Using this bias instead of the threshold, the perceptron rule can be written as:

$$
\text{output} = \begin{cases} 0 & \text{if $w \cdot x + b \leq 0$} \\ 1 & \text{ if $w \cdot x + b > 0$} \end{cases}
$$

You can think of the bias as <span class="underline-move"> as a measure of how easy it is to get the perceptron to output a $1$ </span>. 

For a perceptron with <u> a really big bias </u>, it's extremely easy for the perceptron to <b> output a $1$ </b>. But if the bias is negative, it's difficult for the perceptron to output a $1$. 

I've described perceptrons as <span class="highlight-gradient"> a method for weighing evidence to make decisions </span>. Another way perceptrons can be used is <span class="underline"> to compute the elementary logcal functions </span> we usually think of as underlying computation, functions such as `AND`,`OR`, and `NAND`. 

<picture><img class="img-small-center" src="{{site.baseurl}}/assets/svg/example.svg"></picture>

For example, suppose we have a perceptron with weight $-2$, and an overall bias of $3$. We can see that input $00$ produces the value $3$, so the output is $1$. 

Similar calculations show that $01$ or $10$ produces output $1$. But the input $11$ produces the value -1, hence the output $0$. So our perceptron is a `NAND` gate!

In fact, we can <span class="neon-green"> use networks of perceptrons to compute any logical function at all </span>. 

<div class="textbox"> The reason is that the `NAND` gate is universal for computation. We can build any computation up out of `NAND` gates. </div>

Becaue `NAND` gates are universal for computation, perceptrons are also <span class="gold"> universal for computation </span>. 

It turns out that we can device <span class="neon-pink"> learning algorithms </span> which can <u> automatically tune the weights and biases of a network </u> of artificial neurons. 

<small> This tuning happens in responsce to external stimuli, without direct intervention by a programmer. These learning algorithms enable us to use artifical neurons in a way which is radically different to conventional logic gates. </small>
 
 <h2> Sigmoid Neurons </h2>

 How can we device such <span class="neon-pink"> learning algorithms </span> for a neural network? 

Well, what we'd like to see is for a small change in weight (or bias) to cause only a small corresponding change in the output from the network. This property makes learning possible. 

$$ 
w + \nabla w \rightarrow \text{output} + \nabla \text{output}
$$

The problem is that a small change in weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say 0 to 1.

We can overcome this problem by introducing a new type of artificial neuron called a <span class="shine"> sigmoid neuron </span>. Sigmoid neurons are similar to perceptrons, but modified so that <u> small changes in their weights and biases casue only a small change in their output. 

We can depict the sigmoid neurons in the same way we depicted perceptrons. Just like a perceptron, the sigmoid neuron has inputs $x_1, x_2, \dots$, but instead of being just $0$ or $1$, these inputs can take on any values between $0$ and $1$. 

<picture><img src="{{site.baseurl}}/assets/images/sigmoid.png"></picture>

Just like a perceptron, the sigmoid neuron has weights for each input $w_1, w_2, \dots$ and an overall bias $b$. But the output is not $0$ or $1$ but instead $\sigma(w \cdot x + b)$, where $\sigma$ is called the <span class="flow"> sigmoid function </span> which is defined by:

$$ 
\sigma(z) \equiv \frac{1}{1+e^{-z}}
$$

To put it more explicitly,

$$ 
\frac{1}{1+ \exp(-\sum_j w_j x_j - b)}
$$

To understand the similarity to the perceptron model, suppose $z \equiv w \cdot x + b$ is a large positive number. Then $e^{-z} \approx 0$ and so $\sigma(z) \approx 1$. 

In other words, neuron is approximately $1$, if $z = w \cdot x + b$ is large and positive.  If $z = w \cdot + b$ is negative, $e^{-z} \rightarrow \infty$ and $\sigma(z) \approx 0$. 

<div class="textbox" style="text-align: center !important;"> <picture><img src="{{site.baseurl}}/assets/images/euler.png" style="width: 350px !important;"></picture> <p style="font-size: 70% !important; text-align: left !important;"> Remember our exponential function with Euler's number? $e = \lim_{n\to\infty} (1+1/n)^n = 1+ \frac{1}{1!}+ \frac{1}{2!} + \frac{1}{3!}+ \cdots \approx 2.718$. The gradient function of $y = e^x$ is equal to $e^x$. It is a natural choice as a logarithmic base.</p> </div>

The exact algebraic form of $\sigma$ isn't so important-what really matters is the shape of the function when plotted as in the above figure. 

<div class="textbox" style="text-align: center !important;"><picture><img src="{{site.baseurl}}/assets/images/sigmoid.png" style="width:500px !important;"></picture><p style="font-size: 90% !important; text-align: left !important;">The left figure also show the graph of the derivative of sigmoid function in pink color. The sigmoid function is also called a squashing function as its domain is the set of all real numbers ranging $(0,1)$. When the activation function for a neuron is a sigmoid function, it is guaranteed that the output unit will always be  between 0 and 1.</p> </div> 

<div class="textbox"><p><b> Why Is the Sigmoid Function Important in Neural Networks?</b> The sigmoid is a non-linear function, thus the ouput of this unit would be a non-linear function of the weighted sum of inputs. For a linearly separable problem in an $n$-dimensional space, the linear decision boundary is described by the equation of a hyperplane. The right figure shows a non-linearly separable problem, where a non-linear decision boundary is required. </p><p></p></div>

If we use a linear activation function in a neural network, then this model can only leran linearly separable problems. The addition of one hidden layer and a sigmoid activation function in the hidden layer, the neural network can eaily learn a non-linearly separable problem. 

<picture><img src="{{site.baseurl}}/assets/images/sigmoid-neuron.png"></picture>

<span class="underline-grad"> The only non-linear function that can be used as an activation function is one which is monotonically increasing. </span> The function also requires to be <span class="shine"> continuous </span> and <span class="reveal"> differentiable </span> over the entire space of real numbers. 

The fact that sigmoid function is monotonic, continuous and differentiable everywhere, coupled with the property that its derivatives can be expressed in terms of itself, makes the learning process easier when using the back-propagation algorithm. 

Indeed, its the smoothness of the $\sigma$ function that is the crucial fact. The smoothness of $\sigma$ means that small changes $\nabla w_j$ in the weights and $\nabla b$ in the bias will produce a small change of $\nabla \text{output}$ from the neuron.

<h2> The architecture of Neural Networks </h2>

<picture><img src="{{site.baseurl}}/assets/images/nn4.png"></picture>

The following four-layer network has two hidden layers. Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called <span class="rainbow"> multi-layer perceptrons </span> or <b> MLPs </b> despite being made up of sigmoid neurons, not perceptrons. 

While the design of the input and output layers of a neural network is often straighforward, there can be quite an art to <u> the design of the hidden layers </u>. Researchers have developed many design heuristics for the hidden layers. Up to now, we've discussing neural networks where <span class="underline-fancy"> the output from one layer is used as input to the next layer </span>. Such netwokrs are called <span class="blue"> feedforward neural networks</span>. This means that <span class="underline-fancy"> there are no loops in the network </span>-<span class="underline-move"> information is always fed forward, never fed back </span>. 

<small> There are other models of artificial neural networks in which feedback loops are possible. These models are called <mark class="gold"> recurrent neural networks </mark>. 
<h2> A Simple Network for Digit Recognition </h2>

<picture><img src="{{site.baseurl}}/assets/images/digit.png"></picture>

We are going to use a three-layer neural network to recognize individual digits. 

The input layer of the network contains neurons encoding the values of the input pixels. As discussed in the next section, our training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains $784 = 28 \times 28$ neurons. For simplicity I've omitted most of the 784 input neurons in the diagram above. The input pixels are grayscale, with a value of $0.0$ representing white, a value of $1.0$ representing black, and in between values representing gradually darkening shades of gray.

The second layer of the network is a hidden layer. We denote the number of neurons in the hidden layer by $n$, and we'll experiment with different values for $n$. The example above illustrates a small hidden layer, containing just $n=15$ neurons. 

The output layer of the network contains $10$ neurons. If the first neuron fires, i.e. has an $\text{output} \approx 1$, then that will indicate that the network thinks the digit is a $0$. We number the output neurons from $0$ through $9$ and figure out which neuron has the highest activation value. 

You might wonder why we use $10$ output neurons. A seemingly natural way of doing that is to use just $4$ output neurons, treating each neuron as taking on a binary value. Why should our network use $10$ neurons instead? The ultimate justification is empirical: we can try out both network designs, and it turns out that, for this particular problem, <span class="highlight-green"> the network with 10 output neurons learns to recognize digits better </span> than the network with $4$ output neurons. 

We can give a plausible explanation for why it's better to have $10$ outputs from the network, rather than $4$ because if we had 4 outputs, the first output neuron would be trying to decide what the most significant bit of the digit was. And there's no easy way to relate that most significant bit to simple shapes.  

<h2> Learning with Gradient Descent </h2>

The first thing we'll need is a dataset to learn from. The MNIST data, which we are going to use, comes in two parts. The first part contains 60,000 images to be used as training data. The images are grayscale and $28 \times 28$ images. 

We'll use the test data to evaluate how well our neural network has learned to recognize digits. 

We'll use the notation $x$ to denote a training input. It'll be convenient to regard each training input $x$ as a $28 \times 28 = 784$ -dimensional vector. Each entry in the vector represents the gray value for a single pixel in the image. We'll denote the corresponding desired output by $y(x)$, where $y$ is a 10-dimensional vector. 

What we'd like is an algorithm which lets us find weights and biases so that the output from the network approximates $y(x)$ for all training inputs $x$. To quantify how well we're achieving this goal we define a cost function:

<div class="textbox">

$$ 
C(w,b) \equiv \frac{1}{n}\sum_x \lVert y(x) - a \rVert^2
$$

</div>

Here, $w$ denotes the collection of all weights in the network, $b$ all the biases, $n$ is the total number of training inputs, $a$ is the vector of outputs from the network when $x$ is the input. The notation $\lVert v \rVert$ just denotes the usual length function for a vector $v$. We'll call $C$ the <span class="highlight-sketch"> quadratic cost function </span>; it's also sometimes known as the <span class="shadow-blue"> mean squared error </span> or just <span class="shadow-blue"> MSE </span>. 

Inspecting the form of the quadratic cost function, we see that $C(w,b)$ is non-negative, since every term in the sum is non-negative. Furthermore, the cost $C(w,b)$ becomes small, i.e., $C(w,b) \approx 0$, precisely when $y(x)$ is approximately equal to the output $a$, for all training inputs, $x$. 

So our training algorithm has done a good job if it can find weights and biases so that $C(w,b) \equiv 0$. By contrast, it's not doing so well when $C(w,b)$ is large-that would mean that $y(x)$ is not close to the output $a$ for a large number ofinputs. 

The aim of our training algorithm will be to minimize the cost $C(w,b)$ as a function of weights and biases. In other words, we want to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as <span class="shadow-white"> gradient descent </span>.

Why introduce the <span class="shadow-blue"> quadratic cost </span>? After all, aren't we primarily interested in the number of images correctly classified by the network? Why not try to maximize that number directly, rather than minimizing a proxy measure like the <span class="shadow-blue"> quadratic cost </span>?

The problem with that is that the number of images correctly classified is <span class="underline-pink"> not a smooth function of weights and biases in the network </span>.  For the most part, making small changes to the weights and biases won't cause any change at all in the number of training images classified correctly. That makes it difficult to figure out how to change the weights and biases to get improved performance. 

Even given that we want to use a <span class="underline-pink"> smooth cost function </span>, you may still wonder why we choose the quadratic cost function used in the equation above. This is a <span class="highlight-skew"> valid concern, and later we'll revisit the cost function, and make some modifications <span>.

Now, let's just imagine we've simply been given a function of many variables and we want to minimize that function $C(v)$. This could be any real-valued function of many variables, $v = v_1, v_2, \dots $. Note that I've replaced $w$ and $b$ notation by $v$ to emphasize that this could be any function-we're not specifically thinking in the neural network context anymore. To minimize $C(v)$, it helps to imagine $C$ as a function of just two variables, which we'll call $v_1$ and $v_2$:

<picture><img src="{{site.baseurl}}/assets/images/valley.png" class="img-small-center"></picture>

What we'd like to find <span class="stroke"> where $C$ achieves its global minimum. </span> Now, of course, for the function plotted above, we can eyeball the graph and find the minimum. A general function $C$, however may be a complicated function of many variables, and <mark color="teal"> it won't usually be possible to just eyeball the graph to find the minimum </mark>.

We start by thinking of our function as a kind of valley. And we imagine a <span class="underline-blue"> ball rolling down the slope of the valley </span>. We'd randomly choose a starting point for an (imaginary) ball, and then simulate the motion of the ball as it rolled down to the bottom of the valley. We could do this simulation simply by <span class="neon"> computing derivatives </span> <mark class="coral"> (and perhaps some second derivatives) </mark> of $C$-those derivatives would tell us everything we need to know about the <span class="zoom"> local shape </span> of the valley, and therefore how our ball should roll.

Let's think about what happens when we move the ball a small amount $\Delta v_1$ in the $v_1$ direction, and a small amount $\Delta v_2$ in the $v_2$ direction. Calculus tells us that $C$ changes as follows:

$$
\Delta C \approx \frac{\partial C}{\partial v_1}\Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2 
$$

We're going to find a way of choosing $\Delta v_1$ and $\Delta v_2$ so as to make <mark color="coral"> $\Delta C$ negative; </span> We'll choose them so the ball is rolling down into the valley. 

To figure out how to make such a choice it helps to define $\Delta v$ to be the vector of changes in $v$, $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$ .
j

We'll also define the <span class="shadow"> gradient of $C$ ($\nabla C$) </span> to be the vector of partial derivatives, $(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T$ . We denote the gradient vector by $\nabla C$, i.e.,

$$
\nabla C = (\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2})^T
$$

<mark class="coral"> In a moment we'll rewrite the change $\Delta C$ in terms of $\Delta v$ and the gradient $\nabla C$. 

Before getting to that, though, <b> I want to clarify something that sometimes gets people hung up on the gradient </b>. When meeting the $\nabla C$ notation for the first time, people sometimes wonder how they should think about the $\nabla$ symbol.

<b> What, exactly, does $\nabla$ mean? </b> In fact, it's perfectly fine to think of $\nabla $C$ as a single mathematical object-the vector defined above-which ahppens to be written using two symbols. In this point of view, <mark class="gold"> $\nabla$ is just a piece of notational flag, telling you "hey, <span class="three"> $\nabla C$ is a gradient vector </span>".

With these definitions, $\Delta C$ can be rewritten as 

<div class="textbox">

$$
\Delta C \approx \nabla C \cdot \Delta v
$$

</div>

This equation helps use explain why $\nabla C$ is called the gradient vector: <mark class="blue"> $\nabla C$ relates changes in $v$ to changes in $C$, just as we'd expect something called a gradient to do </mark>. 

But what's really exciting about the equation is that it lets us see <span class="highlight-yellow"> how to choose $/Delta v$ so as to make $\Delta C$ negative. </span>

Suppose we choose 

<div class="neon-green">

$$
\Delta v = - \eta \nabla C
$$

</div>

 where $\eta$ is a small, positive parameter (known as the <span class="reveal"> learning rate </span>). Then the above equation tells us that $\Delta C \approx - \eta \nabla C = - \eta \lVert \nabla C \rVert^2$ .


 Because $\lVert \nabla C \rVert^2$, this guarantees that $\Delta C \leq 0$, i.e., <span class="blink"> $C$ will always decrease </span>, never increase, if we change $\Delta v = - \eta \nabla C$ in $\Delta C = \nabla C \cdot \Delta v$ . 

 <u> This is exactly the property we wanted! </u> So we'll use the equation $\Delta v = - \eta \nabla C$ to compute a value for $/Delta v$, then move the ball's position $v$ by that amount: 

 $$
 v \rightarrow v' = v - \eta \nabla C
 $$

<span class="arrow-highlight"> We'll use this update rule again, to make another move. </span> <b> If we keep doing this, over and over, we'll keep decreasing $C$ until-we hope-we reach a <u>global minimum </u></b>.

The way the <span class="typewriter"> gradient descent algorithm </span> works is to <u> repeatedly compute the gradient $\nabla C$ </u>, and then move in the <span class="shadow-white"> opposite direction </span>, `falling down` the slope of the valley. 

<picture><img src="{{site.baseurl}}/assets/images/valley-ball.png" class="img-small-center"></span>

<mark class="blue"> Notice that, with this rule, the gradient descent doesn't reproduce real physical motion </mark>. In real life a ball has <span class="gif"> momentum </span>, and that momentum may allow it to <span class="monospace"> roll across the slope, or even (momentarily) roll uphill </span>. It's only after the effects of friction st in that the ball is guaranteed to roll down into the valley. 

By contrast, our rule for choosing $\Delta v$ just says `go down, right now`. That's still a pretty good rule for finding the minimum!

<div class="textbox">
<span class="neon-green"> Gradient </span> <ul><li>Gradient measures how much the output of a function changes if you change the inputs a little bit. </li><li>You can also think of a gradient as the slope of a function.</li><li> In mathematical terms, a gradient is a partial derivative with respect to its inputs. </li>
</div>



<h2> Backpropagation Algorithm </h2>

<h3> The Cost Function </h3>

The goal of back-propagation is <mark class="gold"> to compute the partial derivatives $\frac{\partial C}{\partial w}$ and $\frac{\partial C}{\partial b}$ of the cost function $C$ with respect to any weight $w$ or bias $b$ in the network </mark>. 

For back-propagation to work we need to make <span class="three"> two main assumptions </span> about the form of the cost function. 

It's useful to have a cost function in mind, so we'll set a quadratic cost function which has the form ($n$ is the number of training examples; $x$ is the sum over individual training examples; $y(x)$ is the corresponding desired output; $L$ denotes the number of layer in the network; $a^L(x)$ is the vector of activations output from the network when $x$ is the input).

$$
C = \frac{1}{2n} \sum_x \lVert y(x) - a^L(x) \rVert^2
$$

The first assumption about the cost function $C$ is that the cost function can be written as <span class="highlight-gradient"> an average $C = \frac{1}{n} \sum_x C_x$ over cost function $C_x$ for individual training examples $x$ </span>.

The reason we need this assumption is because what back-propagation actually lets us do is <mark class="coral"> is compute the partial derivatives $\partial C_x / \partial w$ and $\partial C_x / \partial b$ for a single training example. We then recover $\partial C / \partial w$ and $\partial C / \partial b$ by averaing over training examples. 

For example, for the quadratic cost function's cost for a single training example
The second assumption we make about the cost is that it can be written as a function of the outputs from the neural network:


<h2> References </h2>
<ul><li><a href="http://neuralnetworksanddeeplearning.com/chap1.html"> online book </a></li>
</ul>

