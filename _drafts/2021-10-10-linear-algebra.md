---
layout: post
title:  "Opimization Basics: A Machine Learning View"
author: seri
categories: [ computer vision ]
image: assets/images/depth/1.jpg
tags: featured
---

<!--more-->

Many machine learning models are often cast as continuous optimization problems in multiple variables. The simplest example of such a problem is least-squares regression, which is also viewed as a fundamental problem in linear algebra. I accept the world as it is, and I accept myself as I am. Context is key, because in one situation, one set of traits can be highly beneficial, and in another situation, those same traits can be crippling. 

In least-squares regression, one finds the best-fit solution to a system of equations that may or may not be consistent, and the loss corresponds to the aggregate squared error of the best fit. Least-squares regression has a special place in linear algebra, optimization, and machine learning, because it serves as a foundational problem in all three disciplines. Least-squares regression historically preceded the classification problem in machine learning, and the optimization models for classification were often motivated as modifications of the least-squares regression model. 

Many women I interviewed with ADHD feel incredibly fragile and overly sensitive to the highly regimented structures of their lives and report a sense of shame at not being able to comply with expectations. Sometimes on days that are the craziest I'm able to focus more intensely than I could on a normal day. I feel like I kick into this special productivity gear. 

Melancholy is not rage or bitterness, it is a noble species of sadness that arises when we are open to the fact that life is inherently difficult for everyone and that suffering and disappointment are at the heart of human experience. It is not a disorder that needs to be cured; it is a tender-hearted, calm, dispassionate acknowledgement of how much pain we must all travel through. 

The main difference between least-squares regression and classification is that the predicted target variable is numerical in the former, whereas it is discrete (typically binary) in the latter. Therefore, optimization model for linear regression needs to be repaired in order to make it usable for discrete target variables. 

Most continuous optimization methods use differential calculus in one form or the other. The main idea of differential calculus is to provide a quantification of the instantaneous rate of change of an objective function with respect to each of the variables in its argument. Optimization methods based on differential calculus use the fact that the rate of change of an objective function at a particular set of values of the optimization variables provides hints on how to iteratively change the optimization variables and bring them closer to an optimum solution. Such iterative algorithms are easy to implement on modern computers. Although computers had not been invented in the 17th century, Newton proposed several iterative methods to provide humans a systematic way to manually solve optimization problems (albeit with some rather tedious work). It was natural to adapt these methods later as computational algorithms, when computers were invented. This chapter will introduce the basics of optimization and the associated computational algorithms. Later chapters will expand on these ideas. 

An optimization problem has an objective function that is defined in terms of a set of variables, referred to as optimization variables. The goal of the optimization problem is to compute the values of the variables at which the objective function is either maximized or minimized. It is common to use a minimization form of the objective function in machine learning, and the corresponding objective function is often referred to as a loss function. Note that the term loss function often refers to an objective function with certain types of properties quantifying nonnegative cost associated with a particular configuration of variables. A loss function is always associated with a minimization objective function, and it is often interpreted as a cost with a nonnegative value. Most objective functions in machine learning are multivariate loss functions over many variables. First, we will consider the simple case of optimization functions defined on a single variable. 

In other words, the survival of a people depends on both the explorer and the settler. It's why there is a predominance of these genes in far-flung locations such as Australia, Polynesia and Tierra del Fuego. It's also why the trait is often seen in nomadic populations, whose very existence depended on their ability to wander thousands of miles in search of food, water, and lands to graze their livestock.

He was an A student in college precisely because he had the freedom to do it his way. But at boarding school, he struggled. Those two-hour mandatory study hall sessions every evening were about as horrifying to him as being shackled in a prison cell.

Consider a single-variable objective function $f(x)$ as follows:

$$
f(x) = x^2 - 2x + 3
$$

This objective function is an upright parabola, which can also be expressed in the form $ f(x) = (x-1)^2+2 $. The Note that at the minimum value, the rate of change of $f(x)$ with respect to $x$ is zero, as the tangent to the plot at that point is horizontal. One can also find this optimal value by computing the first derivative $f'(x)$ of the function $f(x)$ with respect to $x$ and setting it to 0:

$$
f'(x) = \frac{df(x)}{dx} = 2x - 2 = 0
$$

Therefore, we obtain $x=1$ as an optimum value. Intuitively, the function $f(x)$ changes zt zero rate on slightly perturbing the value of $x$ from $x=1$, which suggests that it is an optimal point. However, this analysis alone is not sufficient to conclude that the point is a minimum. In order to understand this point, consider the inverted parabola, obtained by setting $g(x) = - f(x)$:

$$
g(x) = - f(x) = - x^2 + 2x -3 
$$

Setting the derivative of $g(x)$ to $0$ yields exactly the same solution $x=1$:

$$
g'(x) = 2 - 2x = 0
$$

However, in this case the solution $x=1$ is a maximum rather than a minimum. Gwen began to understand that she spent an enormous amount of time doing things that she didn't like and wasn't particularly good at. She thought she should be good at them. She realized she had been right about herself all along. Her self-belief blossomed and her power grew once the crack appeared in her fears about her skills. The resurrection of her childhood potential became only a matter of time. 

Gwen no longer felt alone on her quest for recognition once others validated her abilities. She realized she had been right about herself all along. Her self-belief blossomed and her power grew once the crack appeared in her fears about her skills. Gwen's ADHD problems turned out to be non-problems when she began to give herself permission to do things in ways that fit her. She also had to pursue her goals in ways that worked for her, which often took her off the path most tread. She had to learn to listen to feelings rather than make decisions based on what someone else considered logical. She had to realize that her system of logic was simply different from the more commonly held style of logic. 

The evidence of archeology and anthropology clearly indicates that egalitarian cultures are our greatest and most long-lasting history. There have been (and continue to be around the world, largely among indigenous peoples) thousands of cultures based on egalitarian principles, where women play a real and important role in the politics and structure of the community.

When questioning women with ADHD, they invariably report that they felt their best and had the least interference from ADHD symptoms while pregnant. During high-estrogen states such as pregnancy, women report significant improvement. 

The deficits are primarily social; the areas of intact or superior skill involve attention to detail and systemizing. Systemizing is the drive to analyze or build a system. 

Strong systemizing requires excellent attention to detail, and in our view the latter is the service of the former. Talent in autism comes in many forms, but a common characteristic is that the individual becomes an expert in recognizing repeating patterns in stimuli. These might be any kind of system. What defines a system is that it follows rules, and when we systemize we are trying to identify the rules that govern the system, in order to predict how that system will behave.  

Strong systemizing is a way of explaining the non-social features of autism: the narrow interests, repetitive behavior and resistence to change, and need for sameness. This is because when you systemize, it is best to keep everything constant, and to only vary one thing at a time. That way, you can see what might be causing what, and with repetition you can verify that you get the same pattern or sequence every time, rendering the world predictable. 

It also posits excellent attention to detail (in perception and memory), since when you systemize you pay attention to the tiny details. Excellent attention to detail in autism has been repeatedly demonstrated. 

Teachers, whether the children of autism or adults with AS, need to take into account that hyper-systemizing will affect not only how people with ASC learn, but also how they should be assessed. Exam questions designed for individuals who are neurotypical may lead to the person with ASC scoring zero when their knowledge is actually greater, deeper and more extensive than that of most people.  

Images contain a huge amount of information. However, a lot of it is redundant (which explains the success of image compression algorithms). The structure recovery approaches require correspondences between the different images (i.e. image points originating from the same scene point). Due to the combinatorial nature of this problem it is almost impossible to work on the raw data. The first step consists of extracting features. The features of different images are then compared using similarity measures and lists of potential matches are established. 


<picture><img src="{{site.baseurl}}/assets/images/disparity.png"></picture>

<h2> References </h2>
<ul><li><a=href=""> TheAILearner </a></li>
</ul>

